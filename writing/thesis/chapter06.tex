Discussion goes here. 

Competition vs cooperation of agents. 

My intuition that the state space is too large to observe any influence of REINFORCE is supported by marginal comments in \cite{havrylov2017emergence}; therefore, future work should try a combination of the structural loss with a Gumbel-Softmax component.

My architecture, esp for MS COCO, can essentially be interpreted as just show-casing that you can ground a second (receiver) agent against a pretrained image captioner. It is hard to consider this a multi-agent communication result since the effect or reinforce propagating the functional signal is so marginal. 

Discuss how this multi-agent communication is one approach to building pragmatic language behaviour based on neural models. But there are other options with a more explicit underlying cognitive motivation, as e.g. done by Monroe and Potts, but which faces computational challenges. 

Discuss relation to Lazaridous work and how I extend the speaker tp be conditioned on both images in all experiments. This is more plausible from a task-functional perspective, but, of course, make the model more hard-coded (to specific set of distractors). Would be interesting to compare frozen samples from such a model (oracle speaker) to their results. I argue that the fine-tuning of the language module is more plausible in that it is motivated by trying to teach the speaker to select words that are right for the referential task. However, Asya's point is well taken in that this should not affect core linguistic capabilities which seem to be affected by co-adaptation. It remains an open question how to mitigate this issue; I think, fine-tuning the model yet using different listeners is the most plausible strategy; a completely different approach would be one to generation procedure (low-level conditioning vs. strucutred feature extraction -- re discussion with MF in Tuebingen). 