Communicating about the ever-changing world around us in a context-appropriate way is a fascinating human ability. Yet training artificial neural agents to communicate in the same way is a non-trivial task. This thesis set out to investigate how this task can be approached by training artificial agents to use natural language in order to communicate about realistic images in a multi-agent communication setting. More specifically, the agents were trained to play a \emph{reference game} in which they had to produce maximally \emph{discriminative} messages in order to succeed on the task. Importantly, extending beyond previous work, the agents both communicated in natural language and received visual input in form of real-world photos. Building upon the work by \cite{lazaridou2020multi}, the phenomenon of \emph{language drift} which occurs as agents improve on their task was the main focus of this work. Aiming to measure the degree of language drift and explain its potential sources, various reference game experiments with different configurations were conducted. 

This final chapter concludes the thesis by, first, discussing results and limitations of the presented experiments and outlining potential directions for future studies. Then, present work is put in context of multi-agent communication research as well as cognitive science, in general, before closing with a conclusion. 

\section{Experiments}

Various experiments were conducted in this thesis in order to investigate language drift in reference games. The main experiments wherein agents were trained to communicate about realistic visual input in natural language were conducted on the MS COCO Captions dataset \parencite{chen2015microsoft}. All reference games included two agents, a speaker and a listener, who communicated about pairs of images (i.e., a target in context of one distractor).
First, a baseline experiment established that agents trained with a combination of a task-based functional learning signal and a language structure preserving supervision signal can successfully learn to play the reference game on real-world images. Second, a set of experiments varying the strength of the supervision constraints put on the surface structure of the messages the agent produced showed that this learning contraint did not affect the strength of the language drift. Third, in contrast to preceding experiments where the target and the distractor image were combined at random, an experiment with \emph{similar} target-distractor pairs was conducted. It revelaed that it was much more difficult for the agent to learn the task given images which were difficult to discriminate, confirming the agents' sensitivity to their communicative context. It also showed that in absence of a strong task success based learing signal, language drift was weaker. Finally, in order to address the potential effect of speaker-listener co-adaptation, experiments wherein the speaker was trained against a fixed pretrained listener were conducted. They showed that the functional improvements visible in the previous experiments are to a large extent carried by this co-adaptation; they also showed that, consistent with predictions, the grounded listener mitigated semantic drift.

In order to investigate the importance of the presence of fully descriptive (i.e.,~exhaustive) example captions in the training dataset, the results from MS COCO experiments were compared to experiments on a different dataset---the 3Dshapes dataset \parencite{burgess20183d}. To this end, custom natural language annotations were created for this dataset. Two sets of annotations were created---exhaustive image captions describing all six properties that varied between the images in the dataset, and short image captions which mentioned only up to three properties of the image.
The same experiments as on MS COCO were replicated on the dataset with exhaustive annotations. The qualitative results of these experiments patterened with MS COCO, although the agents were better able to learn the reference game on similar image pairs. Crucially, on average, language drift of the agents trained on the reference games compared to pretrained speakers was smaller relative to MS COCO, indicating that exhaustive annotations and smaller vocabulary size of 3Dshapes mitigate drift. However, this comparison was confounded with a large difference in the vocabulary sizes the agents learned. 
Therefore, additional experiments varying the exhaustivity within the 3Dshapes dataset were conducted, including annotations consisting of both exhaustive and short captions. They showed that agents were still able to learn the reference game, and language drift actually decreased, indicating that the shorter annotations were enough for identifying discriminative properties of the images and that shorter messages were easier to learn for the speaker. Finally, in order to investigate if the need for exhaustive captions is stronger when target and distractor are more difficult to distinguish (i.e., the pairs are similar), the similar pairs experiment was also conducted with annotations including short captions. The results did not differ from the experiments with fully exhaustive annotations in terms of game success, while language drift also decreased, indicating that shorter messsages were easier to learn even given more complex visual input. Nevertheless, a POS analysis revealed that exhaustive annotations might be critical for learning to address discriminative features of the images more precisely.

However, present experiments were subject to some limitations which should be improved upon in future work. First, due to computational constraints all reference games were only trained for two epochs (around 4,800 steps), which might have led to an absence of full effects of the task-based training signal \parencite[compared to, e.g.,][who had 300,000 training steps]{lee2019countering}. Longer training might provide results which can be interpreted more strongly with respect to the influence of the functional training signal. In particular, longer training might allow to better estimate the validity of the continuous overlap measure as a functional drift metric (cf. Section~\ref{hypos_discussion}).
Second, due to the general difficulty of training image captioning models which generalize well to the auto-regressive inference mode \parencite{lamb2016professor}, and the large space of hyperparameters which may be tuned for training such a model, the quality of the pretrained speaker models was not comparable to state-of-the-art image captioners (see Table \ref{tab_coco_metrics_ref}~vs.~Table \ref{coco_grid_searches_speaker_pretrain}). This might have led to overly pessimistic structural drift estimates which are computed under a pretrained LM and, therefore, measure the naturalness of the produced captions. Although the relative comparisons between experimental configurations are not affected by this shortcoming because they were all based on the same pretrained speaker, replicating these results with a better base model could improve the comparability of the presented results to other image captioning work and the interpretability of semantic drift vakues of the ground truth captions of MS COCO. To this end, alternative pretraining schedules like professor-forcing could be considered \parencite{lamb2016professor}. Furthermore, it could be explored whether starting with the auto-regressive pretraining mode and then increasingly transitioning to teacher forcing might yield more stable model performance \parencite[although similar explorations by][suggest that this order might not work very well]{lowe2020interaction}. A pretraining strategy for the 3Dshapes dataset could also be determined separately because presented experiments used a strategy selected based on MS COCO performance. Different annotations for the 3Dshapes (e.g., with different syntactic structures) could also provide a broader picture of structural drift.
Third, all main experiments used pure decoding for sampling the speaker's messages; however, exploratory experiments showed that the decoding strategy might have a substantial influence on the task success (see Section~\ref{expt:coco_greedy}, Appendix \ref{app:grid_search}), so future experiments should also consider other strategies like the popular beam search (see Section \ref{model_pretraining}). Furthermore, a speaker pretrained with greedy decoding should be employed for replicating and extending the presented greedy decoding experiments.
Finally, the embedding layers converting the natural language captions to vectorized representations were trained from scratch in all experiments. While this set up provided the advantage of creating embeddings optimal for the task, it substantially increased the number of learnable parameters of the model and, therefore, the complexity of the computational task. In order to alleviate that, future work should examine if the use of pretrained embeddings is advantageous \parencite[following, e.g.,][]{atliha2021pretrained}.

Turning to the overall experimental set up, an interesting direction for future work might be to explore the effects of a larger number of distractors on the strength of language drift. Intuitively, the more distractors there are, and the more similar they are, the more constrained the functional learning signal might be, so that structural and semantic language drifts might be mitigated. However, this might lead to higher functional drift. Furthermore, training a population of agents (e.g., a single speaker against a population of listeners, or a population with switching roles akin to \cite{bouchacourt2019miss}) would be an interesting future step with potential to uncover the dynamics of language drift in a population, which might connect to the line of work on language evolution in multi-agent communication \parencite[e.g.,][]{graesser2019emergent, chaabouni2019anti, kirby2014iterated}. Note that the fixed listener experiments in this work approximate training a single speaker against an infinite population of listeners which in the limit converge to a single agent. Nonetheless, other fixed listener architectures may be considered. For instance, the listener could be an RSA-style literal listener agent interpreting the speaker's messages according to their probability given each of the images, as assigned by the pretrained speaker model (i.e., the listener's representation of literal semantics,~\cite[cf.][]{frank2016rational}).\footnote{This architecture was suggested by Michael Franke during the conceptualization of this thesis. It was explored for both datasets and did not yield successful reference games due to insuffiecient speaker quality.} 

Additionally, the experimental set up could be enriched by a text-to-image model in order to formalize functional drift through the lens of \emph{recoverability}---i.e., being able to reconstruct the intended image given the produced caption. The idea would be that discriminative captions have a higher recoverability measure for the target compared to distractors. For instance, this could be operationalized in two ways. One architecture could include a pretrained image-text retrieval model like VSE++ \parencite{faghri2017vse++}; this model would provide a top-k recall score which indicates how often the target is recovered as being among the top k most suitable image given a caption. This score is expected to be higher for the target than distractors, given discriminative messages. Alternatively, this could be operationalized via a generative text-to-image model like DALL-E \parencite{ramesh2021zero}, where, ideally, the image produced from the generated caption would be more similar to the original target than the distractor image. The recall score or the similarity metric could then be a functional drift measure. However, these metrics would both heavily depend on the nature of the pretrained models as well as in the chosen image similarity metric. Nevertheless, this might be a promising direction for future work which might resonate with cognitive aspects of communication (see Section~\ref{discussion_cog_perspective} below). 

Last but not least, follow-up experiments might consider alternative functional learning signal formulations. The presented experiments exhibited quite a large variance of the speaker loss component computed with the REINFORCE algorithm. Therefore, either baseline subtraction could be included (see Section \ref{rl_methods}), or the functional signal could be formalized with the straight-through Gumbel-Softmax estimator, as proposed by \cite{havrylov2017emergence}. They note that this might improve convergence for a larger action space (i.e., for the MS COCO experiments' vocabulary) and thus make the task-based training signal more pronounced, better isolating the signal necessary for learning discriminative language use and language drift mitigation.
%My architecture, esp for MS COCO, can essentially be interpreted as just show-casing that you can ground a second (receiver) agent against a pretrained image captioner. It is hard to consider this a multi-agent communication result since the effect or reinforce propagating the functional signal is so marginal. 

In general, the results indicate that multi-agent communication systems can be applied in realistic environments, yet the quality of their language is strongly affected by the speaker pretraining, action space size, the agents\ co-adaptation potential and by the similarity of potential targets in the environment.
%\pt{maybe structure discussion according to: summary, experimental methods, conceptual aspects, relation to extant work and future directions, conclusion.}
%Overall, we see how important the combination of particular hyperparameters and training configurations are, indicating that future research should focus on architectures which might be more stable against the alternations. 
%\pt{def. discuss language drift and the success of the new metrics, comparison to old ones, limitations and future directions. See general neural systems evaluation considerations made e g by \cite{kreiss2022context, mccoy2019right} }. 

\section{Multi-Agent Communication}

Despite the limitations, this work adds two contributions to multi-agent communication research. First, it was shown that two neural agents can successfully learn to play a reference game on realistic visual input, and do so with natural language sentences which would also be sufficient for humans in order to accomplish the task (e.g.,~Fig.~\ref{fig:coco_randPairs_speaker_generations}).
This extends existing literature by combining the use of both real-world images and natural language in multi-agent communication in a way that could be used for real-world applications and might serve as a step towards training agents that could be used for communicating with humans when collaboratively completing tasks \parencite{lazaridou2020emergent, havrylov2017emergence, lazaridou2016multi, andreas2016reasoning, cohn2018pragmatically}. 
Second, this thesis contributes to the body of work addressing language drift in multi-agent systems \parencite[e.g.,][]{lewis2017deal, lee2019countering, lazaridou2020multi}. The results provide additional insights regarding the factors behing language drift. For one, in contrast to, \cite[e.g.,][]{lewis2017deal, lee2019countering}, the functional (REINFORCE-based) training signal did not lead to significantly higher language drift compared to supervised (structural-only) training in the random pairs reference games on either dataset. One reason for this difference might be the pretraining mode of the agents which is usually not described in detail in the literature, indicating the importance of this aspect and suggesting that language drift might partly be observed if the inference mode of the speaker model in the functional task differs from its pretraining mode. Furthermore, making the distinction between easy and difficult target-distractor pairs more explicit and conceptually motivated than \cite{lazaridou2020multi}, the similar pairs experiments in this work showed that while this perceptual similarity affects the agents' success on the reference game, it only slightly affected the strength of language drift in the MS COCO experiment. This hints at a possible interaction between the size of the action space and perceptual complexity when it comes to language drift. The similar pairs experiment on 3Dshapes also showed that agents tend towards producing longer and, therefore, possibly more granular messages in presence of a similar distractor, showing that functional language learning might lead to more human-like language use \parencite[cf.][]{graf2016animal}.
Additionally, the novel comparison of different annotation sets in the 3Dshapes experiment showed that language drift metric values are strongly related to the length of the captions the agents are trained on and generally favor shorter captions. This comparison also showed that the reason for language drift cannot fully be attributed to the absence of exhaustive annotations in the dataset since the agents trained on shorter annotations showed language drift improvements relative to the respective pretrained speakers. This results might be due to the confound based on the length of exhaustive messages; future work could address this issue by creating artificial names for compound features and thereby keeping the annotations short (e.g., ``glob'' referring to a tiny red block etc).
Confirming the results by \cite{lazaridou2020multi}, the experiments with fixed pretrained listeners uncovered speaker-listener co-adaptation. Consistent with literature, this co-adaptation mostly caused semantic drift \parencite[cf.][]{lee2019countering}, but also affected somewhat structural drift. 

The newly tested overlap metrics did not show clear trends in the different experimental configurations on random image pairs, but clearly reflected the agents' functional success, decreasing as the agents' test accuracy decreased. Therefore, at least the discrete overlap turned out to be a valid functional drift metric. However, some aspects of the metrics could be improved upon in future work. First, the vocabularies of the agents were quite large, making metrics based on exact token overlaps and not accounting for semantic similarity rather instable. Second, the embedding based overlap which might address the semantic similarity issue might be susceptible to vanishing gradients, such that the embedding layers might not have been fully trained in the short training times of these experiments. Testing this metric, e.g., in combination with pretrained embeddings might be a promising next step. Finally, their relation to ground truth captions might be conceptually problematic in that referencing descriptive, but not discriminative, ground truth captions might not have the capacity to assess the relevance of image features which were mentioned in the disriminative message, but not in the ground truth caption. One possibility for addressing this issue in future work might be the use of multimodal embeddings which might capture relevance through low-level feature similarity \parencite[cf.][]{bruni2014multimodal}. Overall, the difficulty in capturing functional drift in absence of benchmarks resonates with general difficulties of evaluating functional adequacy of neural networks in the field, emphasizing the importance of further research \parencite[as noted, e.g., by][]{kreiss2022context, mccoy2019right}.

%\pt{This could be the section for discussing less technical, more conceptual points regarding LD which are not discussed in last section of previous chapter.}
Another interesting direction for future work on quantifying functional drift is testing the presented system against other datasets like, e.g., the dataset constructed by \cite{mao2016generation} for testing referential expressions. To this end, a critical question would be to quantify as to how far discriminativity in a reference game aligns with quality of referential expressions. As far as it does, this dataset could provide a good benchmark for evaluating the functional success of the speaker. Another interesting challenge in order to connect presented work to the set up by \cite{mao2016generation} is the need to formalize the step of inferring a suitable target bounding box for the reference game (i.e., identifying a discriminative referent), based on which the discriminative expression would then be generated. 
%Andreas and Klein 2016 seems highly relevant to my intuition about regularization and strucutral weight. 

Presented experiments also added results based on an architecture wherein the speaker agent was conditioned on both the target and the distractor, allowing for the cognitively motivated potential to condition language on visually extracted discriminative differences. This stands in contrast to the literature which mostly conditions the speaker only on the target image \parencite[except for][]{lazaridou2016multi, lazaridou2020multi}. A further step in order to explicate the importance of this conditioning could be the use of an attention-based speaker architecure \parencite[e.g., akin to][]{lee2019countering}. Alternatively, the visual reasoning preceding language generation could be made more explicit by adding, e.g., a scene parsing or object detection component to the model \parencite[e.g.,][]{zhao2017pyramid}, which could allow for more explicit reasoning about the relevant image aspects which should be mentioned in the message.
%Discuss relation to Lazaridous work and how I extend the speaker to be conditioned on both images in all experiments. This is more plausible from a task-functional perspective, but, of course, make the model more hard-coded (to specific set of distractors). Would be interesting to compare frozen samples from such a model (oracle speaker) to their results. I argue that the fine-tuning of the language module is more plausible in that it is motivated by trying to teach the speaker to select words that are right for the referential task. However, Asya's point is well taken in that this should not affect core linguistic capabilities which seem to be affected by co-adaptation. It remains an open question how to mitigate this issue; I think, fine-tuning the model yet using different listeners is the most plausible strategy; a completely different approach would be one to generation procedure (low-level conditioning vs. strucutred feature extraction ). 

One important assumption made in the reference game set up is that the agents are \emph{cooperative} because they receive rewards based on their joint task performance. Therefore, language use which can be studied in such a task could also be considered cooperative. However, human language use is by far not always cooperative, as is exemplified in, e.g., lying \parencite{franke2020strategies}. Training agents to communicate in natural language in non-cooperative environments would contribute to work on competing and self-interested agents \parencite[cf.][]{lazaridou2020emergent} and might present an interesting avenue for investigating language drift which might, e.g., be used by the agents as a strategic device for competitive advantage. 

Overall, results of the presented experiments add to the literature a comprehensive picture of the interaction between different architectural and dataset-related configurations of a reference game and their effects on language drift, while including diverse baselines allowing for better interpretability of the results. 


\section{Cognitive Perspective}
\label{discussion_cog_perspective}
The presented experiments took a step towards the long-standing goal of teaching artificial agents pragmatic language use. The focus of this work was the agents' ability to adjust descriptive messages for images to discriminative needs of the given context, which amounts to learning to generate messages including context-sensitive referential expressions. A critical assumption made in this work is that the discriminative aspects of the context are visual, i.e., the target and distractor are images with different content, and attending to those differences makes the referential messages discriminative (as briefly alluded to above). In the used architecture, visual context is represented in a \emph{subsymbolic} way via pretrained image vectors, and, importantly, the representation is \emph{not} subject to fine-tuning. However, a substantial body of work in cognitive science argues that \emph{structured} model components might be necessary for human-like reasoning \parencite[e.g.,][]{tenenbaum2011grow, lake2017building}. For instance, \cite{monroe2015learning} make a steps towards combining Bayesian RSA components with neural models.
Similarly, a stream of work proposes that learning different reasoning skills, including language, might be conceptualized as program induction rather than learning subsymbolic neural representations. For instance, \cite{nye2020learning} propose a neuro-symbolic system which outperforms neural meta-learning systems on tasks like instruction following by inducing generalization rules on a small set of examples. \cite{wong2022identifying} show that human image description vocabularies can be optimally represented by program libraries; \cite{lake2015human} show that a probabilistic program induction model achieves human-level performance when learning concepts from few visual examples. 

This also relates to a common concern that deep models generally require colossal amounts of training data in order to achieve human-level performance. Applied to linguistic abilities, this stands in contrast to children which learn language from only few examples \parencite[e.g.,][]{xu2007word}. This also raises environmental concerns regarding the costs of building deep learning models \parencite{bender2021dangers}.  
%Discriminativity vs reference vs relevance. Level of discriminativity extraction (as related to future directions for different agent architectures). Multi-turn interactions. Testing against humans. Other pragmatic abilities. 
Another critical aspect which should not be forgetten when building deep learning models is that, besides potentially learning reasoning abilities, they replicate the statistics of the datasets they are trained on, replicating potential biases in those datasets, respectively, which may have serious societal consequences when such models are deployed (\cite{bender2021dangers}, \cite{buolamwini2018gender}). 

This thesis focused on modeling a specific aspect of pragmatic reasoning in one task, yet human pragmatic abilities extend to many more linguistic phenomena beyond context-sensitive referential expression generation, including the use of gradable adjectives \parencite{qing2014gradable}, indirect language \parencite{yoon2016talking} and resolution of lexical uncertainty \parencite{bergen2016pragmatic}.
Furthermore, human language use is not only grounded in the environment, but also supplemented by a vast background of experience, world knowledge and individual beliefs \parencite{lake2017building, franke2016does}, supporting efficient reasoning and learning. There is an increasing interest in enhancing neural language generation systems both with symbolic reasoning components based on structured world knowledge \parencite[e.g.,][]{nye2021improving}, and leveraging large neural language models as sources for world knowledge themselves \parencite{petroni2019language}. However, this aspect has not received much attention in the multi-agent language emergence and learning domain yet. 
All these phenomena present an exciting avenue for further pushing the boundaries of multi-agent communication in order to train neural agents to communicate in a more human-like way. 

\section{Conclusion}
To sum up, this thesis has investigated how artificial agents can be trained to produce discriminative natural language in reference games involving realistic images. The focus thereby was to measure different kinds of language drift the agents are subject to and explain their potential sources. By conducting a series of different experiments on two datasets, this work highlighted that language drift is highly sensitive to the architecture of the agents, the size of their action space, the nature of the dataset and the context from which the agents learn to use natural language. Well-formed discriminative messages optimized for successful reference may be best learned from visual contexts depicting diverse objects, given rather short example messages, and based on robustly pretrained agents.  
This work showed that off-the-shelf agents can interactively learn to successfully complete a novel complex task on extant datasets to which they can flexibly adapt, just like humans can adjust their communication towards the needs of the context.
Thereby, this thesis took a step towards building agents that could naturally communicate with humans in real-world applications, hoping to contribute to future work developing approaches that are robust against language drift.