Communicating about the ever-changing world around us in a context-appropriate way is a fascinating human ability. Yet training artificial neural agents to communicate in the same way is a non-trivial task. This thesis set out to investigate how this task can be approached by training artificial agents to use natural language in order to communicate about realistic images in a multi-agent communication setting. More specifically, the agents were trained to play a \emph{reference game} in which they had to produce maximally \emph{discriminative} messages in order to succeed on the task. Importantly, extending beyond previous work, the agents both communicated in natural language and received visual input in form of real-world photos. Building upon the work by \cite{lazaridou2020multi}, the phenomenon of \emph{language drift} which occurs as agents improve on their task was the main focus of this work. Aiming to measure the degree of language drift and explain its potential sources, various reference game experiments with different configurations were conducted. 

This final chapter concludes the thesis by, first, discussing results and limitations of the presented expriments, outlining potential directions for future studies. Then, present work is put in context of multi-agent communication research as well as cognitive science, in general. 

\section{Experiments}

Various experiments were conducted in this thesis in order to investigate language drift in reference games. The main experiments wherein agents were trained to communicate about realistic visual input in natural language were conducted on the MS COCO Captions dataset \parencite{chen2015microsoft}. All reference games included two agents, a speaker and a listener, who communicated about pairs of images (i.e., a target in context of one distractor).
First, a baseline experiment established that agents trained with a combination of a task-based functional learning signal and a language structure preserving supervision signal can successfully learn to play the reference game on real-world images. Second, a set of experiments varying the strength of the constraints put on the surface structure of the messages the agent produced showed that this learning contraint did not affect the strength of the language drift. Third, in contrast to preceding experiments where the target and the distractor image were combined at random, an experiment with \emph{similar} target-distractor pairs was conducted. \pt{TBD what it showed, but definitely something about the effect of perceptual similarity / discriminability}. Finally, in order to address the potential effect of speaker-listener co-adaptation, experiments wherein the speaker was trained against a fixed pretrained listener were conducted. They showed that the functional improvements visible in the previous experiments are to a large extent carried by this co-adaptation; they also showed that, consistent with intuitions, the grounded listener mitigated semantic drift. \pt{Discuss this with respect to strength of functional signal /action space size in comparison to 3Dshapes}. 

In order to investigate the importance of the presence of fully discriptive, or, exhaustive example captions in the training dataset, the results form MS COCO experiments were compared to experiments on a different dataset---the 3Dshapes dataset \parencite{burgess20183d}. To this end, custom natural language annotations were created for this dataset. Two sets of annotations were created---exhaustive image captions describing all six properties that varied between the images in the dataset, and short image captions which mentioned only up to three properties of the image.
The same experiments as on MS COCO were replicated on this dataset with exhaustive annotations. The qualitative results of these experiments patterened with MS COCO, indicating that, per se, the presence of exhaustive captions did not alleviate language drift. \pt{double check if this is completely correct, I think semantic drift situation was different}. However, this comparison was confounded with a large difference in the vocabulary sizes the agents learned, so an additional baseline experiment was conducted on both exhaustive and short captions. It showed that agents were still able to learn the reference game, and language drift actually decreased, indicating that the partial annotations were enough for identifying discriminative properties of the images and the shorter sequences were easier to learn for the speaker. Finally, in order to investigate if the need for exhaustive captions is stronger when target and distractor are more difficult to distinguish (i.e., the pairs are similar), the similar pairs experiment was also conducted with short captions. \pt{results tbd}.

However, present experiments were subject to some limitations which should be improved upon in future work. First, due to computational constraints all reference games were only trained on two epochs (around 4,800 steps), which might have led to an absence of full effects of the task-based training signal \parencite[compared to, e.g.,][who had 300,000 training steps]{lee2019countering}. Longer training might provide results which can be interpreted more strongly with respect to the influence of the functional training signal. Second, due to the general difficulty of training image captioning models which generalize well to the auto-regressive inference mode \parencite{lamb2016professor}, and the large space of hyperparameters which may be tuned for training such a model, the quality of the pretrained speaker models was not comparable to state-of-the-art image captioners (see Table \ref{tab_coco_metrics_ref}~vs.~Table \ref{coco_grid_searches_speaker_pretrain}). This might have led to overly pessimistic structural drift estimates which are computed under a pretrained LM and, therefore, measure the naturalness of the produced captions. Although the relative comparisons between experimental configurations are not affected by this shortcoming because they were all based on the same pretrianed speaker, replicating these results with a better base model could improve the comparability of the presented results to other image captioning work. To this end, alternative pretraining schedules like professor forcing could be considered \parencite{lamb2016professor}. Furthermore, it could be explored whether starting with the auto-regressive pretraining mode and then increasingly transitioning to teacher-forcing might yield more stable model performance \parencite[although similar explorations by][suggest that this order might nnot work very well]{lowe2020interaction}. A pretraining strategy for the 3Dshapes dataset could also be determined separately because presented experiments used a strategy selected based on MS COCO performance. Different annotations for the 3Dshapes (e. g., with different syntactic structures) could also provide a broader picture of structural drift.
Third, all experiments used pure decoding for sampling the speaker's messages; however, initial exploratory experiments showed that the decoding strategy might have a substantial influence on the task success (see. Appendix \ref{app:grid_search}), so future expriments should also consider other strategies like the popular beam search (see Section \ref{model_pretraining}). \pt{update depending on whether i'll include greedy} 
Finally, the embedding layer converting the natural language captions to vectorized representations were trained from scratch in all experiments. While this set up provided the advantage of being optimal for the task, it substantially increased the number of learnable parameters of the model and, therefore, the complexity of the computational task. In order to alleviate that, future work should examine if the use of pretrained embeddings is advantageous \parencite[following, e.~g.,][]{atliha2021pretrained}.

Turning to the overall experimental set up, an interesting direction for future work might be to explore the effects of a larger number of distractors on the strength of language drift. Intuitively, the more distractors there are, and the more similar they are, the more constrained the functional learning signal might be, so that language drift might be mitigated. \pt{think about this again and compare to H about similar pairs} Furthermore, training a population of agents (e. g., a single speaker against a population of listeners, or a population with switching roles akin to \cite{bouchacourt2019miss}) would be an interesting future step with potential to uncover the dynamics of language drift in a population, which might connect to the line of work on language evolution in multi-agent communication \parencite[e.~g.,][]{graesser2019emergent, chaabouni2019anti, kirby2014iterated}. Note that the fixed listener experiments in this work approximate training a single speaker against an infinite population of listeners which in the limit converge to a single agent. Nonetheless, other fixed listener architectures may be considered. For instance, the listener could be an RSA-style literal listener agent interpreting the speaker's messages according to their probability given each of the images, as assigned by the pretrained speaker model (i.e., the listener's representation of literal semantics) \parencite{frank2016rational}.\footnote{This architecture was suggested by Michael Franke during the conceptualization of this thesis. It was explored for both datasets and did not yield successful reference games due to insuffiecient speaker quality.} 

Last but not least, follow-up experiments might consider alternative functional learning signal formulations. The presented experiments exhibited quite a large variance of the loss component computed with the REINFORCE algorithm \pt{check term correctness}. Therefore, either baseline subtraction could be included (see Section \ref{rl_methods}), or the functional signal could be formalized with the straight-through Gumbel-Softmax estimator, as proposed by \cite{havrylov2017emergence}. They note that this might improve convergence for a larger action space (i.e., for the MS COCO experiments vocabulary) and thus make the task-based training signal more pronounced, better isolating the signal necessary for learning discriminative language use.
%My architecture, esp for MS COCO, can essentially be interpreted as just show-casing that you can ground a second (receiver) agent against a pretrained image captioner. It is hard to consider this a multi-agent communication result since the effect or reinforce propagating the functional signal is so marginal. 

In general, the results indicate that multi-agent communication systems can be applied in realistic environments, yet the quality of their language is strongly affected by the pretraining and, thereby, by the distribution of the pretraining data, as well as architectural considerations like pretraining mode which are often glossed over in the literature.
%\pt{maybe structure discussion according to: summary, experimental methods, conceptual aspects, relation to extant work and future directions, conclusion.}
%Overall, we see how important the combination of particular hyperparameters and training configurations are, indicating that future research should focus on architectures which might be more stable against the alternations. 


\pt{def. discuss language drift and the success of the new metrics, comparison to old ones, limitations and future directions. See general neural systems evaluation considerations made e g by \cite{kreiss2022context, mccoy2019right} }. 

\section{Multi-Agent Communication}

This work hopes to add two contributions to multi-agent communication research. First, it was shown that two neural agents can successfully learn to play a reference game on realistic visual input, and do so with rather intelligible natural language sentences. \pt{double check that i have some examples, and/ or some quantification of the intelligibility}. 
This extends existing literature by combining the use of both real-world images and natural language in multi-agent communication in a way that could be used for real-world applications and might serve as a step towards training agents that could be used for communicating with humans when collaboratively completing tasks \parencite{lazaridou2020emergent}. That is, these experiments extend the work by \cite[e.~g.,][]{havrylov2017emergence, lazaridou2016multi, andreas2016reasoning, cohn2018pragmatically}. 
Second, this thesis contributes to the body of work addressing language drift in multi-agent systems \parencite[e.~g.,]{lewis2017deal, lee2019countering, lazaridou2020multi}.The results provide additional insights regarding the factors behing language drift. For one, in contrast to \cite[e.~g.,][]{lewis2017deal, lee2019countering}, the functional (REINFORCE-based) training signal did not lead to significantly higher language drift compared to supervised (structural-only) training in the baseline reference games on either dataset. One reason for this difference might be the pretraining mode of the agents which is usually not described in detail in the literature, indicating the importance of this aspect and suggesting that language drift might partly be observed if the inference mode of the speaker model in the functional task differs from its pretraining mode. Furthermore, making the distinction between easy and difficult target-distractor pairs more explicit and conceptually motivated than \cite{lazaridou2020multi}, the similar pairs experiments in this work showed that while this perceptual similarity affects the agents' success on the reference game, it only affected the strength of language drift in the MS COCO experiment. This hints at a possible interaction between the size of the action space and perceptual complexity when it comes to language drift. The similar pairs experiment on 3Dshapes also showed that agent tend towards producing longer and, therefore, possibly more granular messages in presence of a similar distractor, showing that functional language learning might lead to more human-like language use \parencite[cf.][]{graf2016animal}.
Additionally, the novel comparison of different annotation sets in the 3Dshapes experiment showed that language drift metric values are strongly related to the length of the captions the agents are trained on and generally favor shorter captions. This comparison also showed that the reason for language drift cannot fully be attributed to the absence of exhaustive annotations in the dataset since the agents trained on both annotation sets showed comparable language drift relative to the respective pretrained speakers. This results might be due to the confound based on the length of exhaustive messages; future work could address this issue by creating artificial names for compound features and thereby keeping the annotations short (e.~g., ``glob'' referring to a tiny red block etc).
Confirming the results by \cite{lazaridou2020multi}, the experiments with fixed pretrained listeners uncovered speaker-listener co-adaptation. Consistent with literature, this co-adaptation mostly caused semantic but not structural drift \parencite[cf.][]{lee2019countering}. 
The newly tested overlap metrics did not show clear trends in the different experimental configurations. \pt{double check} This could be due to several reasons. First, the vocabularies of the agents were quite large, making metrics based on token overlaps and not accounting for semantic similarity rather instable. Second, the embedding based overlap which might address the aforementioned issue might be susceptible to vanishing gradients, such that the embedding layers are not properly trained, or, at least might not be fully trained in the short training times of these experiments. Testing this metric, e.~g., in combination with pretrained embeddings might be a promising next step. Finally, their relation to ground truth captions might be conceptually problematic in that referencing descriptive, but not discriminative, ground truth captions might not have the capacity to assess the relevance of image features which were mentioned in the disriminative message, but not in the ground truth caption. One possibility for addressing this issue in future work might be the use of multimodal embeddings which might capture relevance through low-level feature similarity \parencite[cf.][]{bruni2014multimodal}. 

%\pt{This could be the section for discussing less technical, more conceptual points regarding LD which are not discussed in last section of previous chapter.}
Another interesting direction for future work on quantifying functional drift is testing the presented system against other datasets, e.g., like the dataset constructed by \cite{mao2016generation} for testing referential expressions. To this end, a critical question would be to quantify as to how far discriminativity in a reference game aligns with quality of referential expressions. If it does, this dataset could provide a good benchmark for evaluating the functional success of the speaker. Another interesting challenge in order to connect presented work to the set up by \cite{mao2016generation} is the need to formalize the step of inferring a suitable target bounding box for the reference game (i.e., identify a discriminative referent), based on which the discriminative expression would then be generated. 
%Andreas and Klein 2016 seems highly relevant to my intuition about regularization and strucutral weight. 

These experiments also added results based on an architecture wherein the speaker agent was conditioned on both the target and the distractor, allowing for the cognitively motivated potential to condition language on visually extracted discriminative differences. A further step in order to explicate the importance of this conditioning could be the use of an attention-based speaker architecure \parencite[e.~g., akin to][]{lee2019countering}. Alternatively, the visual reasoning preceeding language generation could be made more explicit by adding, e. g., an scene parsing or object detection component to the model, which could allow for more explicit reasoning about the relevant image aspects which should be mentioned in the message \parencite[cf.][]{zhao2017pyramid}.
%Discuss relation to Lazaridous work and how I extend the speaker to be conditioned on both images in all experiments. This is more plausible from a task-functional perspective, but, of course, make the model more hard-coded (to specific set of distractors). Would be interesting to compare frozen samples from such a model (oracle speaker) to their results. I argue that the fine-tuning of the language module is more plausible in that it is motivated by trying to teach the speaker to select words that are right for the referential task. However, Asya's point is well taken in that this should not affect core linguistic capabilities which seem to be affected by co-adaptation. It remains an open question how to mitigate this issue; I think, fine-tuning the model yet using different listeners is the most plausible strategy; a completely different approach would be one to generation procedure (low-level conditioning vs. strucutred feature extraction ). 

One important assumption made in the reference game set up is that the agents are \emph{cooperative} because they receive rewards based on their joint task performance. Therefore, language use which can be studied in such a task could also be considered cooperative. However, human language use is by far not always cooperative, as is the case in, e.~g., lying \parencite{franke2020strategies}. Training agents to communicate in natural language in such an environment would contribute to work on competing and self-interested agents \parencite[cf.][]{lazaridou2020emergent} and might present an interesting avenue for investigating language drift which might, e.~g., be used by the agents as a strategic device for competitive advantage. 

Overall, results of the presented experiments draw a rather comprehensive picture of the interaction between different architectural and dataset-related configurations of a reference game and their effects of language drift, while including diverse baselines allowing for better interpretability of the results. 


\section{Cognitive Perspective}
The presented experiments took a step towards the long-standing goal of teaching artificial agent pragmatic language use. The focus of this work was the ability to adjust descriptive messages for images to the needs of the discriminative needs of the given context, which amounts to learning to generate messages including context-sensitive referential expressions. A critical assumption made in this work is that the discriminative aspects of the context are visual, i.e., the target and distractor are images with different content, and attending to those differences makes the referential messages discriminative. In the used architecture, visual context is represented in a \emph{sub}symbolic way via pretrained image vectors, and, importantly, the representation is \emph{not} subject to fine-tuning. However, an substantial body of work in cognitive science argues that \emph{structured} model components might be necessary for human-like reasoning \parencite[e.~g.,][]{tenenbaum2011grow, lake2017building}. For instance, \cite{monroe2015learning} make a steps towards combining Bayesian RSA components with neural models.
Similarly, a stream of work proposes that learning different reasoning skills, including language, might be conceptualized as program induction rather than learning subsymbolic neural representations. For instance, \cite{nye2020learning} propose a neuro-symbolic system which outperforms neural meta-learning systems on tasks like instruction following by inducing generalization rules on a small set of examples. \cite{wong2022identifying} show that human image description vocabularies can be optimally represented by program libraries; \cite{lake2015human} show that a probabilistic program induction model achieves human-level performance when learning concepts from few visual examples. This also relates to a common concern that deep models generally require colossal amounts of training data in order to achieve human-level performance. Applied to linguistic abilities, this stands in contrast to children which learn language from only few examples \parencite[e.~g.,][]{xu2007word}. This also raises environmental concerns regarding the costs of building deep learning models \parencite{bender2021dangers}.  
%Discriminativity vs reference vs relevance. Level of discriminativity extraction (as related to future directions for different agent architectures). Multi-turn interactions. Testing against humans. Other pragmatic abilities. 
Another a critical aspect which should not be forgetten when building deep learning models is that, besides potentially learning reasoning abilities, they replicate the statistics of the datasets they are trained on, replicating potential biases in those datasets, respectively, which may have serious societal consequences when such models are deployed (\cite{bender2021dangers}, \cite{buolamwini2018gender}). 

This thesis focused on modeling a specific aspect of pragmatic reasoning in one task, yet human pragmatic abilities extend to many more linguistic phenomena beyond context-sensitive referential expression generation, including the use of gradable adjectives \parencite{qing2014gradable}, indirect lanaguge \parencite{yoon2016talking} and \pt{add something...}.
Furthermore, human language use is not only grounded in the environment, but also supplemented by a vast background of experience, world knowledge and individual beliefs \parencite{lake2017building, franke2016does}, supporting efficient reasoning and learning. There is an increasing interest in enhancing neural language generation systems both with symbolic reasoning components based on structured world knowledge \parencite[e.~g.,][]{nye2021improving}, and leveraging large neural language models as sources for world knowledge themselves \parencite{petroni2019language}. However, this aspect has not received much attention in the multi-agent language emergence and learning domain yet. 
All these phenomena present an exciting avenue for further pushing the boundaries of multi-agent communication in order to train neural agents to communicate in a more human-like way. 

\section{Conclusion}
%...hinting at the fragility of extant approaches to building artificial agents. Therefore, there might be a lot of potential for improvement in attempting to build cognitively-inspired / plausible architectures. This thesis hopes to inspire such future work. 