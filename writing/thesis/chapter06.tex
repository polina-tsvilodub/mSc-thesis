\pt{Discussion goes here. So far, only some brainstorming notes can be found below.}

Competition vs cooperation of agents. 

My intuition that the state space is too large to observe any influence of REINFORCE is supported by marginal comments in \cite{havrylov2017emergence}; therefore, future work should try a combination of the structural loss with a Gumbel-Softmax component.

My architecture, esp for MS COCO, can essentially be interpreted as just show-casing that you can ground a second (receiver) agent against a pretrained image captioner. It is hard to consider this a multi-agent communication result since the effect or reinforce propagating the functional signal is so marginal. 

Discuss how this multi-agent communication is one approach to building pragmatic language behaviour based on neural models. But there are other options with a more explicit underlying cognitive motivation, as e.g. done by Monroe and Potts, but which faces computational challenges. 

Discuss relation to Lazaridous work and how I extend the speaker to be conditioned on both images in all experiments. This is more plausible from a task-functional perspective, but, of course, make the model more hard-coded (to specific set of distractors). Would be interesting to compare frozen samples from such a model (oracle speaker) to their results. I argue that the fine-tuning of the language module is more plausible in that it is motivated by trying to teach the speaker to select words that are right for the referential task. However, Asya's point is well taken in that this should not affect core linguistic capabilities which seem to be affected by co-adaptation. It remains an open question how to mitigate this issue; I think, fine-tuning the model yet using different listeners is the most plausible strategy; a completely different approach would be one to generation procedure (low-level conditioning vs. strucutred feature extraction ). 

Andreas and Klein 2016 seems highly relevant to my intuition about regularization and strucutral weight. 

Potential other future investigation: using pretrained embeddings.
Also different pretrianing strategies, potentially start with auto-regression and then transition to teacher forcing. 

Overall, we see how important the combination of particular hyperparameters and training configurations are, indicating that future research should focus on architectures which might be more stable against the alternations. 

Conditioning of the sepaker: during pretraining it already saw both images (necessary for architectural reasons), but we cannot know if it did not result in an implicit spill over and discriminativity already during pretraining. also this makes the model task specific (pairs of images only). 

An interesting direction for future research is also testing the present system against other datasets, e.g., for testing referential expressions like the dataset constructed by \cite{mao2016generation}. An interesting question there would be to quantify inhowfar discriminativity in a reference game aligns with good referential expressions; if it does, this dataset could provide a good benchmark for evaluating the functional success of the speaker. In order to connect to their work, one would need to to formalize the step of inferring a suitable target region or bounding box for the reference game (i.e., identify a discriminative referent), and then generate the discriminative expression. 

Beam search in decoding.
\pt{maybe structure discussion according to: summary, experimental methods, conceptual aspects, relation to extant work and future directions, conclusion.}