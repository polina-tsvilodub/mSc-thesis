This chapter introduces the notion of language drift and reviews work investigating this phenomenon. Literature often distinguishes between \textit{syntactic} (also sometimes called \textit{structural}), \textit{semantic} and \textit{pragmatic} drift \parencite{lazaridou2020multi}. \pt{TODO: add some relation or examples from natural language dynamics, cf. \cite{jacob2021multitasking} p. 3.}

First, prior work addressing language drift is reviewed. Second, metrics aiming to capture these different kinds of drift which are also used in presented experiments are reviewed. Then, novel aspects in investigating language drift tested in this thesis are described. Finally, specific hypotheses regarding expected language drift in the presently conducted experiments are presented along with their operationalizations. 

\section{Mitigating Language Drift}

\pt{Images with examples of diversely drifting captions will be added.}
The phenomenon of \textit{language drift} was first detected by \cite{lewis2017deal} who trained artificial agents to cooperate on a negotation task in natural language, given a corpus of human dialogues. They showed that the negotiation skills were significantly improved by optimizing the pretrained sequence-to-sequence agents with a preformance-based reward via REINFORCE. However, this came at a cost of divergence from human language---i.e., the human intelligibility of the communication produced by the agents drastically decreased. This phenomenon is called language drift. To combat that, they switched between reinforcment learning and supervised learning. However, no precise quantification of the drift was presented. 

Building upon this work, \cite{lee2019countering} observed that counteracting drift by imposing a supervised learning constraint on the produced language, i.e., by trying to maximize the likelihood of the communication under a pretrained language model, mainly the \textit{surface structure} of the learned language was preserved. However, there was no guarantee that the \textit{semantics} were not drifting. That is, there was no constraint for the model to mean a cat with the word ``cat''; it could instead attach a different meaning to it.  To address this issue, they suggested to ground the language usage task in visual data, the idea being that by co-occuring with images, the words kept their semantics. As described in Section \ref{mac}, they cast a translation task into a multi-agent task wherein the agents communicate via the pivot language English which was used for investigating drift. It was also used to compare the efficacy of grounding to a direct lanuage-model likelihood regularization, as performed by \cite{lewis2017deal}. The visual model they used for grounding was an image-caption retrieval model, determining the likelihood of the target image under the produced message, which is similar to the listener component of the noisy channel model by \cite{lazaridou2020multi}.
They first estimated language drift within the evaluation of the overall translation quality by reporting BLEU scores (see Section \ref{image_cap_metrics}). They observed a drop in French-English translations in the vanilla model when the scores for the French-German performance increased. The highest scores were achieved with the model containing both constraints, both on French-Eglish and the French-German translation. LM regularization has shown better improvement of the English scores, though this was partly due to the preference of the BLEU score for the surface form which was better regulated by an LM. Yet combining the LM and the image retrieval model yielded the best results, indicating that the drift may also occur at semantic level, which was mitigated by grounding. Second, they looked at by part-of-speech recall at inference time, finding that the vanilla model had difficulties with function words as well as produced in a flatter token distribution, compared to the combined regularized model. Further, the vanilla model was more prone to repeating words. To sum up, their results hint that both syntactic and semantic drift take place when optimizing agents with external task rewards. However, they captured the drift by task-specific metrics (i.e., translation BLEU scores) which might make it difficult to extend their diagnostics to other experiments.

\pt{My paper should be included here at least somehow.}

A different approach to mitigating language drift, rather focusing on stabilizing the language by creating community pressures on the agents, was taken by \cite{lu2020countering}. They tested the so-called \textit{seeded iterated learning} approach whereby they periodically finetuned the pretrained speaker agent to imitate the behaviour of a teacher agent finetuned on the task. It was motivated by iterated learning models which are prominent in research on emergence and evolution of language structure. More precisely, the teacher agent was initially a duplicate of the learning speaker agent which essentially provided the \textit{seed} for the supervision iterations. The imitation sequences consisted of supervised training on data sampled from the fnetuned teacher agent. They tested this approach on a reference game and the translation game from \cite{lee2019countering}. The success of countering the drift in the reference game was measured via the ``Sender Language Score'' and the ``Task Scores'' (p. 5). The former compared the generated and ground-truth captions by-token. The latter was the referential accuracy. The SIL model was shown to outperfrom several baselines.
For the latter task it was measured via BLEU scores, negative log likelihood of the messages (NLL, for capturing structural drift) and ranking scores under a pretrained image ranker (for capturing semantic drift). The SIL model was shown to be more robust against BLEU, ranking and NLL decreases than baselines.
%--  evolutionary / community / stabilisation via listener variation / against coadaptation.

Work on language drift closely related to \cite{lazaridou2020multi} was conducted by \cite{lowe2020interaction}. They investigate the effects of different approaches to combining task based learning (the mode they call ``self-play'', p. 1) and supervised learning of the natural language communication protocol. Essentially they also investigate a functional and structural loss and their potential combinations. %, with the goal of optimally training agents to use well-formed natural language in communication tasks. 
More specifically, they compare different orders of alternating between self-play and supervised learning, possibly freezing the speaker, as well as population based training. For the latter, they train a population of speakers and then combine them back into one via policy distillation. They also compare randomized alternations between the two training modes to scheduled training. 
Critically, they do not consider approaches where both learning signals are present simultaneously, as is done by \cite{lazaridou2020multi} and in presented work.
Tey find that better task performance can be achieved when first training the agents in the supervised mode, before fine-tuning them with self-play. Additionally, they find that population based training outperforms all other set-ups, although an ensemble method wherein a population of different agents are used for maing predictions improved on the distilled policy population based agent. Overall, theay argue that the training set up is an important factor heavily influencing the number of data points required for training the agents, and argue that the two training modes essentially act as competing constraints. Alternating training allows to approximate a behaviour step-by-step which satidfies both constraints.  

Last but not least, \cite{jacob2021multitasking} studied semantic drift of the so-called \textit{latent language policies} (LLP) which are used to train instructor and executor agent pairs. Semantic drift refers to the phenomenon whereby instuctors use messages in ways inconsistent with their initial semantics. Applied to the signalling game setting, they proposed an executor (i.e., speaker) which simulataneously received reward functions based on two different tasks and two different executors (i.e., listeners), respectively. Yet it remains a task for future research to investigate if this approach can be applied to training agents in the reference game setting with natural language.

The approaches outlined above tested specific architectural constraints tailored towards reducing language drift. The goal of this thesis, on the other hand, is to take one step back and first effectively \textit{measure} language drift within a given architecture in order to \textit{explain} its possible sources, before making some recommendations about potential architecture improvements. Therefore, the next section summarizes existing and newly applied language drift metrics which are employed in own experiments.  

\section{Measuring Language Drift}

Some language drift metrics have been identified in the reviewed work. In particular, \cite{lazaridou2020multi} identified several measurements which are adopted in this thesis. These can be summarized as follows. 
\begin{itemize}
	\item \textit{Structural}, or, syntactic language drift can be measured as the log probability $P(m)$ of the generated message $m$ under a pre-trained unconditional language model. In this thesis, the pretrained TransformerXL model accessed through the \texttt{huggingface} library is used \parencite{dai2019transformer, wolf2019huggingface}.
	\item \textit{Semantic} language drift can be measured as the conditional log probability $P(m|i)$ of the generated message $m$ given the image $i$ under a pretrained image captioning model. In this thesis, the pretrained speaker models are used for this computation. %Another measure includes the $n$-gram overlap of generated messages and the ground-truth captions (ignoring stopwords) \cite{lazaridou2020multi}. Semantic drift is also addressed by \cite{lee2019countering}, \parencite{} but their approaches rather propose specific training methods than measures for identifying language drift, so their proposals wouldn't be considered here.
%	In alternative framings, semantic drift has been measured as the difference between the message semantics and the action taken by the receiver agent \cite{}. lu2020countering.
	\item Finally, \textit{pragmatic} language drift is assessed by \cite{lazaridou2020multi} as the discrepancy in referential success between human and listener agents in absense of structural or semantic drift. This is assessed by comparing the performance of humans to the performance of trained listener agents with a speaker trained to rerank the ground truth image captions (see Chapter \ref{chapter03} for details). 
\end{itemize}

However, given that the conducted experiments don't have access to human baseline data, pragamtic drift will have to be assessed differently. Although the proposed approach has the advantage of being task-agnostic, this work proposes to focus on a referential task drift approximating the pragmatic one. This approximation is referred to as \textit{functional} drift. 

In this context, funtional drift refers to the deterioration of language which would make the referential task impossible for humans---for instance, leaving out critical content words. 
In contrast, other kinds of drift like structural drift might involve mixing up the word order, which nevertheless does not necessarily hinder the referential task, if distinctive content words are still present. \pt{TODO: example images will be added}%For instance, the caption ``A plate food with'' would exemplify functional drift, while the caption ``A plate red food with'' wouldn't, if the target image and Fig.2 was the distractor. Check is this can be exemplified with some images already used elsewhere or at intro of the chapter.} 
Crucially, the goal is to capture this drift in absense of human experiments. The difference between the proposed metric and pragmatic drift would be that functional drift is proposed in terms of the presense of discriminative words in the discription, which can be approximated as words or caption parts which have a higher probability for the target image than for the distractor. Thereby the metric becomes operationalizable under open-source pretrained models and does not depend on the availability of human data anymore. 

The following concrete operationalizations are tested in the experiments in order to capture functional drift. \begin{itemize}
	\item One approach for identifying functional language drift which is stable against compositional alternations within the caption and, therefore, isolates functional discriminativity, is the word overlap between the generated captions and the target vs. distractor ground truth captions, respectively. From a functional perspective, an optimal generated target caption maximizes the overlap with the target ground truth, while minimizing the overlap with the distractor ground truth. This idea is related to the omission score suggested by \cite{havrylov2017emergence} (also cf. \cite{andreas2016reasoning, gunel2020supervised}). This is also similar to the unigram metric employed by \cite{lazaridou2020multi}, but it adds the functional aspect via the difference computation. To sum up, the difference between the discrete word overlap of the target and generated captions and the overlap of the distractor and generated captions is computed. This is an intuitive step to take since ground truth captions are available for all images in the dataset. This metric is called \textit{discrete overlap} in the subsequent chapters.
	\item Complementarily, the idea described above is also formalized by computing the cosine similarity between the caption embeddings instead of word overlap scores. This also hints at whether the trained embedding layer of the speaker, i.e., a representational layer, is affected by the functional learning signal. Again, the respective difference is computed as the drift metric and is expected to increase with successful task learning. This metric is called \textit{continuous overlap}.
	\item \pt{Depending on time constraints, this might be dropped altogether.} Finally, a rather exploratory approach is taken to measuring the recoverability of the target image based on the caption. Recoverability here is used in the loose sense of being able to reconstruct the intended image given the caption. Again, discriminative captions would present higher recoverability of the target comapred to the distractor. \pt{Either of two following approaches might be used, depending on what I might get to work} This is operationalized via a pretrained image-text retrieval model. Alternatively, this can be operationalized via a text-to-image model, where, ideally, the image produced from the generated caption would be more similar to the target than the distractor image. However, these metrics both heavily depend on the nature of the pretrained models as well as in the chosen image similarity metric. This metric is rather used in an exploratory way, in order to investigate if it alignes with intuitions and other metrics.
\end{itemize}
These metrics are used as diagnostic measures in order to identify potential sources of language drift. To this end, different experiments are conduceted. The section below outlines the hypothesized language drift behavior in the different experiments, as can be expected based on results from the literature.

\section{Hypotheses}

For simplicity of reference in the discussion of results, the different hypotheses are enumerated (note that these are rather research questions and not hypotheses in a strictly statistical sense).
First, hypotheses regarding language drift in the main experiments on the MS COCO dataset are described. 

As the speaker improves on the functional task, i.e., as the functional loss is minimized, the following observations are hypothesized: \\
\newline 
% The speaker vocab size relation also to previous work 
\textbf{H1:} The structural drift is expected to increase. That is, the log probability of the generated captions under a pretrained model is expected to decrease. \newline
\textbf{H2:} The semantic drift is expected to increase. That is, the log probability of the generated caption given the target under the pretrained frozen speaker model is expected to decrease. \newline
\textbf{H3:} The discriminativity of the captions is expected to increase. That is, the word overlap difference is expected to increase, both for the discrete and the continuous overlap metric. \newline
\textbf{H4:} Given that the pressure to stay close to a natural language distribution is provided by the structural loss component in the chosen model architecture, it is expected that both the structural and the semantic drifts increase as the weight of the structural loss component decreases. To investigate this, four reference games with the strctural loss weights $\lambda_s = [0, 0.25, 0.5, 0.75, 1]$ are conducted. The functional loss component is computed as $\lambda_f = 1 - \lambda_s$, respectively. \newline
%Based on exploratory experiments (see Appendix \ref{appendix}) and on literature regarding the learnability of large action spaces by REINFORCE, 
\textbf{H5:} The discriminativity of the captions is expected to increase less in the similar pairs experiment than in the baseline experiment on random target-distractor pairs. That is, the overlap values are expected to be smaller than in the baseline experiment. This is expected due to the higher perceptual difficulty of discriminating images depicting similar things. Due to the intuitive necessity to produce more specific messages in the similar pairs experiment, it is also expected that the message length and possibly the specificity of the words increases. This will be approximated by analysing the occuring parts of speech and message lengths until the first END token, possibly accompanied by manual sample message inspection.\newline
\textbf{H6:}  The semantic and structural drifts are expected to be smaller in experiments where in the speaker is trained against a fixed (i.e., pretrained) listener compared to a listener trained jointly with the speaker. This is due to the observation made in prior work that especially semantic drift arises due to speaker-listener co-adaptation and the emergence of conventions among them. \newline
\textbf{H7*\footnote{The * indicated the optionality of this hypothesis based on available time}:} \pt{MS COCO vocab size variation to look at functional convergence speed, if there is time. It is more about the functional optimization potential than language drift though.} \newline
\textbf{H8*:} The recoverability of the target given the sampled caption is expected to increase with training. That is, the metric provided by the image-text retrieval model is expected to improve. \pt{if time allows} \newline

In order to investigate the influence of the characteristics of the dataset on the drift, experiments with the manually annotated 3Dshapes dataset are conducted. Hypotheses \textbf{H1--H6} are also tested on 3Dshapes, but additional hypotheses about the comparison between MS COCO and 3Dshapes are explored. \\
\newline
\textbf{H9:} The relative strucutral drift is expected to be smaller compared to the MS COCO experiments. That is, the message log probability under the pretrained language model is expected to be higher. This is expected due to the a priori given availability of exhasutively descriptive features in the training data. This availability is expected to alleviate the need of resorting to structurally misformed messages in order to adapt to the functional needs for the agents. \newline
%Message length, grammatical structure. 
%Granularity in random vs similar pairs.
%Again Ls, difference between joint and fixed listener.
\textbf{H10:} To deconfound the role of the availability of fully exhaustive descriptive captions and the size of the action space learned by the agent, an experiment is conducted wherein the speaker is pretrained on non-exhaustive captions. Comparing this experiment to the baseline 3Dshapes experiment, higher structural drift is expected, as operationalized by lower caption log likelihoods of the messages under the pretrained model.

It is important to note that absolute values of the presented drift metrics should be interpreted with caution. Especially the structural drift metric is based on a model pretrained on possibly very different data distributions. Therefore, absolute log likelihood might be an artifact due to differences between the reference games data and pretraining data. Therefore, the metrics are intended to be interpreted comparatively within the training dynamics of the experiments, or between experiments with varying training configurations.

The next chapter finally turns to experiments in scope of which language drift using discussed metrics is investigated. 