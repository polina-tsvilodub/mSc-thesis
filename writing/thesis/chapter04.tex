This chapter introduces the notion of language drift and reviews work investigating this phenomenon, with a special focus on language drift in multi-agent communication. Language drift phenomena are distinguished between syntactic, semantic and pragmatic \pt{check how to introduce structural naming here; if i were to suggest some functional drift, mention here too} Then, it introduces the approach and tehe metrics used for investigating language drift in this thesis.  

\section{Prior Work}

\pt{maybe add images with examples of diverse drift}
The phenomenon of \textit{language drift} was first detected by \cite{lewis2017deal} who train artificial agents to cooperate on a negotation task in natural language, given a corpus of human dialogues. They show that the negotiation skills are significantly improved by optimizing the pretrained sequence-to-sequence agents with a preformance-based reward via REINFORCE. However, this comes at a cost of divergence from human language---i.e., the human intelligibility of the communication produced by the agents drastically decreases. This phenomenon is called language drift. To combat that, they switch between reinforcment learning and supervised learning. However, no precise quantification of the drift is presented. 

Building upon this work, \cite{lee2019countering} observe that counteracting drift by imposing a supervised learning constraint on the produced language, i.e., by trying to maximize the likelihood of the communication under a pretrained language model, mainly the \textit{surface structure} of the learned language is preserved. However, there is no guarantee that the \textit{semantics} are not drifting. That is, there is no constraint for the model to mean a cat with the word ``cat''; it could instead attach a different meaning to it.  To address this issue, they suggest to ground the language usage task in visual data, the idea being that the by co-occuring with images, the words keep the semantics. As described in Section \ref{mac}, they cast a translation task into a multi-agent task wherein the agents communicate via the pivot language English which is used for investigating drift. It is also used to compare the efficacy of grounding to a direct lanuage-model likelihood regularization, as performed by \cite{lewis2017deal}. \pt{check if regularizer description should be ported over here.}. Interestingly, the visual model they use for grounding is an image-caption retrieval model, determining the likelihood of the target image under the produced message, in  contrast to the grounding model by \cite{lazaridou2020emergent} where the likelihood of the message given the image is maximized.
They first estimate language drift within the evaluation of the overall translation quality by reporting BLEU scores. The observe a drop in French-Englis translations in the vanilla model when the scores for the French-German performance increase. The highest scores were achieved with the model containing both constraints both on French-Eglish translation and the French-German translation. LM regularization has shown better improvement of the English scores, though this is partly due to the preference of the BLEU score for the surface form which is better regulated by an LM. Yet combining the LM and the image retrieval model yielded the best results, indicating that the drift may also occur at semantic level which is mitigated by grounding. Second, they look at by part-of-speech recall at inference time, finding that the vanilla model has difficulties with function words as well as produces in a flat toke distribution, compared to the combined regularized model. Further, the vanilla model is proner tp repeating words. To sum up, their results hint that both syntactic and semantic drift take place when optimizing agents with external task reward. However, they capture it by task-specific metrics (i.e., translation BLEU scores) which might make it difficult to extend their diagnostics to other experiments.

A different approach to mitigating language drift, focusing rather on stabilizing the language by 

jacob, lewis, andreas, 2021: multitasking inhibits drift )

Lu 2020 iterated seed learning --> evolutionary / community / stabilisation via listener variation / against coadaptation.


\pt{from proposal below.}
Existing language drift metrics include:
\begin{enumerate}
	\item Structural / syntactic language drift: log probability of the generated message $m$ under a pre-trained unconditional language model $P(m)$ \cite{lazaridou2020multi}
	\item Semantic language drift: conditional log probability $P(m|i)$ of the generated message $m$ given the image $i$. Another measure includes the n-gram overlap of generated messages and the ground-truth captions (ignoring stopwords) \cite{lazaridou2020multi}. Semantic drift is also addressed by \cite{lee2019countering}, \parencite{} but their approaches rather propose specific training methods than measures for identifying language drift, so their proposals wouldn't be considered here.
	In alternative framings, semantic drift has been measured as the difference between the message semantics and the action taken by the receiver agent \cite{}. lu2020countering.
	\item Pragmatic language drift: \cite{lazaridou2020multi} assess this drift as referential failure in absense of structural or semantic drift by comparing human and listener agent referential success given a model which only reranks ground-truth captions. Given that the currently proposed experiment won't have human data, this kind of drift will have to be assessed differently. Although the proposed approach has the advantage of being task-agnostic, in this work, I would propose to focus on a referential task drift which I referred to as \textit{functional} drift. 
\end{enumerate}
Similar to the experimental models,first, MVPs of the models for language drift analysis would be implemented and tested, before training them full-scale.



\section{Measuring Language Drift}

A novel metric would focus on evaluating both structural and \textbf{functional drift} of the produced expressions. In this context, funtional drift would refer to the deterioration of language which would make the referential task impossible for humans (e.g., leaving out critical content words). Structural drift, in contrast, might involve mixing up the word order, which nevertheless wouldn't hinder the referential task, if distinctive content words are still present. For instance, the caption ``A plate food with'' would exemplify functional drift, while the caption ``A plate red food with'' wouldn't, if was the target image and Fig.2 was the distractor. 

Different approaches to developing such a metric could be taken in the thesis. 
\begin{enumerate}
	\item One idea for identifying functional language drift which would also be stable against compositional alternations within the caption would be to compute the word overlap between the generated captions and the target and distractor ground truth captions, respectively. From a functional perspective, an optimal generated target caption would maximize the overlap with the target ground truth, while minimizing the overlap with the distractor ground truth. This idea is related to the omission score suggested by \cite{havrylov2017emergence} (cf. \cite{andreas2016reasoning, gunel2020supervised}).
	\item Alternatively, the idea described above could be formalized by computing the cosine similarity between the caption embeddings instead of word overlap scores. 
	\item A more structured approach to identifying whether critical discrimnative components have been captured in the caption might be to consider the labeled objects in the two images, identify the ones that are only present in the target image, and perform image patch to caption alignment, as proposed by \cite{karpathy2015deep}. Higher alignment scores would indicate more discriminative captions. However, the availability of the model and the usefulness of the approach are not completely clear, as the similarity of images even within MS Coco Captions categories might be quite variable.
	\item Finally we discussed the idea to measure the drift as the similarity between the original image and an image generated by a pretrained text-to-image model given the generated caption. Some text-to-image architectures include DALL-E, StackGAN++ or other models \cite{ramesh2021zero, zhang2018stackgan++, zhou2021lafite}. However, it seems rather difficult to find models, pretrained on our dataset. Example sources for other datasets might be \url{https://tinyurl.com/y4pz7ymz} or \url{https://github.com/ShanHaoYu/Text2Image}. Although comprehensive guides to training these models exist, computational demands for this task might be rather cosmic.
	Furthermore, a similarity metric for the original and generated image would be necessary. One could potentially compare embeddings of these images extracted by the ResNet module via cosine similarity, but the interpretability of such a comparison might be unclear.  
	\item Based on this idea, a different approach to training could be considered, whereby an image could be generated from the ground truth caption as an intermediate representation (cf. \cite{lee2019countering}). Specifics of the architecture would have to be determined, provided the availability of a text-to-image model.  
\end{enumerate}
If feasible all these approaches could be tested in similar way as the exisiting metrics (i.e., during training and on the test split), and compared with respect to their accuracy and functional adequacy by manual inspection of caption samples. 