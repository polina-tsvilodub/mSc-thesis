
This chapter reviews related work on multi-agent communication---the core domain in which the presented experiments are situated. First, different architectural architectures used for building multi-agent communication experiments are reviewed. Then, different tasks the agents are trained on are summarized, focusing on the reference game setting. 
%Finally, the paper by \cite{lazaridou2020multi} which serves as the starting point for this thesis is summarized in detail.  

\pt{maybe find a way to say that there are other neighboring areas of research like visual dialogue, multimodal semantics, visual question answering etc. possibly in discussion.}

\section{Multi-Agent Communication}
Research on multi-agent communication has received increasing attention over the recent years, as this domain focuses on research questions of high relevance to cognitive science \parencite{lazaridou2020emergent}. More precisely, two main areas of research are often addressed: first, the conditions under which a language emerges and which emergent properties can be observed; and second, the ability of the agents to pick up a given language system. % thus investigating whether tractable human-agent communication channels can be learned via realistic interaction.
The second direction has also been considered with respect to \textit{grounding} a given language in the visual world. That is, approaches to teaching agents to use language applied to realistic visual input are investigated. Grounding is important both from modeling and cognitive perspectives: for instance, \cite{bruni2014multimodal} show that representations learned from both text and image data are more plausible than purely text-based representations; furthermore, vision is an integral part of human communication \parencite{tomasello2010origins, harnad1990symbol}. 

These two research directions have different implications for understanding human language: the former often focuses on what the causal factors behind prominent features of human language like compositionality are, allowing to shed light on human language evolution; the second rather addresses how we can train agents to communicate optimally for interacting with humans. Equivalently, these two main directions can be classified as investigating \textit{language emergence} and \textit{language acquisition} among and by artificial agents \parencite{lazaridou2018emergence, lazaridou2020emergent}.
Presented work focuses on the latter question, aiming to train agents to use English language grounded in real-world images.

Irrespectively of the research focus, artifical agents which interact with each other are at the core of multi-agent communication studies. Therefore, first, different agent architectures are presented in the next section.

\pt{I feel like a section on super general aspects, like having two agents doing some task and being jointly trained, is missing. Maybe visuals would be cool.}

\subsection{Agent Architecture and Training}

The main architectural differences concern the type of model representing the artificial agent and the type of communication channel they use.
As described in Section \ref{rl}, experiments wherein agents learn to complete a task by interacting with the environment and with each other are typically trained with reinforcement learning. Multi-agent communication is a specific type of such experiments in that (one of) the tasks agents learn are communicative and cooperative, and one of the agents can be considered the sender and the other the receiver of the communication \parencite[cf.][]{tan1993multi, lazaridou2016multi}.
Based on the chosen communication channel and the precise communicative task, specific architectural decisions and optimization algorithms differ. 

While early work in multi-agent communication focuses on structured agents \parencite[e. g., see][for reviews]{christiansen2003language, cangelosi2002symbol}, deep agent architectures became increasingly dominant in the domain over the last years \parencite{lazaridou2020emergent}. As this thesis also uses deep neural network agents, the review of related work focuses on deep multi-agent communication experiments. 

%Architectural considerations: 
%1) agent architecture: hand crafted vs deep; for deep: kind of network, esp. recurrent layer
%2) loss and training approach: Q-learning \parencite{foerster2016learning}, Gumbel-Softmax \parencite{havrylov2017emergence}, REINFORCE.  \parencite{lazaridou2020emergent, williams1992simple}. 

\cite{lazaridou2016multi} first show that neural agents can efficiently reference realistic visual inputs in the presence of distractors by developing a communication protocol from scratch. To this end, \textcite{lazaridou2016multi} conduct several experiments employing feed-forward and CNN agents. In particular, in their main experiment, they set up a reference game, where a sender agent emits a single discrete symbol referring to one of two images provided to both the sender and the receiver, sampled from the ImageNet dataset \parencite{deng2009imagenet}. The receiver agent has to guess  which image is the intended target, based on the symbol received from the sender (see Section \ref{reference_games} for details). %They use a subset of the dataset containing 100 images from each of 20 general categories.
\cite{lazaridou2016multi} set up two versions of the sender. The ``agnostic sender'' consists of a two-layer feed forward neural network, embedding both images and producing a score over the vocabulary given the concatenation of image embeddings \parencite[][p. 3]{lazaridou2016multi}. The ``informed sender'' consists of a three-layer neural network, applying two convolutional layers to embeddings of both images which are treated as different channels \parencite[][p. 3]{lazaridou2016multi}. It also outputs scores over the vocabulary. For both architecures the scores are converted into a Gibbs distribution from which the emitted message symbol is sampled. The receiver agent consists if a two-layer feed forward network which embeds both images and computes the dot products between these embeddings and the one-hot encoded message. The final target image choice is sampled from the Gibbs distribution computed from the dot products. %The agents are trained with REINFORCE which updates parameters of the speaker and listener policies by minimizing the negative expected reward. The speaker and listener policies are parametrised by the weights of the respective neural networks. The sender policy is $\pi(v \in V \mid i_L, i_R, t \in \{L, R\})$ where $V$ is the vocabulary, $i_L, i_R$ are the left and right images and $t$ is the position of the target. The receiver policy is $\pi(t \in \{L, R\}\mid v_s, i_L, i_R)$, where $v_s$ is the message received from the sender and $t$ is the guessed position of the target image. \textcite{lazaridou2016multi} set the reward to 1 if the image picked by the receiver is the target and 0 otherwise. 
They found that agents achieve high referential success, and that the informed sender produces semantically more natural symbols (i.e., the same ones for the same image categories) than the agnostic one. In a further experiment, the authors successfully pressure the agents to communicate about more high-level properties by sampling instances of different subcategories within the same basic-level category for the image pairs.%, and achieve a small increase in label purity (i. e., in the proportion of labels agreeing with the major cluster label). 
Finally, they perform an experiment grounding the communication in natural language wherein the sender agent has to use natural language category labels as message symbols. The sender is trained by switching between reference game play and image classification on ImageNet data. However, the vocabulary is limited to 100 words, and the sender only produces one-word messages. %They observe a higher symbol purity in this experiment, while retaining referential success. 	
%	\item They use REINFORCE for training the speaker. Overall framing: interactive communicative setting for training agents allows to capture functional aspects of communication, i.e., the goal-directed nature of communication, as opposed to purely supervised conversational agent training. 
%	\item \textit{multi-agent coordination communication game}: first functional task (a basic function of language): reference
	
Another foundational study in deep multi-agent communication was conducted by \cite{foerster2016learning}. In contrast to \cite{lazaridou2016multi}, the authors let the agents use a centrally trained system in their ``DIAL'' architecture (i. e., with parameter sharing across agents)  which can also be extended to continuous communication. This system is compared to the ``RIAL'' system where agents use traditional discrete communication \parencite[][p. 2]{foerster2016learning}. This is one of the first studies using a recurrent network in the sender architecture.
For both systems, the sender consists of an input network with a fully connected layer and lookup tables creating embeddings of the task environment, two recurrent GRU layers, and two feed-forward output layers.
The receiver has the same architecture. The DIAL agents are connected via a continuous vector which can be seen as an activation layer passing the internal state of the sender to the receiver.
Experiments with different variants of a riddle task and an image classification task on the MNIST dataset were conducted. Results on the former task show that the agents were able to learn an optimal policy although DIAL agents converged faster, yet sharing the weights between the sender and receiver agents was critical for RIAL. DIAL with parameter sharing outperformed other experiments. Furthermore, they found that adding noise to the communication was critical for successful learning for RIAL. Results of the second task provided similar results for DIAL, while RIAL failed to converge stably on the multi-step interaction version of this task. 
The discrete system is trained with deep $Q$-learning (see Section \ref{rl_methods}), while the continuous system was trained by additionally backpropagating gradients end-to-end between the two agents. %Furthermore, the former but not the latter case includes modeling wherein agents have access to each other's internal representations of the environment, which was also argued to be unnatural when developing a system to investigate human communication \pt{REF}. 
The DIAL results pose an interesting question regarding the comparability of such a system where the internal representations of a speaker are directly passed to the listener to human communication \parencite[cf.][]{lazaridou2020emergent, hockett1960origin}. The authors, however, argue that this direct error propagation can be interpreted as communicative feedback which is also part of human communication.

\cite{havrylov2017emergence} further extend work on multi-agent communication by modeling agents which  communicate with variable-length strings of symbols in a reference game. 
Their architecture is based on the proposal \cite{lazaridou2016multi} with the difference that the sender agent only has access to the target image. Furthermore, their agents consist of LSTM layers. Information about the target image is injected by initializing the hidden state of the sender LSTM with the image feature vector, extracted from a pretrained VGG CNN. The sender generates the message by sampling tokens from the vocabulary similarly to the image captioning system by \cite{vinyals2015show} described in Section \ref{image_captioning}. The vocabulary consists of 10,000 tokens, the LSTM hidden size is 512, and the token embeddings are 256-dimensional. 
The receiver interprets the message by encoding it with the LSTM and computing a probability distribution over dot products of image features and the affine-transformed hidden state of the LSTM. The image with the highest probability is chosen as the target. \cite{havrylov2017emergence} further attempt to make the statistical properties of the emergent protocol as similar to natural language as possible by minimizing the KL-divergence between the learned conditional token distribution and a pretrained language model.
They compare agents trained via the straight-through Gumbel-softmax estimator to agents trained with REINFORCE. The straight-through Gumbel-softmax estimator is a differentiable approximation of discrete actions (i. e., tokens) with a continuous relaxation applied in the backward pass of the backpropagation, while still using discrete representations in the forward pass.
That is, one-hot encoded tokens are replaced with samples $w_k$ obtained by sampling $K$ samples  $\{u_k\}_{k=1}^K$ from the random variable $u \sim U(0,1)$. Each $u_k$ is transformed and then used in the sample computation:
\begin{equation}
\begin{aligned}
g_k = -log(-log(u_k)) \\
w_k = \frac{exp((log \; p_k + g_k) / \tau)}{\sum_{i=1}^{K} exp((log \; p_i + g_i) / \tau)}
\end{aligned}
\end{equation}
where $\tau$ is the temperature, $p_i$ and $p_k$ are token probabilites. The samples are discretized with argmax for the forward pass. 
It is argued in the literature that this estimation allows more efficient gradient estimation given larger action spaces for which REINFORCE produces very high variance estimates, e. g., when the vocabulary size is larger \parencite{havrylov2017emergence}.

Experiments on the MS COCO dataset are conducted, selecting one image in batches of 128 as the target. They showed that agents trained with Gumbel-softmax converge faster than those trained with REINFORCE, but both are able to achieve high referential success. Further, they find that communication success increases with maximum message length for both algorithms. They also find that the emergent communication protocol has multiple representations of the same information and that the messages seem to have hierarchical coding. They also conduct an experiment wherein the sender is trained with a combined loss on both an image captioning and reference game task in order to attempt to grounding the tokens in the images. However, this system didn't improve in terms of caption quality. To sum up, this work showed that agents can successfully develop multi-token communication protocols for solving referential tasks.

Similar work is conducted by \cite{lazaridou2018emergence}. The agent architecture closely matches \cite{havrylov2017emergence}, yet the study focuses on comparing reference games with realistic images to games on symbolic data. The protocol learned on symbolic data was shown to exhibit higher topographic similarity to the input data and more compositional features compared to raw pixel input experiments. Nevertheless, agents were able to successfully develop a protocol based on raw input, as well. These results suggest that the proposed architecture does not have an inductive bias sufficient for extracting symbolic compositional features from raw inputs. 

\cite{lazaridou2020multi} whose work is replicated and extended in this thesis use an architecture which combines the architectures by \cite{lazaridou2016multi} and \cite{havrylov2017emergence} in that they also use LSTM based agents, but condition them on combined image features of both the target and the distractor. That is, they conduct reference game experiments with pairs of images. They explore different speaker architectures. More precisely, they compare a speaker learning an emergent communication protocol maximizing the reference task success (``functional learning''), a speaker learning to emit images captions as messages based on ground truth captions in a supervised manner (``structural learning''), and speakers trained with a combination of the two learning approaches \parencite[][p. 4]{lazaridou2020multi}. The functional learning signal is also based on REINFORCE, while the structural learning is conducted with the standard cross-entropy loss. Multiple combined speaker parametrizations are compared: a speaker pretrained on an image captioning task which is then fine-tuned with functional learning on the reference game (conditioned on the target image only) (``reward finetuning''), a speaker trained with a weighted combination of the structural and functional losses (``multi-task learning'') and a speaker pretrained on image captioning and then learning a reranking function based on the reference game (``reward-learned rerankers'') \parencite[][p. 4--5]{lazaridou2020multi}. Two variations of the reranking model are presented; both speakers learn to rerank samples obtained from a pretrained image captioner. The ``product of experts reranker'' reranks the samples proportionally to the message probability times the probability of the message given the image pair. The latter is obtained by re-embedding the sampled captions with a trainable layer as bag-of-words vectors and computing dot ptoducts between the vectors and image embeddings, renormalizing the scores.  The ``noisy channel reranker'' learns the reranking according to Bayes rule, i. e., proportionally to the likelihood of obtaining the target image given the sample times the sample probability. The likelihood can be seen as the speaker's internal listener model and is also computed as the re-normalized dot product between a learned bag-of-words caption representation and each image embedding \parencite[][p. 5--6]{lazaridou2020multi}. The agents were trained on a reference game on the \textit{Abstract Scenes} dataset.
They showed that the ``product of experts reranker'' model outperforms other speakers with respect to referential success in the game, while also counteracting language drift (see Chaper \ref{chapter04} for details). 
Experiments in this thesis focuses on the ``multi-task learning'' architecture. This choice is discussed in more details in Chapter \ref{chapter05}.

The noisy channel reranker model by \cite{lazaridou2020multi} is closely related to work by \cite{andreas2016reasoning}. They also employ a Bayesian model wherein the agents learn to produce discriminative, i. e., task-directed grounded messages in a reference game by reasoning about the other agent. They also use the Abstract Scenes dataset. The architecture consists of a base speaker, base listener and a reasoning speaker. All agent modules are based on fully-connected linear layers with non-linear activations. The literal listener consists of modules embedding input images and the received message, and computing scores over image-message pairs. The literal speaker is used to sample possible messages for a given image by computing scores over vocabulary tokens given the image with a feed-forward conditional language model. The reasoning speaker then derives the optimal message by sampling candidate messages from the literal speaker and reasoning about their utility by ``passing'' them through the literal listener and choosing the message maximizing the listener's referential success probability. This architecture is closely related to idea of Rational Speech Act models \parencite{goodman2016pragmatic}. 
\cite{andreas2016reasoning} found that referential success increases with the number of samples taken from the literal speaker, as well as with increasing weight of reasoning about the listener in the reasoning speaker model. Further, they find that the reasoning speaker outperforms the literal speaker, indicating that pragmatic reasoning might be crucial for modeling task-conditional language use. \pt{seems highly relevant to my intuition about regularization and strucutral weight. add to discussion.}

\cite{lee2019countering} use another architecture---an attention based sequence-to-sequence machine translation model with one GRU layer---for both agents. 
The agents play a multi-modal translation game wherein the first agent is tasked with the translation of a sequence from French to English and the second---from English to German. Both agents are pretrained on the translation task before the communication game. This setup is used in order to investigate \textit{language drift} of the pivot language English, i. e., its deterioration (see Chapter \ref{chapter04} for details), as the agents are trained on French-German translation with REINFORCE (see Chapter \ref{chapter02} for details). They compare the mitigation of language drift through a language modeling constraint (i. e., incorporating the maximization of the likelihood of the English sentence under a pretrained LM into the reward) to grounding (i. e., maximizing the likelihood of an image corresponding to the English sentence under a pretrained image retrieval model). The LM is a one-layer LSTM; in the grounding model, image features are extracted using a pretrained ResNet-152. \cite{lee2019countering} find that the agents are best able to learn decent French-German translation when mitigating language drift with both an LM constraint and grounding. \pt{check italics of language drift and reference to chapter 4 thru chapter}

\textbf{Training}

The discretization of the agents' communication channel is sometimes also called the \textit{discrete bottelneck} \parencite{lazaridou2020multi}. 
While being easier from a modeling perspective due to the differentiability of a continuous channel, the former channel architecture is less naturalistic, as human languages are considered discrete signalling systems \parencite{hockett1960origin}. 

There are different ways to overcome the technical challenge of training a system with a discrete bottleneck: using the REINFORCE algorithm, or using a continuous approximation \cite{havrylov2017emergence}. 

\subsection{Tasks}

\pt{\parencite{evtimova2017emergent} do multi-step interactions. Akin to the multi-step MNIST task variation of \cite{foerster2016learning}.}

The studies reviewed so far employed a particular  task, or environment, for the agents, namely, the standard reference game. However, a variety of other tasks has been modelled as well.
Evtimova, Drozdov, Kiela, and Cho 2018 consider multi-step interactions. Bouchacourt and Baroni 2019 see also Cao et al., 2018. Fruits and tools, using reasoning based on human judgements.
Looking at pressures important for properties of the emergent language e g  \parencite{bouchacourt2018agents}. 
Others study complex cooperative environments where communication is just an auxiliary channel (\parencite{das2019tarmac}, Lowe, Foerster, Boureau, Pineau, and Dauphin (2019),). Here, the authors particularly investigate the efficacy of having an auxiliary communication channel for the actual task at hand.

More an evolutionary perspective: modeling contact linguistics, i.e., convergence to the same language: Graesser, Cho, and Kiela (2019). Similarly driven by onvestiagting human properties of language, a lot of works has focused on compositionality: (Choi, Lazaridou, de Freitas, 2018). (Lazaridou et al., 2018; Andreas, 2019; Chaabouni, Kharitonov, Bouchacourt, Dupoux, and Baroni, 2020) show that generalization to novel composite meanings is supported even without superficial compositionality. 
``generational transmission of language favors compositionality e.g., Kirby and Hurford, 2002 Kirby, Griffiths, and Smith, 2014, an observation recently confirmed for deep agents Li and Bowling, 2019 Ren, Guo, Havrylov, Cohen, and Kirby, 2019.''

Other evolutionary perspective: agent co-evolution \parencite{dagan2020co}.

\textbf{Properties of the communication \parencite{lazaridou2020multi}}: difficulty to converge on consistent meanings of tokens for color values, generally task-oriented codes with complexity minimization. Anti-efficient code development --> I am looking somewhat into this with my different-lengths experiments and grammars.  Larger communities lead to more systematic languages --> fixed listener experiment.

Interesting pressures leading to more human-like language properties: memory bottleneck. Other internal and external pressures \parencite{luna2020internal}. 

\pt{check how the structure would make sense, such that i don't repeate myself when talking about the reference game}

Compositionality. also paper by Bruni.

%\subsection{Paper for Replication}

\pt{Summarise in detail base paper}


\section{Reference Games}
\label{reference_games}
Following previous work, presented multi-agent communication is investigated in an environment wherein the communication itself plays a central role, namely a reference game. This setting is developed based on the so-called \textit{singalling game} \parencite[e.g.,][]{lewis1969convention, skyrms2010signals} \pt{ add some prose}. 

\cite{lazaridou2018emergence} also helpful for a review.

Describe the idea of seeing several images, target distractor, producing and sending message.
The current experiments are limited to using one target image in context of only one distractor, but thy do not depend on this confinement.
Discuss the similar experiment. 

Multimodality aspects.

Discriminative caption goal. Generally work on discriminative / contrastive communication, more froma cogsci and linguistics perspective.

The experiments conducted in this thesis are \textit{reference games} played by the agents. Therefore, this section provides an overview and background of the task. 
A reference game is an instance of the \textit{Lewis signaling game} . 