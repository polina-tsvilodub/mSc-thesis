
This chapter reviews related work on multi-agent communication---the core domain in which the presented experiments are situated. First, different architectural architectures used for building multi-agent communication experiments are reviewed. Then, different tasks the agents are trained on are summarized, focusing on the reference game setting. 
%Finally, the paper by \cite{lazaridou2020multi} which serves as the starting point for this thesis is summarized in detail.  

\section{Multi-Agent Communication}
Research on multi-agent communication has received an increasing interest over the recent years, as this domain focuses on research questions of high relevance to cognitive science \parencite{lazaridou2020emergent}. More precisely, two main areas of research are often addressed: first, the conditions under which a language emerges and which emergent properties can be observed; and second, the ability of the agents to pick up a given language system and ground it in the visual world.% thus investigating whether tractable human-agent communication channels can be learned via realistic interaction.
These research questions have different implications for understanding human language: the former often focuses on what the causal factors behind prominent features of human language like compositionality are, allowing to shed light on human language evolution; the second rather addressed how we can train agents to communicate optimally for interacting with humans. Equivalently, these two main directions can be classified as investigating \textit{language emergence} and \textit{language acquisition} among and by artificial agents \parencite{lazaridou2018emergence, lazaridou2020emergent}.
Presented work focuses on the latter question, aiming to train agents to use English language to refer to real-world images.

Irrespectively of the research focus, artifical agents which interact with each other are at the core of multi-agent communication studies. Therefore, different agent architectures are presented in the next section.

\subsection{Agent Architecture and Training}

The main architectural differences concern the type of model representing the artificial agent and the type of communication channel they use.
As described in Section \ref{rl}, experiments wherein agents learn to complete a task by interacting with the environment and with each other are typically trained with reinforcement learning. However, based on the chosen communication channel and the task, the specific optimization algorithms differ. 

While early work in multi-agent communication focuses on structured agents \parencite[e. g., see][for reviews]{christiansen2003language, cangelosi2002symbol}, deep agent architectures became increasingly dominant in the domain over the last years \parencite{lazaridou2020emergent}. As this thesis also uses deep neural network agents, the review of related work focuses on deep multi-agent communication experiments. 

%Architectural considerations: 
%1) agent architecture: hand crafted vs deep; for deep: kind of network, esp. recurrent layer
%2) loss and training approach: Q-learning \parencite{foerster2016learning}, Gumbel-Softmax \parencite{havrylov2017emergence}, REINFORCE.  \parencite{lazaridou2020emergent, williams1992simple}. 

\cite{lazaridou2016multi} first show that neural agents can efficiently reference realistic visual inputs in the presence of distractors by developing a communication protocol from scratch. To this end, \textcite{lazaridou2016multi} conduct several experiments employing feed-forward and CNN agents. In particular, in their main experiment, they set up a reference game, where a sender agent emits a single discrete symbol referring to one of two images provided to both the sender and the receiver, sampled from the ImageNet dataset \parencite{deng2009imagenet}. The receiver agent has to guess  which image is the intended target, based on the symbol received from the sende \pt{refer to section below for details}. They use a subset of the dataset containing 100 images from each of 20 general categories. \cite{lazaridou2016multi} set up two versions of the sender: the ``agnostic sender'' consists of a two-layer feed forward neural network, embedding both images and producing a score over the vocabulary given the concatenation of image embeddings \parencite[][p. 3]{lazaridou2016multi}. The ``informed sender'' consists of a three-layer neural network, applying two convolutional layers to embeddings of both images which are treated as different channels \parencite[][p. 3]{lazaridou2016multi}. It also outputs scores over the vocabulary. For both architecures the scores are converted into a Gibbs distribution from which the emitted message symbol is sampled. The receiver agent consists if a two-layer feed-forward network which embeds both images and computes the dot products between these embeddings and the one-hot encoded message. The final target image choice is sampled from the Gibbs distribution computed from the dot products. %The agents are trained with REINFORCE which updates parameters of the speaker and listener policies by minimizing the negative expected reward. The speaker and listener policies are parametrised by the weights of the respective neural networks. The sender policy is $\pi(v \in V \mid i_L, i_R, t \in \{L, R\})$ where $V$ is the vocabulary, $i_L, i_R$ are the left and right images and $t$ is the position of the target. The receiver policy is $\pi(t \in \{L, R\}\mid v_s, i_L, i_R)$, where $v_s$ is the message received from the sender and $t$ is the guessed position of the target image. \textcite{lazaridou2016multi} set the reward to 1 if the image picked by the receiver is the target and 0 otherwise. 
They found that agents achieve high referential success after around 50000 training games and 1000 test games for different hyperparameter settings. Furthermore, they showed that the informed sender produces more semantically natural symbols (i.e., the same ones for the same categories) than the agnostic one. In a further experiment, the authors successfully pressure the agents to communicate more about high-level properties by sampling instances of different subcategories for the image pairs.%, and achieve a small increase in label purity (i. e., in the proportion of labels agreeing with the major cluster label). 
Finally, they perform an experiment grounding the communication in natural language wherein the sender agent has to use natural language category labels as message symbols. The sender is trained by switching between reference game play and image classification on ImageNet data. However, the vocabulary is limited to 100 words, and the sender only produces one-word sentences. %They observe a higher symbol purity in this experiment, while retaining referential success. 	
%	\item They use REINFORCE for training the speaker. Overall framing: interactive communicative setting for training agents allows to capture functional aspects of communication, i.e., the goal-directed nature of communication, as opposed to purely supervised conversational agent training. 
%	\item \textit{multi-agent coordination communication game}: first functional task (a basic function of language): reference
	
Another foundational study in deep multi-agent communication was conducted by \cite{foerster2016learning}. In contrast to \cite{lazaridou2016multi}, the authors let the agents use a centrally trained continuous communication system in their ``DIAL'' architecture. This system is compared to the ``RIAL'' system where agents use traditional discrete communication \parencite[][p. 2]{foerster2016learning}. This is one of the first studies using a recurrent network in the sender architecture.
For both systems, the sender consists of an input network creating embeddings of the task environment, two recurrent GRU layers, and two feed-forward output layers.
The receiver has the same architecture. The DIAL agents are connected via a continuous vector which can be seen as an activation layer passing the internal state of the sender to the listener.
Experiments with different variants of a riddle task and an image classification task on the MNIST dataset were conducted. Results on the former task showed that the agents were able to learn an optimal policy and DIAL agents converged faster, yet sharing the weights between the sender and receiver agents was critical in the DIAL architecture. Results of the second task provided similar results for DIAL, while RIAL failed to converge stably. 
The discrete system is trained with deep $Q$-learning (see Section \ref{rl_methods}), while the continuous system was trained by additionally backpropagating gradients end-to-end between the two agents. %Furthermore, the former but not the latter case includes modeling wherein agents have access to each other's internal representations of the environment, which was also argued to be unnatural when developing a system to investigate human communication \pt{REF}. 
The DIAL results pose an interesting question regarding the comparability of such a system where the internal representations of a speaker are directly passed to the listener to human communication \parencite[cf.][]{lazaridou2020emergent, hockett1960origin}.

\cite{havrylov2017emergence} further extend work on multi-agent communication by modeling agents which are allowed to communicate with variable-length strings of symbols in a reference game. 
Their architecture is based on the proposal \cite{lazaridou2016multi} with the difference that the sender agent only has access to the target image. Furthermore, their agents consist of LSTM layers. Information about the target image is injected by initializing the hidden state of the sender LSTM with the image feature vector, extracted from a pretrained VGG CNN. The sender generates the message by sampling tokens from the vocabulary similarly to the image captioning system by \cite{vinyals2015show} described in Section \ref{image_captioning}. The vocabulary consists of 10,000 tokens, the LSTM hidden size is 512, and the token embeddings are 256-dimensional. 
The receiver interprets the message by encoding it with the LSTM and computing a probability distribution over dot products of image features and the affine-transformed hidden state of the LSTM. The image with the highest probability is chosen as the target. \cite{havrylov2017emergence} further attempt to make the statistical properties of the emergent protocol as similar to natural language as possible by minimizing the KL-divergence between the learned conditional token distribution and a pretrained language model.
They compare agents trained via the straight-through Gumbel-softmax estimator to agents trained with REINFORCE. The straight-through Gumbel-softmax estimator is a differentiable approximation of discrete actions (i. e., tokens) with a continuous relaxation applied in the backward pass of the optimization, while still using discrete representations in the forward pass. 
Experiments on the MS COCO dataset are conducted, selecting one image in batches of 128 as the target. They showed that agents trained with Gumbel-softmax converge faster than with REINFORCE, but both are able to achieve high referential success. Further, they find that communication success increases with maximum message length for both algorithms. They also find that the emergent communication protocol has multiple representations of the same information and that the messages seem to have hierarchical semantic coding. They also conduct an experiment wherein the sender is trained with a combined loss on both an image captioning and reference game task in order to attempt correctly grounding the tokens. However, this system didn't improve in terms of caption quality. To sum up, this work showed that agents can successfully develop multi-token communication protocols for solving referential tasks.

Similar work is conducted by \cite{lazaridou2018emergence}. The agent architecture closely matches \cite{havrylov2017emergence}, yet the study focuses on comparing reference games with realistic images with games on symbolic data. The protocol learned on symbolic data was shown to exhibit higher topographic similarity to the input data and more compositional features compared to raw pixel input experiments. Nevertheless, agents were able to successfully develop a protocol based on raw input, as well. These results suggested that the proposed architecture does not have an inductive bias sufficient for extracting symbolic compositional features from raw inputs. 

\cite{lazaridou2020multi} whose work is replicated and extended in this thesis use an architecture which combines the architectures by \cite{lazaridou2016multi} and \cite{havrylov2017emergence} in that they also use LSTM based agents, but condition them on combined image features of both the target and the distractor. That is, they conduct reference game experiments with pairs of images. They explore different speaker architectures differing in the precise parametrization. More precisely, they compare a speaker learning an emergent communication protocol maximizing the reference task success (``functional learning''), a speaker learning to emit images captions as messages based on ground truth captions in a supervised manner (``structural learning''), and speakers trained with a combination of the two learning approaches \parencite[][p. 4]{lazaridou2020multi}. The functional learning signal is also based on REINFORCE, while the structural learning is conducted with the standard cross-entropy loss. \pt{check if it occurs in image captioning intro} Multiple combined speaker parametrizations are compared: a speaker pretrained on an image captioning task which is then fine-tuned with functional learning on the reference game (conditioned on the target image only) (``reward finetuning''), a speaker trained with a weighted combination of the structural and functional losses (``multi-task learning'') and a speaker pretrained on image captioning and then learning a reranking function based on the reference game (``reward-learned rerankers'') \parencite[][p. 4--5]{lazaridou2020multi}. Two variations of the reranking model are presented; both speakers learn to rerank samples obtained from the pretrained image captioner. The ``product of experts reranker'' reranks the model proportionally to the message probability times the probability of message for the task sucess. The latter is obtained by a re-embedding the sampled captions with a trainable layer.  The ``noisy channel reranker'' learns the reranking according to Bayes' rule, i. e., proportionally to the likelihoof of obtaining the target image given the sample times the sample caption probability. The likelihood can be seen as the speake's internal listener model and is computed with a trainable embedding layer \parencite[][p. 5--6]{lazaridou2020multi}. The agents were trained on a reference game on the \textit{Abstract Scenes} dataset.
They showed that the ``product of experts reranker'' model outperforms other speakers with respect to referential success in the game, while also counteracting language drift (\pt{see Chaper \ref{chapter04} for details}). 
Work in this thesis focuses on the \textit{multi-task learning architecture}; exploring other speaker parametrizations in presented experiments is an interesting direction for future work.

The noisy channel reranker model by \cite{lazaridou2020multi} is closely related to work by \parencite{andreas2016reasoning}. They also employ a Bayesian model wherein the agents learn to produce discriminative, i. e., task-directed grounded messages in a reference game by reasoning about the other agent. They also use the Abstract Scenes dataset. The architecture consists of a base speaker, base listener and a reasoning speaker. All agent modules are based on fully-connected linear layers with non-linear activations. The literal listener consists of modules embedding input images and the received message, and computing scores over image-message pairs. The literal speaker is used to sample possible messages for a given image by computing scores over the tokens given the image with a feed-forward conditional language model. The reasoning speaker then derives the optimal message by sampling candidate messages from the listeral speaker and reasoning about their utility by ``passing'' them through the listeral listener and choosing the message maximizing the listener's referential success probability. This architecture is closely related to idea of Rational Speech Act models \pt{REF and check mentioning timing}
\cite{andreas2016reasoning} found that referential success increases with the number of samples taken from the literal speaker, as well as with increasing weight of reasoning about the listener by the reasoning speaker model, and outperforms a purely literal speaker, indicating that pragmatic reasoning might be crucial for modeling task-conditional language use. \pt{seems highly relevant to my intuition about regularization and strucutral weight. add to discussion.}

\cite{lee2019countering} use an attention based machine translation model, seq2seq with attention. 

\pt{Maybe work by Monroe and Potts???}

\pt{Check if there is any MARL work with multimodal models or thelike}

\textbf{Training}

\pt{\parencite{evtimova2017emergent} do multi-step interactions. Akin to the multi-step MNIST task variation of \cite{foerster2016learning}.}

The discretization of the agents' communication channel is sometimes also called the \textit{discrete bottelneck} \parencite{lazaridou2020multi}. \parencite{foerster2016learning}: from \cite{lazaridou2020multi}: ``The study found that allowing agents to communicate improves coordination, as indicated by higher team rewards compared to no-communication controls. However, while continuous communication systematically results in improved coordination (see also Sukhbaatar, Szlam, and Fergus, 2016; Kim, Moon, Hostallero, Kang, Lee, Son, and Yi, 2019; Singh, Jain, and Sukhbaatar, 2019), discrete communication does not yield consistent improvements when the complexity of the environment grows, and it only manages to marginally improve on the baselines when the agents are constrained to share the same weight parameters, a rather unrealistic assumption.''

While being easier from a modeling perspective due to the differentiability of a continuous channel, the former channel architecture is less naturalistic, as human languages are considered discrete signalling systems \parencite{hockett1960origin}. 

There are different ways to overcome the technical challenge of training a system with a discrete bottleneck: using the REINFORCE algorithm, or using a continuous approximation \cite{havrylov2017emergence}. 

\subsection{Tasks}

The studies reviewed so far employed a particular  task, or environment, for the agents, namely, the standard reference game. However, a variety of other tasks has been modelled as well.
Evtimova, Drozdov, Kiela, and Cho 2018 consider multi-step interactions. Bouchacourt and Baroni 2019 see also Cao et al., 2018. Fruits and tools, using reasoning based on human judgements.
Looking at pressures important for properties of the emergent language e g  \parencite{bouchacourt2018agents}. 
Others study complex cooperative environments where communication is just an auxiliary channel (\parencite{das2019tarmac}, Lowe, Foerster, Boureau, Pineau, and Dauphin (2019),). Here, the authors particularly investigate the efficacy of having an auxiliary communication channel for the actual task at hand.

More an evolutionary perspective: modeling contact linguistics, i.e., convergence to the same language: Graesser, Cho, and Kiela (2019). Similarly driven by onvestiagting human properties of language, a lot of works has focused on compositionality: (Choi, Lazaridou, de Freitas, 2018). (Lazaridou et al., 2018; Andreas, 2019; Chaabouni, Kharitonov, Bouchacourt, Dupoux, and Baroni, 2020) show that generalization to novel composite meanings is supported even without superficial compositionality. 
``generational transmission of language favors compositionality e.g., Kirby and Hurford, 2002 Kirby, Griffiths, and Smith, 2014, an observation recently confirmed for deep agents Li and Bowling, 2019 Ren, Guo, Havrylov, Cohen, and Kirby, 2019.''

Other evolutionary perspective: agent co-evolution \parencite{dagan2020co}.

\textbf{Properties of the communication \parencite{lazaridou2020multi}}: difficulty to converge on consistent meanings of tokens for color values, generally task-oriented codes with complexity minimization. Anti-efficient code development --> I am looking somewhat into this with my different-lengths experiments and grammars.  Larger communities lead to more systematic languages --> fixed listener experiment.

Interesting pressures leading to more human-like language properties: memory bottleneck. Other internal and external pressures \parencite{luna2020internal}. 

\pt{check how the structure would make sense, such that i don't repeate myself when talking about the reference game}

%\subsection{Paper for Replication}

\pt{Summarise in detail base paper}


\section{Reference Games}

Following previous work, presented multi-agent communication is investigated in an environment wherein the communication itself plays a central role, namely a reference game. This setting is developed based on the so-called \textit{singalling game} \parencite[e.g.,][]{lewis1969convention, skyrms2010signals} \pt{ add some prose}. 

\cite{lazaridou2018emergence} also helpful for a review.

Describe the idea of seeing several images, target distractor, producing and sending message.
The current experiments are limited to using one target image in context of only one distractor, but thy do not depend on this confinement.
Discuss the similar experiment. 

Multimodality aspects.

Discriminative caption goal. Generally work on discriminative / contrastive communication, more froma cogsci and linguistics perspective.

The experiments conducted in this thesis are \textit{reference games} played by the agents. Therefore, this section provides an overview and background of the task. 
A reference game is an instance of the \textit{Lewis signaling game} . 