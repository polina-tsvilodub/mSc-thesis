
This chapter reviews related work on multi-agent communication---the core domain in which the presented experiments are situated. First, different architectures used for building the agent are reviewed. Then, different tasks the agents are trained on are summarized, focusing on the reference game setting. 
%Finally, the paper by \cite{lazaridou2020multi} which serves as the starting point for this thesis is summarized in detail.  

\pt{maybe find a way to say that there are other neighboring areas of research like visual dialogue, multimodal semantics, visual question answering etc. possibly in discussion.}

\section{Introduction}
Research on multi-agent communication has received increasing attention over the recent years, as this domain focuses on research questions of high relevance to cognitive science \parencite{lazaridou2020emergent}. More precisely, two main areas of research are often addressed: first, the conditions under which a language emerges and which emergent properties can be observed; and second, the ability of the agents to pick up a given language system. % thus investigating whether tractable human-agent communication channels can be learned via realistic interaction.
The second direction has also been considered with respect to \textit{grounding} a given language in the visual world. That is, approaches to teaching agents to use language applied to realistic visual input are investigated. Grounding is important both from modeling and cognitive perspectives: for instance, \cite{bruni2014multimodal} show that representations learned from both text and image data are more plausible than purely text-based representations; furthermore, vision is an integral part of human communication \parencite{tomasello2010origins, harnad1990symbol}. 

These two research directions have different implications for understanding human language: the former often focuses on what the causal factors behind prominent features of human language like compositionality are, allowing to shed light on human language evolution; the second rather addresses how we can train agents to communicate optimally for interacting with humans. Equivalently, these two main directions can be classified as investigating \textit{language emergence} and \textit{language acquisition} among and by artificial agents \parencite{lazaridou2018emergence, lazaridou2020emergent}.
Presented work focuses on the latter question, aiming to train agents to use English language grounded in real-world images.

Irrespectively of the research focus, artifical agents which interact with each other are at the core of multi-agent communication studies. Therefore, first, different agent architectures are presented in the next section.

\pt{I feel like a section on super general aspects, like having two agents doing some task and being jointly trained, is missing. Maybe visuals would be cool.}

Communication has been embedded in various tasks simulated in multi-agent settings. In some tasks like \pt{FILL ME} the idea is to use communication for more efficient cooperation and expertise transfer; in other tasks, communication itself is the core task. The latter allows to focus on properties of communicative systems that artificial agents might develop or acquire, thus allowing to compare them to human language. Since studying the ability of artificial agents to produce \textit{discriminative captions} for images in context within reference games is the goal of this work, this task and related work with human participants is described first, before transitioning to related work on multi-agent communication.

\pt{just to a section the chapter proceeds as follows and through out first paragraph and remove intro section.}

\section{Reference Games}
\label{reference_games}

Reference games are a type of the so-called \textit{Lewis singalling game} \parencite{lewis1969convention, skyrms2010signals}.
Signalling games were developed as an accont of \textit{conventional meaning} \pt{add def}. Traditionally, these games include two agents, a sender and a receiver. Only the sender observes a state sampled at random and chooses a message which is sent to the receiver. The set of possible messages is known to both agents. Based on the message, the receiver selects a state she chose based on the message. The game is a success if the guessed state and the state observed by the sender match, such that this game is driven by the agents having a common interest \parencite{lewis1969convention, franke2016evolution}. \pt{check if there might be a better reference}. 

\textit{Reference games} are then a specific instance of signalling games in which the meaning is already conventioanlly established. The sender samples a particular target among a set of distractors which may be a subset of all possible objects in the given world and \textit{refers} to it with her message. The target and the set of distractors are accessible to both agents. The receiver's task is then the identification of the target. Thus, the target and the distractors which are often represented by visual context in practice play a critical role in reference games.

Reference games have been employed in a wide range of studies with various research questions, often in order to study the pragmatic reasoning involved in generating referential expressions \pt{double check}. For example, \cite{franke2016reasoning} investigate whether pragmatic reasoning in reference games is population-level or individual. \pt{Def double check} For instance, \cite{graf2016animal} conduct an experiment wherein participants generate referential expression to a target image presented in context of distractors which either belonged to the same basic-level object category or to a different one.\footnote{\pt{define basic level categories and add reference, Rosch 1976}} They show that participants generated subordinate category, i.e., more specific, referential expressions in presence of more similar distractors, but not for dissimilar distractors. They show that humans flexibly increase the specificity of nominal expressions they use in order to unambiguously refer to the target, when required by the context. However, human speech also presents abundant examples of \textit{overmodification} in referential expressions, i.e., the use of additional modifiers like color expressions even when it is not strictly necessary for discrimination in the given conext. \cite{degen2020redundancy} explain this phenomenon in terms of complexity of the visual scenes, typicality of the referents and of the described features. They reveal complex pragmatic reasoning underlying this phenomenon. Much more work has been done on referential expressions both experimentally and theretically; these examples go to show that humans inherently generate highly discriminative, or, contrastive, messages in situations like reference games. Even further, they also expect and anticipate such behavior from other interlocutors, as has been shown in processing literature \parencite[e. g., cf.]{sedivy1999achieving}.

Applied to artificial agents, a reference game proceeds as follows: arrays containing a target image among $n \geq 1$ distractor images are sampled from the set of all images $I$. Both agents have access to the images and usually have the same representation; the sender agent knows which image is the target, the receiver does not. The sender emits a message $m, |m| \geq 1$ (assuming discrete communication; see Section \ref{multi_agent_arch}) sampled from the shared vocabulary $V$. Given $m$, the receiver has to correctly guess which of the images in the array is the target. Experiments in this thesis are limited to using one target image in context of only one distractor, but thy do not depend on this confinement.

\pt{Add Dale and Reiter, Computational interpretations of the Gricean maxims in the generation of referring expressions.  1995 }

\pt{Upshot: hypothesis about differing lengths as a proxy for different granularity of description. Bruni's paper as ground to believe that this might not be borne out as the bias in terms of length in the training dataset is too strong.}

\section{Discriminative Language Use}

Before turning to approaches wherein agents learn discriminative message generation in reference games, akin to human communication, rather machine learning-based approaches to building discriminative image captioning systems are reviewed.  \pt{it would probably make the most sense to review \cite{andreas2016reasoning} first, but then this framing doesn't quite make sense.}

\cite{sadovnik2012image} first approach discriminative image captioning by looking at visual discriminablity and saliency, and cosnstructing captions based on hand-crafted rules.

\cite{vedantam2017context} build an image captioning model which performs joint inference over a language model that is context-agnostic and a listener model which provides feedback on discriminativity of the captions. More precisely, they introduce a reaoning speaker consisting of a basic image captioning speaker and a listener model. The basic speakre is an image captioner pretrained on images and target concepts represented in the images. The listener is only dependent on the generative basic speaker, as it computes the log-likelihood ratio between the speaker's utterances for the same image but for different concepts. The overall speaker is the trained to maximize the basic generator probability while also maximizing the likelihood ratio represnting the discriminativity.
Thay apply the model to a justification task wherein the image feature corresponding to a given discriminative aspect of the caption has to be explained. They also apply it to standard image captioning on similar image pairs from the MS COCO dataset. They find that their architecture is able to generate caption which leads to higher discrimination success of human evaluators compatred to non-discriminative baseline.

\cite{dai2017contrastive} proposel the ``Contrastive Learning'' training objective for this task which constrains the image captioner to maximize the true caption likelihood compared to a pretrained reference image captioner, while learning to assign lower probabilities to mismatching caption for the target image, compared to that same reference model. Critically, the success of the system depends on the quality of the reference model. Their model achieves competitive results on the MS COCO test set.

While alleviating some issues like language drift \pt{check if it is okay to foreshadow this here like thsi}, these systems miss a critical component in the approach to the task---they are missing the communicative and interactive context of applying such cpations. Yet interactivity and social aspects of the communicative task are critical for developing systems which are supposed to be able to adequatly interact with humans. The arguably most prominent framework formalizing social and cooperative aspects of communication is the Rational Speech Act (RSA) framework \parencite{goodman2016pragmatic}. Some approaches to discriminative image captioning are highly inspired by RSA and thereby incorporate interactive communicative aspects into their models.\footnote{Basic knowledge of the RSA family of models is presupposed. For a gentle introduction, see, e.g., \cite{problang}.} \pt{double check adjectives here}

\cite{andreas2016reasoning} are the first to employ an RSA-style Bayesian model wherein the agents learn to produce discriminative messages in a reference game by reasoning about the other agent. They also use the Abstract Scenes dataset. The architecture consists of a base speaker, base listener and a reasoning speaker. All agent modules are based on fully-connected linear layers with non-linear activations. The literal listener consists of modules embedding input images and the received message, and computing scores over image-message pairs. The literal speaker is used to sample possible messages for a given image by computing scores over vocabulary tokens given the image with a feed-forward conditional language model. The reasoning speaker then derives the optimal message by sampling candidate messages from the literal speaker and reasoning about their utility by ``passing'' them through the literal listener and choosing the message maximizing the listener's referential success probability. 
\cite{andreas2016reasoning} found that referential success increases with the number of samples taken from the literal speaker, as well as with increasing weight of reasoning about the listener in the reasoning speaker model. Further, they find that the reasoning speaker outperforms the literal speaker, indicating that pragmatic reasoning might be crucial for modeling task-conditional language use. \pt{seems highly relevant to my intuition about regularization and strucutral weight. add to discussion.}

\cite{cohn2018pragmatically} also build a model following the standard RSA architecture. Their model performs character-wise caption generation in order to constrain the space of possile alternatives the speaker may choose among when generating the caption. Therefore, their generation process is an incremental one, modelled by an LSTM-based speaker agent. Their model  turns out to outperform word-level generation and non-pragmatic models. \pt{see if we want more detials}

\cite{nie2020pragmatic} propose an issue-sensitive image captioner, also based on the RSA famly of models. Issue-sensitivity is represented as partitioning  the space of images into cells with images having the same relevant (i.e., at-issue) feature as the target. Their base speaker is a standard  pretrained image captioning model. Additionally to standard RSA mechanics, they extend the pragmatic speaker to be sensitive to the relevant partitioning. They further include a utility term in order to tackle arising semantic drift (e. g., mentioning features that are not true of the target) of the captions and constrain the speaker from overgenerating. This model also employs an incremental speaker generation procedure. Evaluations include visual question answering (VQA) on MS COCO, showing that the model learns adequate issue representations from questions as well as generates promising issue-sensitive captions. 

Finally, an approach fully embracing the idea of learning language use from active interaction, not just representations of other agents,  is multi-agent communication. Therefore, the next section turns to work on discriminative language learning and other related research in this domain.

\section{Multi-Agent Communication}
\label{mac}
The main architectural differences concern the type of model representing the artificial agent and the type of communication channel they use.
As described in Section \ref{rl}, experiments wherein agents learn to complete a task by interacting with the environment and with each other are typically trained with reinforcement learning. Multi-agent communication is a specific type of such experiments in that (one of) the tasks agents learn are communicative and cooperative, and one of the agents can be considered the sender and the other the receiver of the communication \parencite[cf.][]{tan1993multi, lazaridou2016multi}.
Based on the chosen communication channel and the precise communicative task, specific architectural decisions and optimization algorithms differ. 

\subsection{Agent Architecture and Training in Reference Games}
\label{multi_agent_arch}

While early work in multi-agent communication focuses on structured agents \parencite[e. g., see][for reviews]{christiansen2003language, cangelosi2002symbol}, deep agent architectures became increasingly dominant in the domain over the last years \parencite{lazaridou2020emergent}. As this thesis also uses deep neural network agents, the review of related work focuses on deep multi-agent communication experiments. 

%Architectural considerations: 
%1) agent architecture: hand crafted vs deep; for deep: kind of network, esp. recurrent layer
%2) loss and training approach: Q-learning \parencite{foerster2016learning}, Gumbel-Softmax \parencite{havrylov2017emergence}, REINFORCE.  \parencite{lazaridou2020emergent, williams1992simple}. 

\cite{lazaridou2016multi} first show that neural agents can efficiently reference realistic visual inputs in the presence of distractors by developing a communication protocol from scratch. To this end, \textcite{lazaridou2016multi} conduct several experiments employing feed-forward and CNN agents. In particular, in their main experiment, they set up a reference game, where a sender agent emits a single discrete symbol referring to one of two images provided to both the sender and the receiver, sampled from the ImageNet dataset \parencite{deng2009imagenet}. The discretization of the agents' communication channel is sometimes also called the \textit{discrete bottelneck} \parencite{lazaridou2020multi}. 
The receiver agent has to guess  which image is the intended target, based on the symbol received from the sender (see Section \ref{reference_games} for details). %They use a subset of the dataset containing 100 images from each of 20 general categories.
\cite{lazaridou2016multi} set up two versions of the sender. The ``agnostic sender'' consists of a two-layer feed forward neural network, embedding both images and producing a score over the vocabulary given the concatenation of image embeddings \parencite[][p. 3]{lazaridou2016multi}. The ``informed sender'' consists of a three-layer neural network, applying two convolutional layers to embeddings of both images which are treated as different channels \parencite[][p. 3]{lazaridou2016multi}. It also outputs scores over the vocabulary. For both architecures the scores are converted into a Gibbs distribution from which the emitted message symbol is sampled. The receiver agent consists if a two-layer feed forward network which embeds both images and computes the dot products between these embeddings and the one-hot encoded message. The final target image choice is sampled from the Gibbs distribution computed from the dot products. %The agents are trained with REINFORCE which updates parameters of the speaker and listener policies by minimizing the negative expected reward. The speaker and listener policies are parametrised by the weights of the respective neural networks. The sender policy is $\pi(v \in V \mid i_L, i_R, t \in \{L, R\})$ where $V$ is the vocabulary, $i_L, i_R$ are the left and right images and $t$ is the position of the target. The receiver policy is $\pi(t \in \{L, R\}\mid v_s, i_L, i_R)$, where $v_s$ is the message received from the sender and $t$ is the guessed position of the target image. \textcite{lazaridou2016multi} set the reward to 1 if the image picked by the receiver is the target and 0 otherwise. 
They found that agents achieve high referential success, and that the informed sender produces semantically more natural symbols (i.e., the same ones for the same image categories) than the agnostic one. In a further experiment, the authors successfully pressure the agents to communicate about more high-level properties by sampling instances of different subcategories within the same basic-level category for the image pairs.%, and achieve a small increase in label purity (i. e., in the proportion of labels agreeing with the major cluster label). 
Finally, they perform an experiment grounding the communication in natural language wherein the sender agent has to use natural language category labels as message symbols. The sender is trained by switching between reference game play and image classification on ImageNet data. However, the vocabulary is limited to 100 words, and the sender only produces one-word messages. %They observe a higher symbol purity in this experiment, while retaining referential success. 	
%	\item They use REINFORCE for training the speaker. Overall framing: interactive communicative setting for training agents allows to capture functional aspects of communication, i.e., the goal-directed nature of communication, as opposed to purely supervised conversational agent training. 
%	\item \textit{multi-agent coordination communication game}: first functional task (a basic function of language): reference
	
Another foundational study in deep multi-agent communication was conducted by \cite{foerster2016learning}. In contrast to \cite{lazaridou2016multi}, the authors let the agents use a centrally trained system in their ``DIAL'' architecture (i. e., with parameter sharing across agents)  which can also be extended to continuous communication. This system is compared to the ``RIAL'' system where agents use traditional discrete communication \parencite[][p. 2]{foerster2016learning}. This is one of the first studies using a recurrent network in the sender architecture.
For both systems, the sender consists of an input network with a fully connected layer and lookup tables creating embeddings of the task environment, two recurrent GRU layers, and two feed-forward output layers.
The receiver has the same architecture. The DIAL agents are connected via a continuous vector which can be seen as an activation layer passing the internal state of the sender to the receiver.
Experiments with different variants of a riddle task and an image classification task on the MNIST dataset were conducted. Results on the former task show that the agents were able to learn an optimal policy although DIAL agents converged faster, yet sharing the weights between the sender and receiver agents was critical for RIAL. DIAL with parameter sharing outperformed other experiments. Furthermore, they found that adding noise to the communication was critical for successful learning for RIAL. Results of the second task provided similar results for DIAL, while RIAL failed to converge stably on the multi-step interaction version of this task. 
The discrete system is trained with deep $Q$-learning (see Section \ref{rl_methods}), while the continuous system was trained by additionally backpropagating gradients end-to-end between the two agents. %Furthermore, the former but not the latter case includes modeling wherein agents have access to each other's internal representations of the environment, which was also argued to be unnatural when developing a system to investigate human communication \pt{REF}. 
The DIAL results pose an interesting question regarding the comparability of such a system where the internal representations of a speaker are directly passed to the listener to human communication \parencite[cf.][]{lazaridou2020emergent, hockett1960origin}. The authors, however, argue that this direct error propagation can be interpreted as communicative feedback which is also part of human communication. Further, while the possibility to use DIAL with continuous communication is attractive from a machine learning perspective due to its straightforward differentiability, human languages are typically considered discrete signalling systems \parencite{hockett1960origin}. 

\cite{havrylov2017emergence} further extend work on multi-agent communication by modeling agents which  communicate with variable-length strings of symbols in a reference game. 
Their architecture is based on the proposal \cite{lazaridou2016multi} with the difference that the sender agent only has access to the target image. Furthermore, their agents consist of LSTM layers. Information about the target image is injected by initializing the hidden state of the sender LSTM with the image feature vector, extracted from a pretrained VGG CNN. The sender generates the message by sampling tokens from the vocabulary similarly to the image captioning system by \cite{vinyals2015show} described in Section \ref{image_captioning}. The vocabulary consists of 10,000 tokens, the LSTM hidden size is 512, and the token embeddings are 256-dimensional. 
The receiver interprets the message by encoding it with the LSTM and computing a probability distribution over dot products of image features and the affine-transformed hidden state of the LSTM. The image with the highest probability is chosen as the target. \cite{havrylov2017emergence} further attempt to make the statistical properties of the emergent protocol as similar to natural language as possible by minimizing the KL-divergence between the learned conditional token distribution and a pretrained language model.
They compare agents trained via the \textit{straight-through Gumbel-softmax estimator} to agents trained with REINFORCE. The straight-through Gumbel-softmax estimator is a differentiable approximation of discrete actions (i. e., tokens) with a continuous relaxation applied in the backward pass of the backpropagation, while still using discrete representations in the forward pass. 
That is, one-hot encoded tokens are replaced with samples $w_k$ obtained by sampling $K$ samples  $\{u_k\}_{k=1}^K$ from the random variable $u \sim U(0,1)$. Each $u_k$ is transformed and then used in the sample computation:
\begin{equation}
\begin{aligned}
g_k = -log(-log(u_k)) \\
w_k = \frac{exp((log \; p_k + g_k) / \tau)}{\sum_{i=1}^{K} exp((log \; p_i + g_i) / \tau)}
\end{aligned}
\end{equation}
where $\tau$ is the temperature, $p_i$ and $p_k$ are token probabilites. The samples are discretized with argmax for the forward pass. Thus, it is an alternative to REINFORCE in this setting.
It is argued in the literature that this approximation allows more efficient gradient estimation given larger action spaces for which REINFORCE produces very high variance estimates, e. g., when the vocabulary size is larger \parencite{havrylov2017emergence}.

Experiments on the MS COCO dataset are conducted, selecting one image in batches of 128 as the target. They showed that agents trained with Gumbel-softmax converge faster than those trained with REINFORCE, but both are able to achieve high referential success. Further, they find that communication success increases with maximum message length for both algorithms. They also find that the emergent communication protocol has multiple representations of the same information and that the messages seem to have hierarchical coding. They also conduct an experiment wherein the sender is trained with a combined loss on both an image captioning and reference game task in order to attempt to grounding the tokens in the images. However, this system didn't improve in terms of caption quality. To sum up, this work showed that agents can successfully develop multi-token communication protocols for solving referential tasks.

Similar work is conducted by \cite{lazaridou2018emergence}. The agent architecture closely matches \cite{havrylov2017emergence}, yet the study focuses on comparing reference games with realistic images to games on symbolic data. The protocol learned on symbolic data was shown to exhibit higher topographic similarity to the input data and more compositional features compared to raw pixel input experiments. Nevertheless, agents were able to successfully develop a protocol based on raw input, as well. These results suggest that the proposed architecture does not have an inductive bias sufficient for extracting symbolic compositional features from raw inputs. 

\cite{lazaridou2020multi} whose work is replicated and extended in this thesis use an architecture which combines the architectures by \cite{lazaridou2016multi} and \cite{havrylov2017emergence} in that they also use LSTM based agents, but condition them on combined image features of both the target and the distractor. That is, they conduct reference game experiments with pairs of images. They explore different speaker architectures. More precisely, they compare a speaker learning an emergent communication protocol maximizing the reference task success (``functional learning''), a speaker learning to emit images captions as messages based on ground truth captions in a supervised manner (``structural learning''), and speakers trained with a combination of the two learning approaches \parencite[][p. 4]{lazaridou2020multi}. The functional learning signal is also based on REINFORCE, while the structural learning is conducted with the standard cross-entropy loss. Multiple combined speaker parametrizations are compared: a speaker pretrained on an image captioning task which is then fine-tuned with functional learning on the reference game (conditioned on the target image only) (``reward finetuning''), a speaker trained with a weighted combination of the structural and functional losses (``multi-task learning'') and a speaker pretrained on image captioning and then learning a reranking function based on the reference game (``reward-learned rerankers'') \parencite[][p. 4--5]{lazaridou2020multi}. Two variations of the reranking model are presented; both speakers learn to rerank samples obtained from a pretrained image captioner. The ``product of experts reranker'' reranks the samples proportionally to the message probability times the probability of the message given the image pair. The latter is obtained by re-embedding the sampled captions with a trainable layer as bag-of-words vectors and computing dot ptoducts between the vectors and image embeddings, renormalizing the scores.  The ``noisy channel reranker'' learns the reranking according to Bayes rule, i. e., proportionally to the likelihood of obtaining the target image given the sample times the sample probability. The likelihood can be seen as the speaker's internal listener model and is also computed as the re-normalized dot product between a learned bag-of-words caption representation and each image embedding \parencite[][p. 5--6]{lazaridou2020multi}. The agents were trained on a reference game on the \textit{Abstract Scenes} dataset.
They showed that the ``product of experts reranker'' model outperforms other speakers with respect to referential success in the game, while also counteracting language drift (see Chaper \ref{chapter04} for details). 

More specifically, they conduct a set of evaluations: they evaluate the different trained speakers trained simulataneously with a listener against the same joint listener, a fixed pretrained listener and a human, using an easy and difficult test set consisting of 1000 image pairs each. Furthermore, they vary the source of the samples for the reranker models: they are either the ground truth captions or samples from an image captioner pretrained on single image captioning only. As a baseline, they have human speakers pick the most discriminative caption among the ground truth options for a human listener and achieve an acuracy of 0.97. By contrast, an oracle reranker model operating on the ground truth captions learned against a joint listener only achieves an accuracy of 0.92 with human listeners, indicating that the capacity of the listener and reranker modules are somewhat below human performance. They find that the noisy channel speaker performs best ith humans when trained with a joint listener, while the PoE model performed best with humans when reranking ground truth captions. Investigating the reasons for language drift, they find that the speaker-listener co-adaptation has the least effect on the noisy channel speaker (human performance 0.86 vs 0.87 for joint vs fixed speaker training), and most significant effect on the the reward finetuning with KL speaker (human performance was 0.69 vs. 0.0.75). Finally, they find dthat unfreezing the visual model weights increses the gap between the performance of the joint and human listeners when training the reranker with a joint listener on with ground truth captions. 

Experiments in this thesis focuses on the ``multi-task learning'' architecture. This choice is discussed in more details in Chapter \ref{chapter05}.
\pt{make sure to describe in detail the results of easy-difficult comparison and tests against oracle speakers.}

%The noisy channel reranker model by \cite{lazaridou2020multi} is closely related to work by \cite{andreas2016reasoning}. They also employ a Bayesian model wherein the agents learn to produce discriminative, i. e., task-directed grounded messages in a reference game by reasoning about the other agent. They also use the Abstract Scenes dataset. The architecture consists of a base speaker, base listener and a reasoning speaker. All agent modules are based on fully-connected linear layers with non-linear activations. The literal listener consists of modules embedding input images and the received message, and computing scores over image-message pairs. The literal speaker is used to sample possible messages for a given image by computing scores over vocabulary tokens given the image with a feed-forward conditional language model. The reasoning speaker then derives the optimal message by sampling candidate messages from the literal speaker and reasoning about their utility by ``passing'' them through the literal listener and choosing the message maximizing the listener's referential success probability. This architecture is closely related to idea of Rational Speech Act models \parencite{goodman2016pragmatic}. 
%\cite{andreas2016reasoning} found that referential success increases with the number of samples taken from the literal speaker, as well as with increasing weight of reasoning about the listener in the reasoning speaker model. Further, they find that the reasoning speaker outperforms the literal speaker, indicating that pragmatic reasoning might be crucial for modeling task-conditional language use. \pt{seems highly relevant to my intuition about regularization and strucutral weight. add to discussion.}

\cite{lee2019countering} use another architecture---an attention based sequence-to-sequence machine translation model with one GRU layer---for both agents. 
The agents play a multi-modal translation game wherein the first agent is tasked with the translation of a sequence from French to English and the second---from English to German. Both agents are pretrained on the translation task before the communication game. This setup is used in order to investigate \textit{language drift} of the pivot language English, i. e., its deterioration (see Chapter \ref{chapter04} for details), as the agents are trained on French-German translation with REINFORCE (see Chapter \ref{chapter02} for details). They compare the mitigation of language drift through a language modeling constraint (i. e., incorporating the maximization of the likelihood of the English sentence under a pretrained LM into the reward) to grounding (i. e., maximizing the likelihood of an image corresponding to the English sentence under a pretrained image retrieval model). The LM is a one-layer LSTM; in the grounding model, image features are extracted using a pretrained ResNet-152. \cite{lee2019countering} find that the agents are best able to learn decent French-German translation when mitigating language drift with both an LM constraint and grounding. \pt{check italics of language drift and reference to chapter 4 thru chapter}

\subsection{Other Tasks}

Next to studying communicative success and the ability of agents to use natural language in reference games, multi-agent communication work has also focused on other aspects in reference games as well as studied other tasks.
 
For instance, \cite{evtimova2017emergent} extend the one-iteration reference games to \textit{multi-step interactions}, wherein agents can exchange information back and forth several times. More specifically, their sender agent only has access to visual representations of the set of objects, while the receiver only has access to textual descriptions of the target and distractors. Therefore, the agents need to align their communication across modalities. They compare a feed-forward and an attention-based sender architecture, and a GRU-based and attenion-based receiver, training them with REINFORCE. They find that agents successfully make use of back-and-forth communication about ImageNet images with WordNet based descriptions, showing that agents produce longer conversations on more difficult concepts. Further, they observe an increase in sender's message specificity with progressing conversation iterations.
Their experiments are also related to the multi-step MNIST task variation of \cite{foerster2016learning}.

A further step is taken by \cite{bouchacourt2019miss} who model a decision task wherein one agent is assigned a fruit and the other two tools, the latter having to decide which tool is more suitable for using with the given fruit. The tool utility is retrieved from human judgements. Noteworthily, the type of objects assigned to an agents is varied at random. They observe that the agents develop role-dependent communicative protocols. 

Other work studies the communication in more complex cooperative environments like 2D or 3D grid worlds \parencite{das2019tarmac}. In contrast to other studies, they use the Actor-Critic training method and continuous communication protocols. In short, they find that the benifit of including communication increases with task complexity, and that the learned communication and underlying agent behaviour are intuitive and interpretable. However, \cite{lowe2019pitfalls} also highlight the difficulty of adequately evaluating the added value of communication in complex multi-agent environments.

Finally, a body of work focuses on investigating the properties of emergent languages with the goal of understanding how natural language properties might have developed \parencite{lazaridou2020emergent}. For instance, \cite{graesser2019emergent} show, among other findings, that a shared communication protocol emerges from distinct ones in presence of a single new agent in the community participating in the communication. This presents an important connection to natural language which largely preserves its structure precisely because of the pressure to communicate with different interlocutors and culturally transmit the language to further generations. \pt{reference! and check!} \cite{chaabouni2019anti} investigate whether emergent communicative protocols follow Zipf's law positing that freuquency of messages in a language is anti-correlated with message length. Surprisingly, they find that emergent protocols are \textit{anti-efficient}---that is, the messages that have to be used most frequently are the longest ones. This is explained by a lack of production cost pressure in emergent communication.

Similarly, driven by investigating properties of natural language, a lot of work has focused on \textit{compositionality}.\footnote{Measuring compositionality is an important and not trivial aspect of this line of work. For an overview of different approaches, see, e.g., \cite{lazaridou2020emergent}.} For instance, \cite{lazaridou2018emergence} show that compositional communication emerges more easily from symbolic input than raw pixel input. 
\cite{chaabouni2020compositionality} show that compositionality in emergent protocols facilitates language transmission, but is not predictive of its generalization potential. The emergence of compositionality is shown to be strongly dictated by the variability of the input environment. Importantly, they also use a structured symbolic input representation.
This is in line with the observation made in human experiments and grounded in theoretical work that ``generational transmission of language favors compositionality'' (\cite{lazaridou2020emergent}, p. 12; cf. \cite{kirby2014iterated}).
\cite{luna2020internal} investigate cognitively plausible internal and external pressures which might influence compositionality---the ``principle of least effort'' (i.e., keeping the messages as short as possible) and ``object constancy'' (i.e., grouping together constant patterns into conceptual classes by abstracting away from context-contingent variation) (p. 1). More precisely, operationalizations of the latter principle in terms of sensitivity towards location invariance, color constancy and the distribution of objects in the environment are compared. The results show that the least effort pressure makes the protocols less redundant. Furthermore, agents pressured towards object constancy produce protocols with highest compositionality scores, while some operationalizations also improve zero-shot generalization abilities. 

To sum up, these studies show that emergent communicative protocols often exhibit properties that are far from natural language. Nevertheless, the ability to test the effect of different environmental and architectural pressures on emergent properties makes multi-agent communication a fascinating avenue for further research. 
%Other evolutionary perspective: agent co-evolution \parencite{dagan2020co}.

%\textbf{Properties of the communication \parencite{lazaridou2020multi}}: difficulty to converge on consistent meanings of tokens for color values, generally task-oriented codes with complexity minimization. Anti-efficient code development --> I am looking somewhat into this with my different-lengths experiments and grammars.  Larger communities lead to more systematic languages --> fixed listener experiment.

\pt{it would be good to have something with different listeners, i e the cultural transmission aspect.}
%Compositionality. also paper by Bruni.

\pt{Don't forget chapter summary.}