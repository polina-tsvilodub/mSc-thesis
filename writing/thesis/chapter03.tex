
This chapter reviews related work on multi-agent communication---the core domain in which the presented experiments are situated. First, the task and communicative setting which are used in this work are presented. Then, different architectures used for building the agents as well as related studies are reviewed. Finally, additional communciative settings are summarized. 
%Finally, the paper by \cite{lazaridou2020multi} which serves as the starting point for this thesis is summarized in detail.  

%\pt{maybe find a way to say that there are other neighboring areas of research like visual dialogue, multimodal semantics, visual question answering etc. possibly in discussion.}

\section{Introduction}
Research on multi-agent communication has received increasing attention over the recent years, as this domain focuses on research questions of high relevance to cognitive science \parencite{lazaridou2020emergent}. More precisely, two main areas of research are often addressed: first, the conditions under which a language emerges and which emergent properties can be observed; and second, the ability of artificial agents to pick up a given language system. % thus investigating whether tractable human-agent communication channels can be learned via realistic interaction.
The second direction has also been considered with respect to \textit{grounding} a given language in the visual world, i.e., teaching agents to use language applied to realistic visual input are investigated. Grounding is important both from modeling and cognitive perspectives: for instance, \cite{bruni2014multimodal} show that representations learned from both text and image data are more plausible than purely text-based representations; furthermore, vision is an integral part of human communication \parencite{tomasello2010origins, harnad1990symbol}. 

These two research directions have different implications for understanding human language: the former often focuses on what the causal factors behind prominent features of human language like compositionality are, allowing to shed light on human language evolution; the second rather addresses how we can train agents to communicate optimally for interacting with humans. Equivalently, these two main directions can be classified as investigating \textit{language emergence} and \textit{language acquisition} among and by artificial agents \parencite{lazaridou2018emergence, lazaridou2020emergent}.
Presented work focuses on the latter question, aiming to train agents to use English language grounded in real-world images.
%Irrespectively of the research focus, artifical agents which interact with each other are at the core of multi-agent communication studies. Therefore, first, different agent architectures are presented in the next section.
%\pt{I feel like a section on super general aspects, like having two agents doing some task and being jointly trained, is missing. Maybe visuals would be cool.}

Communication has been embedded in various tasks simulated in multi-agent settings. In some studies the idea was to use communication for improved cooperation \parencite[e.~g.,][]{foerster2016learning}; in other tasks, communication about the environment itself is the core task \parencite[e.~g.][]{lazaridou2018emergence}. The latter allows to focus on properties of communicative systems that artificial agents might develop or acquire, thus allowing to compare them to human language. Since studying the ability of artificial agents to produce \textit{discriminative captions} for images in context within reference games is the goal of this work, this task and related work with human participants is described first, before transitioning to related work on multi-agent communication.

%\pt{just to a section the chapter proceeds as follows and through out first paragraph and remove intro section.}

\section{Reference Games}
\label{reference_games}

Reference games are a type of the so-called \textit{Lewis singaling game} \parencite{lewis1969convention, skyrms2010signals}.
Signaling games were developed as an accont of \textit{conventional meaning} \parencite{grice1975logic}. Traditionally, these games include two agents, a sender and a receiver. Only the sender observes a state sampled at random and chooses a message which is sent to the receiver. The set of possible messages is known to both agents. Based on the message, the receiver selects a state she chose based on the message. The game is a success if the guessed state and the state observed by the sender match, such that this game is driven by the agents having a common interest \parencite{lewis1969convention}. 

\textit{Reference games} are a specific instance of signaling games in which the meaning is already conventionally established. Here, the sender samples a particular target among a set of distractors which may be a subset of all possible objects in the given world and \textit{refers} to it with her message. The target and the set of distractors are accessible to both agents. The receiver's task is then the identification of the target. Thus, the target and the distractors which in practice are often represented by visual context play a critical role in reference games.

\pt{The following paragraph is still in progress and revision.} Reference games have been employed in a wide range of human studies with various research questions. For example, \cite{franke2016reasoning} used a reference game for a comprehension and a production study to study pragmatic reasoning. They investigated whether pragmatic reasoning is pervasive throughout individual interlocutors or arises at population level. \cite{graf2016animal} study the communicative task of reference. They conducted an experiment wherein participants generated referential expressions for a target image presented in context of distractors which either belonged to the same or to a different basic-level object category.\footnote{Basic-level categories refer to general conceptual categories like dogs, birds or flowers. Several subordinate, i.e., more specific categories like Great Danes or hummingbirds, are comprised within basic-level categories. On the other hand, more general categories like mammals or plants are called superordinate categories \parencite{rosch1976basic}.} They showed that participants generated subordinate category level, i.e., more specific, referential expressions in presence of more similar distractors, but not for dissimilar ones. Thus, they showed that humans flexibly increase the specificity of nominal expressions they use in order to unambiguously refer to the target, when required by the context. However, human speech also presents abundant examples of \textit{overmodification} in referential expressions, i.e., cases of use of additional modifiers like color expressions even when it is not strictly necessary for discriminating the target in the given conext. \cite{degen2020redundancy} explained this phenomenon in terms of complexity of the visual scenes, typicality of the referents and of the described features. They revealed complex pragmatic reasoning underlying this phenomenon. Much more work has been done on referential expressions both experimentally and theoretically; these examples go to show that, guided by pragmatic reasoning, humans inherently generate highly discriminative, or, contrastive, messages in situations like reference games. Even further, they also expect and anticipate such behavior from other interlocutors, as has been shown in the processing literature \parencite[e. g., cf.]{sedivy1999achieving}.

A computational perspective on studiying reference was first taken by \cite{dale1995computational}. Followingly, many studies have used reference games in computational simulations. Applied to artificial agents, a reference game proceeds as follows: arrays containing a target image among $n \geq 1$ distractor images are sampled from the set of all images $I$. Both agents have access to the images and usually share the same representation; the sender agent knows which image is the target, the receiver does not. The sender emits a message $m, |m| \geq 1$ (assuming discrete communication; see Section \ref{multi_agent_arch}) sampled from the shared vocabulary $V$. Given $m$, the receiver has to correctly guess which of the images in the array is the target. Experiments in this thesis are limited to using one target image in context of only one distractor, but qualitative results are not expected to depend on this confinement.

%\pt{Upshot: hypothesis about differing lengths as a proxy for different granularity of description. Bruni's paper as ground to believe that this might not be borne out as the bias in terms of length in the training dataset is too strong.}

\section{Discriminative Language Use}

Before turning to approaches wherein agents learn discriminative message generation from interaction in reference games, rather machine learning-based approaches to building discriminative image captioning systems are reviewed. Additionally, work inspired by the Rational Speech Act family of models is reviewed \parencite{goodman2016pragmatic}.

\cite{sadovnik2012image} first approached discriminative image captioning by looking at visual discriminablity and saliency, and constructing captions based on hand-crafted rules.

Moving to deep architectures, \cite{vedantam2017context} built an image captioning model which performed joint inference over a language model that was context-agnostic and a listener model which provided feedback on discriminativity of the sampled captions. More precisely, they introduced a ``reasoning speaker'' consisting of a basic image captioning speaker and a listener model. The basic speaker was an image captioner, additionally conditioned on target concepts represented in the images. The listener only depended on the basic speaker, as it computed the log-likelihood ratio between the speaker's utterances for a given image but for different concepts in that image. The overall speaker was then trained to maximize the basic speaker message probability while also maximizing the likelihood ratio representing the discriminativity of the correct concept over others.
They applied the model to a justification task wherein the image features corresponding to a given discriminative aspect of the caption had to be explained. They also applied it to standard image captioning on similar image pairs from the MS COCO dataset. Their architecture was able to generate captions which led to higher discrimination success of human evaluators, compared to non-discriminative baselines.

\cite{dai2017contrastive} proposed the ``Contrastive Learning'' training objective for this task which constrained the image captioner to maximize the true caption likelihood compared to a pretrained reference image captioner, while learning to assign lower probabilities to mismatching captions for the target image, compared to that same reference model. Critically, the success of the system depends on the quality of the reference model. Their model achieved competitive results on the MS COCO test set.

While alleviating some modeling issues like language drift (see Chapter \ref{chapter04} for details), these systems miss a critical component in their approach to the reference task---they are missing the communicative and interactive context of applying such image captions for successful reference establishment. Yet interactivity and social aspects of the communicative task are critical for developing systems which are supposed to have potential for interacting with humans \parencite{lazaridou2020emergent}. One of the most prominent framework formalizing social and cooperative aspects of communication is the Rational Speech Act (RSA) framework \parencite{goodman2016pragmatic}. Some approaches to discriminative image captioning are closely inspired by RSA and thereby incorporate interactive communication aspects into their models.\footnote{Basic knowledge of the RSA family of models is presupposed. For a gentle introduction, see, e.g., \cite{goodman2016pragmatic, problang}.}

\cite{andreas2016reasoning} were the first ones to employ an RSA-style Bayesian model wherein agents learned to produce discriminative messages in a reference game by reasoning about each other. The architecture consisted of a base speaker, base listener and a reasoning speaker. All agent modules were based on fully-connected linear layers with non-linear activations. The literal listener consisted of modules embedding input images and the received message, computing scores over image-message pairs. The literal speaker was used to sample possible messages for a given image by computing scores over vocabulary tokens given the image with a feed-forward language model. The reasoning speaker then derived the optimal message by sampling candidate messages from the literal speaker and reasoning about their utility by ``passing'' them through the literal listener and choosing the message maximizing the listener's referential success probability. 
\cite{andreas2016reasoning} found that referential success increased with the number of samples taken from the literal speaker, as well as with increasing weight of reasoning about the listener in the reasoning speaker model. Further, they found that the reasoning speaker outperformed the literal speaker, indicating that pragmatic reasoning might be crucial for modeling discriminative language use. 

\cite{cohn2018pragmatically} also built a model following the standard RSA principles. Their model performed character-wise caption generation. This generation strategy was chosen in order to constrain the space of possile alternatives the speaker may choose among when generating the caption. Therefore, their generation process was an incremental one, modeled by an LSTM-based speaker agent. Otherwise, their model was similar to \cite{andreas2016reasoning}. Their model  turned out to outperform word-level generation and non-pragmatic models, further supporting the importance of considering an interactive collaborative setting for successful discriminative language use.

\cite{nie2020pragmatic} proposed an issue-sensitive image captioner, also based on the RSA family of models. Issue-sensitivity was represented as partitioning the space of images into cells with images that shared the same relevant (i.e., \textit{at-issue}) feature as the target belonging to one cell. Their base speaker was a standard  pretrained image captioning model. Additionally to standard RSA mechanics, they extended the pragmatic speaker to be sensitive to the relevant partitioning. They further included a utility term in order to tackle arising semantic drift (e.~g., mentioning features that are not true of the target) of the captions and constrain the speaker from overgenerating. Akin to \cite{cohn2018pragmatically}, this model also employed an incremental message generation procedure. Evaluations included visual question answering (VQA) on MS COCO \parencite{chen2015microsoft}, showing that the model learned adequate issue representations from questions as well as generated promising issue-sensitive captions. 

Finally, an approach fully embracing the idea of learning language use from active interaction, not just representations of other agents,  is multi-agent communication. The next section turns to work on discriminative language learning and other related research in this domain.

\section{Multi-Agent Communication}
\label{mac}
Irrespectively of the research focus, artifical agents which interact with each other are at the core of multi-agent communication studies.
The main architectural differences concern the type of model representing the artificial agent and the type of communication channel they use.
As described in Section \ref{rl}, experiments wherein agents learn to complete a task by interacting with the environment and with each other are typically trained with reinforcement learning. Multi-agent communication is a specific type of RL tasks in that (one of) the tasks agents learn are communicative and cooperative, and one of the agents can be considered the sender and the other the receiver of the communication \parencite[cf.][]{tan1993multi, lazaridou2016multi}.
Based on the chosen communication channel and the precise communicative task, specific architectural decisions and optimization algorithms differ. 

\subsection{Agent Architectures and Training in Reference Games}
\label{multi_agent_arch}

While early work in multi-agent communication focuses on structured agents \parencite[e.~g.,~see][for reviews]{christiansen2003language, cangelosi2002symbol}, deep agent architectures became increasingly dominant in the domain over the last years \parencite{lazaridou2020emergent}. As this thesis also uses deep neural network agents, the review of related work focuses on deep multi-agent communication experiments. 

%Architectural considerations: 
%1) agent architecture: hand crafted vs deep; for deep: kind of network, esp. recurrent layer
%2) loss and training approach: Q-learning \parencite{foerster2016learning}, Gumbel-Softmax \parencite{havrylov2017emergence}, REINFORCE.  \parencite{lazaridou2020emergent, williams1992simple}. 

\cite{lazaridou2016multi} first showed that neural agents can efficiently reference realistic visual inputs in the presence of distractors by developing a communication protocol from scratch. To this end, \textcite{lazaridou2016multi} conducted several experiments employing feed-forward and CNN agents. In particular, in their main experiment, they set up a reference game, where a sender agent emitted a single discrete symbol referring to one of two images provided to both the sender and the receiver, sampled from the ImageNet dataset \parencite{deng2009imagenet}. The discretization of the agents' communication channel is sometimes also called the \textit{discrete bottelneck} \parencite{lazaridou2020multi}. 
The receiver agent had to guess  which image was the intended target, based on the symbol received from the sender (see Section \ref{reference_games} for details). %They use a subset of the dataset containing 100 images from each of 20 general categories.
\cite{lazaridou2016multi} set up two versions of the sender. The ``agnostic sender'' consisted of a two-layer feed forward neural network, embedding both images and producing a score over the vocabulary given the concatenation of image embeddings \parencite[][p. 3]{lazaridou2016multi}. The ``informed sender'' consisted of a three-layer neural network, applying two convolutional layers to embeddings of both images which were treated as different channels \parencite[][p. 3]{lazaridou2016multi}. It also output scores over the vocabulary. For both architecures the vocabulary scores were converted into a Gibbs distribution from which the emitted message symbol was sampled. The receiver agent consisted of a two-layer feed forward network which embedded both images and computed the dot products between these embeddings and the one-hot encoded message. The final target image choice was sampled from the Gibbs distribution computed from the dot products. %The agents are trained with REINFORCE which updates parameters of the speaker and listener policies by minimizing the negative expected reward. The speaker and listener policies are parametrised by the weights of the respective neural networks. The sender policy is $\pi(v \in V \mid i_L, i_R, t \in \{L, R\})$ where $V$ is the vocabulary, $i_L, i_R$ are the left and right images and $t$ is the position of the target. The receiver policy is $\pi(t \in \{L, R\}\mid v_s, i_L, i_R)$, where $v_s$ is the message received from the sender and $t$ is the guessed position of the target image. \textcite{lazaridou2016multi} set the reward to 1 if the image picked by the receiver is the target and 0 otherwise. 
They found that agents achieved high referential success, and that the informed sender produced semantically more natural symbols (i.e., the same ones for the same image categories) than the agnostic one. In a further experiment, the authors successfully pressured the agents to communicate about more high-level properties by sampling instances of different subcategories within the same basic-level category for the image pairs.%, and achieve a small increase in label purity (i. e., in the proportion of labels agreeing with the major cluster label). 
Finally, they performed an experiment grounding the communication in natural language wherein the sender agent had to use natural language category labels as message symbols. The sender was trained by switching between reference game play and image classification on ImageNet data. However, the vocabulary was limited to 100 words, and the sender only produced one-word messages, making the experiment rather far from realistic scenarios. %They observe a higher symbol purity in this experiment, while retaining referential success. 	
%	\item They use REINFORCE for training the speaker. Overall framing: interactive communicative setting for training agents allows to capture functional aspects of communication, i.e., the goal-directed nature of communication, as opposed to purely supervised conversational agent training. 
%	\item \textit{multi-agent coordination communication game}: first functional task (a basic function of language): reference
	
Another foundational study in deep multi-agent communication was conducted by \cite{foerster2016learning}. In contrast to \cite{lazaridou2016multi}, the authors let the agents use a centrally trained system in their ``DIAL'' architecture (i. e., with parameter sharing across agents)  which could also be extended to continuous communication. This system was compared to the ``RIAL'' system where agents used traditional discrete communication \parencite[][p. 2]{foerster2016learning}. This was one of the first studies using a recurrent network in the sender architecture.
For both systems, the sender consisted of an input network with a fully connected layer and lookup tables creating embeddings of the task environment, two recurrent GRU layers, and two feed-forward output layers.
The receiver had the same architecture. The DIAL agents were additionally connected via a continuous vector which can be seen as an activation layer passing the internal state of the sender to the receiver.
Experiments with different variants of a riddle task and an image classification task on the MNIST dataset were conducted. Results on the former task showed that the agents were able to learn an optimal policy although DIAL agents converged faster, yet sharing the weights between the sender and receiver agents was critical for RIAL. DIAL with parameter sharing outperformed other experiments. Furthermore, they found that adding noise to the communication was critical for successful learning for RIAL. Results of the second task provided similar results for DIAL, while RIAL failed to converge stably on the multi-step interaction version of this task. 
The discrete system was trained with deep $Q$-learning (see Section \ref{rl_methods}), while the continuous system was trained by additionally backpropagating gradients end-to-end between the two agents. %Furthermore, the former but not the latter case includes modeling wherein agents have access to each other's internal representations of the environment, which was also argued to be unnatural when developing a system to investigate human communication \pt{REF}. 
The DIAL results pose an interesting question regarding the comparability of such a system where the internal representations of a speaker are directly passed to the listener to human communication \parencite[cf.][]{lazaridou2020emergent, hockett1960origin}. The authors, however, argue that this direct error propagation can be interpreted as communicative feedback which is also part of human communication. Further, while the possibility to use DIAL with continuous communication is attractive from a machine learning perspective due to its differentiability, human languages are typically considered discrete signalling systems \parencite{hockett1960origin}. 

\cite{havrylov2017emergence} further extended work on multi-agent communication by modeling agents which communicated with variable-length strings of symbols in a reference game. 
Their architecture was based on the proposal by \cite{lazaridou2016multi} with the difference that the sender agent only had access to the target image. Furthermore, their agents consisted of LSTM layers. Information about the target image was injected by initializing the hidden state of the sender LSTM with the image feature vector, extracted from a pretrained VGG CNN. The sender generated the message by sampling tokens from the vocabulary similarly to the image captioning system by \cite{vinyals2015show} described in Section \ref{image_captioning}. The vocabulary consisted of 10,000 tokens, the LSTM hidden size was 512, and the token embeddings were 256-dimensional. 
The receiver interpreted the message by encoding it with an LSTM and computing a probability distribution over dot products of image features and the affine-transformed hidden states of the LSTM. The image with the highest probability was chosen as the target. \cite{havrylov2017emergence} further attempted to make the statistical properties of the emergent protocol as similar to natural language as possible by minimizing the KL-divergence between the learned conditional token distribution and a pretrained language model.
They compared agents trained via the \textit{straight-through Gumbel-softmax estimator} to agents trained with REINFORCE. The straight-through Gumbel-softmax estimator is a differentiable approximation of discrete actions (i.e., tokens) with a continuous relaxation applied in the backward pass of the backpropagation, while still using discrete representations in the forward pass. 
That is, one-hot encoded tokens are replaced with samples $w_k$ obtained by sampling $K$ samples  $\{u_k\}_{k=1}^K$ from the random variable $u \sim U(0,1)$. Each $u_k$ is transformed and then used in the sample computation
\begin{equation}
\begin{aligned}
g_k = -log(-log(u_k)) \\
w_k = \frac{exp((log \; p_k + g_k) / \tau)}{\sum_{i=1}^{K} exp((log \; p_i + g_i) / \tau)}
\end{aligned}
\end{equation}
where $\tau$ is the temperature, $p_i$ and $p_k$ are token probabilites. The samples are discretized with argmax for the forward pass. Thus, it is an alternative to REINFORCE in this setting.
It is argued in the literature that this approximation allows more efficient gradient estimation given larger action spaces for which REINFORCE produces very high variance estimates---e.~g., when the vocabulary size is large \parencite{havrylov2017emergence}.

Experiments on the MS COCO dataset were conducted, selecting one image in batches of 128 as the target. \cite{havrylov2017emergence} showed that agents trained with Gumbel-softmax converged faster than those trained with REINFORCE, but both were able to achieve high referential success. Further, they found that communication success increased with maximum message length for both algorithms. They also found that the emergent communication protocol had multiple representations of the same information and that the messages seemed to have hierarchical coding. They also conducted an experiment wherein the sender was trained with a combined loss on both an image captioning and reference game task in order to attempt grounding the tokens in the images. However, this system didn't improve in terms of caption quality. To sum up, this work showed that agents can successfully develop multi-token communication protocols for solving referential tasks.

Similar work was conducted by \cite{lazaridou2018emergence}. The agent architecture closely matched \cite{havrylov2017emergence}, yet the study focused on comparing reference games with realistic images to games on symbolic data. The protocol learned on symbolic data was shown to exhibit more compositional features compared to raw pixel input experiments. Nevertheless, agents were able to successfully develop a protocol based on raw input, as well. These results suggested that the proposed architecture did not have an inductive bias sufficient for extracting symbolic compositional features from raw inputs. 

\cite{lazaridou2020multi} whose work is replicated and extended in this thesis used an architecture which combined the architectures by \cite{lazaridou2016multi} and \cite{havrylov2017emergence} in that they also used LSTM-based agents in a reference game, but conditioned them on combined image features of both the target and the distractor. They explored different speaker architectures. More precisely, they compared a speaker learning an emergent communication protocol maximizing the reference game success (``functional learning''), a speaker learning to emit image captions as messages based on ground truth captions in a supervised manner (``structural learning''), and speakers trained with a combination of the two learning approaches \parencite[][p.~4]{lazaridou2020multi}. The functional learning signal was also based on REINFORCE, while the structural learning was conducted with the standard cross-entropy loss. Multiple combined speaker parametrizations were compared: a speaker pretrained on an image captioning task which was then fine-tuned with functional learning on the reference game (conditioned on the target image only; ``reward finetuning''), a speaker trained with a weighted combination of the structural and functional losses (``multi-task learning'') and a speaker pretrained on image captioning and then learning a reranking function based on the reference game (``reward-learned rerankers'') \parencite[][p.~4--5]{lazaridou2020multi}. Two variations of the reranking model were presented. Both speakers learned to rerank samples obtained from a pretrained image captioner. The ``product of experts'' model reranked the samples proportionally to the message probability times the probability of the message given the image pair. The latter was obtained by re-embedding the sampled captions with a trainable layer as bag-of-words vectors and computing dot products between the vectors and image embeddings, renormalizing the scores.  The ``noisy channel'' reranker learned the reranking according to Bayes' rule, i.e., proportionally to the likelihood of obtaining the target image given the sample times the sample probability. The likelihood can be seen as the speaker's internal listener model and was also computed as the re-normalized dot product between a learned bag-of-words caption representation and each image embedding \parencite[][p.~5--6]{lazaridou2020multi}. The agents were trained on a reference game on the \textit{Abstract Scenes} dataset \parencite{zitnick2013bringing}.
Overall, they showed that the ``product of experts'' model outperformed other speakers with respect to referential success in the game, while also counteracting \textit{language drift} (see Chaper \ref{chapter04} for details). 

More specifically, in order to investigate language drift, they conducted a set of evaluations: they evaluated the different speakers trained simulataneously with a listener against the same \textit{joint} listener, a \textit{fixed} pretrained listener and a human, using an easy and difficult test set consisting of 1000 image pairs each. Furthermore, they varied the source of the samples for the reranker models: they were either the ground truth captions or samples from an image captioner pretrained on single image captioning only. As a baseline, they had human speakers pick the most discriminative caption among the ground truth options for a human listener and achieved an acuracy of 0.97. By contrast, the reranker model operating on the ground truth captions trained against a joint listener only achieved an accuracy of 0.92 with human listeners, indicating that the capacity of the listener and reranker modules were somewhat below human performance. They found that the noisy channel speaker performed best with humans when trained with a joint listener, while the PoE model performed best with humans when reranking ground truth captions. Investigating the reasons for language drift, they found that the speaker-listener co-adaptation had the least effect on the noisy channel speaker (human performance was 0.86 vs 0.87 for joint vs fixed speaker training), and had most significant effect on the reward finetuning speaker with KL regularization (human performance was 0.69 vs. 0.0.75). Finally, they found that unfreezing the visual model weights incresed the gap between the performance of the joint and human listeners when training the reranker with a joint listener with ground truth captions. 

Experiments in this thesis focus on the ``multi-task learning'' architecture. This choice is discussed in more details in Chapter \ref{chapter05}.


%The noisy channel reranker model by \cite{lazaridou2020multi} is closely related to work by \cite{andreas2016reasoning}. They also employ a Bayesian model wherein the agents learn to produce discriminative, i. e., task-directed grounded messages in a reference game by reasoning about the other agent. They also use the Abstract Scenes dataset. The architecture consists of a base speaker, base listener and a reasoning speaker. All agent modules are based on fully-connected linear layers with non-linear activations. The literal listener consists of modules embedding input images and the received message, and computing scores over image-message pairs. The literal speaker is used to sample possible messages for a given image by computing scores over vocabulary tokens given the image with a feed-forward conditional language model. The reasoning speaker then derives the optimal message by sampling candidate messages from the literal speaker and reasoning about their utility by ``passing'' them through the literal listener and choosing the message maximizing the listener's referential success probability. This architecture is closely related to idea of Rational Speech Act models \parencite{goodman2016pragmatic}. 
%\cite{andreas2016reasoning} found that referential success increases with the number of samples taken from the literal speaker, as well as with increasing weight of reasoning about the listener in the reasoning speaker model. Further, they find that the reasoning speaker outperforms the literal speaker, indicating that pragmatic reasoning might be crucial for modeling task-conditional language use. \pt{seems highly relevant to my intuition about regularization and strucutral weight. add to discussion.}

Finally, \cite{lee2019countering} used another architecture---an attention based sequence-to-sequence machine translation model with one GRU layer---for both agents in a multi-agent communication task. More specifically, the agents played a multi-modal translation game wherein the first agent was tasked with the translation of a sequence from French to English and the second---from English to German. Both agents were pretrained on the translation task before the communication game. This setup was used in order to also investigate language drift, namely of the pivot language English, as the agents were trained on French-German translation with REINFORCE (see Chapter \ref{chapter02} for details). They compared the mitigation of language drift through a language modeling constraint (i.e., incorporating the maximization of the likelihood of the English sentence under a pretrained LM into the reward) to grounding (i.e., maximizing the likelihood of an image corresponding to the English sentence under a pretrained image retrieval model). The LM was a one-layer LSTM; in the grounding model, image features were extracted using a pretrained ResNet-152. \cite{lee2019countering} found that the agents were best able to learn decent French-German translation when mitigating language drift with both an LM constraint and grounding. 

To sum up, several studies investigated multi-agent communication in a reference game setting and employed different agent architefctures for that goal. The next section outlines how similar architectures can be used in order to address other research questions.

\subsection{Other Tasks}

Next to studying communicative success and the ability of agents to use natural language in reference games, multi-agent communication work has also focused on other aspects in reference games as well as studied other tasks.
 
For instance, \cite{evtimova2017emergent} extended one-iteration reference games to \textit{multi-step interactions}, wherein agents could exchange information back and forth several times. More specifically, their sender agent only had access to visual representations of the set of objects in a reference game, while the receiver only had access to textual descriptions of the target and distractors. Therefore, the agents needed to align their communication across modalities. They compared a feed-forward and an attention-based sender architecture, and a GRU-based and an attenion-based receiver, training them with REINFORCE. They found that agents successfully made use of back-and-forth communication about ImageNet images with WordNet based descriptions, showing that agents produced longer conversations on more difficult concepts. Further, they observed an increase in sender's message specificity with progressing conversation iterations.
Their experiments were also related to the multi-step MNIST task variation of \cite{foerster2016learning}.

A further step was taken by \cite{bouchacourt2019miss} who modeled a decision task wherein one agent was assigned a fruit and the other two tools, the latter having to decide which tool was more suitable for using with the given fruit. The tool utility was retrieved from human judgements. Noteworthily, the type of objects assigned to an agents was varied at random. They observed that the agents developed role-dependent communicative protocols, assigning different messages to the same concepts depending on their role. 

Other work studied the communication in more complex cooperative environments like 2D or 3D grid worlds \parencite{das2019tarmac}. In contrast to other studies, they used the Actor-Critic training method and continuous communication protocols (cf.~Chapter \ref{chapter02}). In short, they found that the benifit of including communication increased with task complexity, and that the learned communication and underlying agent behaviour were intuitive and interpretable. However, \cite{lowe2019pitfalls} also highlight the difficulty of adequately evaluating the added value of communication in complex multi-agent environments.

Finally, a body of work focuses on investigating the properties of emergent languages with the goal of understanding how natural language properties might have developed \parencite{lazaridou2020emergent}. For instance, \cite{graesser2019emergent} showed, among other findings, that a shared communication protocol emerged from distinct ones in presence of a single new agent in the community participating in the communication. This presents an important connection to natural language which preserves its structure precisely because of the pressure to communicate with different interlocutors and transmit the language to further generations \parencite{tomasello2010origins}. \cite{chaabouni2019anti} investigated whether emergent communicative protocols followed Zipf's law which posits that freuquency of messages in a language is anti-correlated with message length. Surprisingly, they found that emergent protocols were \textit{anti-efficient}---that is, the messages that had to be used most frequently were the longest ones. This was explained by a lack of production cost pressure in emergent communication.

Similarly, driven by investigating properties of natural language, a lot of work has focused on \textit{compositionality}.\footnote{Measuring compositionality is an important and not trivial aspect of this line of work. For an overview of different approaches, see, e.g., \cite{lazaridou2020emergent}.} For instance, \cite{lazaridou2018emergence} showed that compositional communication emerges more easily from symbolic input than raw pixel input. 
\cite{chaabouni2020compositionality} showed that compositionality in emergent protocols facilitated language transmission, but was not predictive of its generalization potential. The emergence of compositionality was shown to be strongly dictated by the variability of the input environment. Importantly, they also used a structured symbolic input representation.
This finding is also in line with the observation made in human experiments and grounded in theoretical work that ``generational transmission of language favors compositionality'' (\cite{lazaridou2020emergent}, p. 12; cf. \cite{kirby2014iterated}).
\cite{luna2020internal} investigated cognitively plausible internal and external pressures which might influence compositionality---the ``principle of least effort'' (i.e., keeping the messages as short as possible) and ``object constancy'' (i.e., grouping together constant patterns into conceptual classes by abstracting away from context-contingent variation; p.~1). More precisely, operationalizations of the latter principle in terms of sensitivity towards location invariance, color constancy and the distribution of objects in the environment were compared. The results showed that the least effort pressure made the protocols less redundant. Furthermore, agents pressured towards object constancy produced protocols with highest compositionality scores, while some operationalizations also improved zero-shot generalization abilities. 

To sum up, these studies show that emergent communicative protocols often exhibit properties that are far from natural language. Nevertheless, the ability to test the effect of different environmental and architectural pressures on emergent properties makes multi-agent communication a fascinating avenue for further research. 
%Other evolutionary perspective: agent co-evolution \parencite{dagan2020co}.

%\textbf{Properties of the communication \parencite{lazaridou2020multi}}: difficulty to converge on consistent meanings of tokens for color values, generally task-oriented codes with complexity minimization. Anti-efficient code development --> I am looking somewhat into this with my different-lengths experiments and grammars.  Larger communities lead to more systematic languages --> fixed listener experiment.

%\pt{it would be good to have something with different listeners, i e the cultural transmission aspect. also Jaques et al with social aspects.}
%Compositionality. also paper by Bruni.

Overall, this chapter reviewed prior work on multi-agent communication, in particular showing how artificial agents can be trained in reference game settings. However, little has been said about the issues connected to teaching the agents to use natural language in reference games. The next chapter turns towards these issues.