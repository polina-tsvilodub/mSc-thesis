import torch
import torch.nn as nn
from agents import resnet_encoder
from . import image_captioner
from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer, TransfoXLConfig


class DriftMeter():
    """
    A class instantiating models and functions for computing language drift.
    """
    def __init__(self, semantic_encoder, semantic_decoder, structural_model, embed_size, vis_embed_size, hidden_size, vocab):
        """
        Initialize the object holding all functions and models for 
        computing the language drift metrics. Importantly, the models are only loaded once.

        Arguments:
        ---------
        semantic_encoder: path & name to weights of pretrained speaker MLP encoder 
        semantic_decoder: path & name to weights of pretrained LSTM 
        structural_model: string name of pretrained conditional LM from huggingface
        structural_tokenizer: string to pretrained huggingface tokenizer matching the LM 
        embed_size: int
        vis_embed_size: int
        vocab: length of vocab
        """
        super(DriftMeter, self).__init__()
        self.structural_model = TransfoXLLMHeadModel.from_pretrained(structural_model)
        self.structural_model.eval()
        self.tokenizer = TransfoXLTokenizer.from_pretrained(structural_model)
        self.decoder = semantic_decoder
        self.encoder = semantic_encoder
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.visual_embed_size = vis_embed_size
        self.vocab_len = vocab
        # instantiate models
        self.semantic_encoder = resnet_encoder.EncoderCNN(self.visual_embed_size)
        self.semantic_decoder = image_captioner.ImageCaptioner(self.embed_size, self.hidden_size, self.vocab_len) # this is a 1-image conditioned one now (with prepenading of embedding)
        self.semantic_encoder.load_state_dict(torch.load(self.encoder))
        self.semantic_decoder.load_state_dict(torch.load(self.decoder))
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.semantic_encoder.to(device)
        self.semantic_decoder.to(device)
        # set to .eval()
        self.semantic_encoder.eval()
        self.semantic_decoder.eval()
        # softmax for computing the probabilities over the scores
        self.softmax = nn.Softmax(dim=-1)
        


    # language drift metric (a)
    def compute_discrete_overlap(self, generated_cap, target_cap, distractor_cap):
        """
        Compute an overlap score over the generated caption with the two ground truth captions.
        This score is an attempt to capture language drift, while being agnostic towards compositional alternations.
        
        For batched input, expected input shape is (batch_size, caption_len), output is (batch_size,)
        generated_cap: index lists
            Caption generated by the speaker.
        target_cap:
            Ground truth caption for the target image.
        distractor_cap:
            Ground truth caption for the distractor image.
        """
        target_score_list = [i == j for i, j in list(zip(generated_cap, target_cap))]
        # target_score_list = torch.eq(generated_cap, target_cap)
        distractor_score_list = [i == j for i, j in list(zip(generated_cap, distractor_cap))]
        # distractor_score_list = torch.eq(generated_cap, distractor_cap)
        overlap_score = sum(target_score_list) - sum(distractor_score_list)
        # overlap_score = target_score_list.sum(dim=1) - distractor_score_list.sum(dim=1) 
        
        return overlap_score

    # metric (b)

    def compute_cont_overlap(self, generated_cap, target_cap, distractor_cap):
        """
        Compute an overlap score over the generated caption with the two ground truth captions.
        This score is an attempt to capture language drift, while being agnostic towards compositional alternations.
        
        generated_cap: embeddings tensors (batch_size, len_caption, embed_size)
            Caption generated by the speaker.
        target_cap:
            Ground truth caption for the target image.
        distractor_cap:
            Ground truth caption for the distractor image.
            
        Returns:
            overlap_scor (batch_size,)
                tensor of cosine similarity scores by-caption.
        """
        target_dist = nn.functional.cosine_similarity(generated_cap, target_cap, dim=-1) # elementwise, then take average 
        print(target_dist)
        distractor_dist = nn.functional.cosine_similarity(generated_cap, distractor_cap, dim=-1)
        print(distractor_dist)
        overlap_score = target_dist.mean(dim=1) - distractor_dist.mean(dim=1)
        return overlap_score

    # old metric
    def semantic_drift(self, caption, image):
        """
        P(caption|image) under image caption model pretrained on one image only.
        
        image: (batch_size, 3, 224, 224)
        caption: (batch_size, caption_len)
        
        Returns:
        -------
            prob: (batch_size,)
                Tensor of conditional log probabilities measuring the semantic drift. 
        """
        # load pretrained models
        sent_probs = []
        with torch.no_grad():   
            # embed image
            features = self.semantic_encoder(image)
            # pass image, embedded caption through lstm
            scores = self.semantic_decoder(features, caption) # TODO which format does the caption have to have?
            # retrieve log probs of the target tokens (probs at given indices) 
            scores_prob = self.softmax(scores) 
            # exclude START and END tokens
            sent_probs = []
            for num, cap in enumerate(caption.tolist()):
                sent_probs.append(torch.stack([scores_prob[num][i][j] for i, j in enumerate(cap[1:-1])]))
            # compute log probability of the sentence
            prob = torch.log(torch.stack(sent_probs)).sum(dim=1)
        return prob

    def structural_drift(self, caption):
        """
        P(caption) under some pretrained language model. 
        
        Caption needs to be natural language str.
        """
        # inputs_str = clean_sentence(caption)
        inputs = self.tokenizer(caption, return_tensors="pt")
        print("Inputs: ", inputs)
        with torch.no_grad():
            # pass labels in order to get neg LL estimates of the inputs as the loss
            outputs = self.structural_model(**inputs, labels = inputs["input_ids"])
            neg_ll = outputs[0]
        # compute sentence-level LL
        sent_ll = -neg_ll.sum(-1)
        return sent_ll