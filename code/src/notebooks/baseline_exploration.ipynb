{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659a0e16",
   "metadata": {},
   "source": [
    "## Baseline thesis experiment MVP\n",
    " \n",
    "This notebook provides the proof of concept for the baseline experiment for the thesis. That is, this implements the **multi-task learning** experiment from Lazaridou, Potapenko and Tieleman (2020), using the MS COCO dataset. The goal of the notebook is to ensure the conceptal and technical correctness of the code; the efficiency and minor updates to the flow of the process are still subject to improvement. \n",
    "\n",
    "I would really appreciate feedback on code contained in cells indicated with **<-- please check me -->**; these are also the ones containing code which I have questions about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0029df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets as dset\n",
    "from torchvision import transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from random import shuffle\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9eb3b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9168659e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86342e2d",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "First, some utility functions are implemented. The Vocabulary class instantiates or loads the vocabulary (full vocabulary constructed from the entire dataset for now). The Dataset class loads image-caption pairs. The `get_loader` provides a DataLoader for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "134c2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils: vocab used by the agents (shared)\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self,\n",
    "        vocab_threshold,\n",
    "        vocab_file='./vocab.pkl',\n",
    "        start_word=\"START\",\n",
    "        end_word=\"END\",\n",
    "        unk_word=\"UNK\",         \n",
    "        annotations_file=\"captions_val2014.json\",\n",
    "        pad_word=\"PAD\",\n",
    "        vocab_from_file=False):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary.\n",
    "        Args:\n",
    "        -----\n",
    "          vocab_threshold: Minimum word count threshold.\n",
    "          vocab_file: File containing the vocabulary.\n",
    "          start_word: Special word denoting sentence start.\n",
    "          end_word: Special word denoting sentence end.\n",
    "          unk_word: Special word denoting unknown words.\n",
    "          annotations_file: Path for train annotation file.\n",
    "          pad_word: Pad token.\n",
    "          vocab_from_file: If False, create vocab from scratch & override any existing vocab_file\n",
    "                           If True, load vocab from from existing vocab_file, if it exists\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.pad_word = pad_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        # create / load the vocab\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"\n",
    "        Load the vocabulary from file OR build the vocabulary from scratch.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        \"\"\"\n",
    "        Populate the dictionaries for converting tokens to integers (and vice-versa).\n",
    "        \"\"\"\n",
    "        self.init_vocab()\n",
    "        # add special tokens andd all tokens from all captions\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_word(self.pad_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"\n",
    "        Initialize the dictionaries for converting tokens to integers (and vice-versa).\n",
    "        \"\"\"\n",
    "        self.word2idx = {} \n",
    "        self.idx2word = {} \n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a token to the vocabulary.\n",
    "        Args:\n",
    "        ----\n",
    "            word: str\n",
    "                Token to be added.\n",
    "        \"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"\n",
    "        Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\n",
    "        \"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(ids):\n",
    "            caption = str(coco.anns[id]['caption'])\n",
    "            caption = caption.lower().strip()\n",
    "            caption = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", caption)\n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            tokens = tokenizer(caption) \n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        \"\"\"\n",
    "        Return index of given word.\n",
    "        Args:\n",
    "        ----\n",
    "            word: str\n",
    "        Returns:\n",
    "            int: index of word\n",
    "        \"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns number of unique tokens in vocabulary. \n",
    "        \"\"\"\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7999f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset generator\n",
    "# padding will be made more consistent across pre-training and reference game training\n",
    "\n",
    "class COCOCaptionsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom class for preprocessing datapoints and sampling a random caption per image.\n",
    "    For training, 70.000 images are sampled.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        file: str\n",
    "            Path to annotations json file.\n",
    "        download_dir: str\n",
    "            Path to root directory containing images and annotations.\n",
    "        img_transform: transformations.Compose\n",
    "            Transformations to be applied to the loaded image before it is embedded.\n",
    "        batch_size: int\n",
    "            Batch size.\n",
    "        mode: str\n",
    "            Train, test or validation.\n",
    "        vocab_threshold: int\n",
    "            Minimal token frequency to be considered in the vocab.\n",
    "        vocb_file: str\n",
    "            Path to vocab file.\n",
    "        start_toke, end_token, unk_token, pad_token: str\n",
    "            Special tokens.\n",
    "        vocab_from_file: bool\n",
    "            Load existing vocab from file?\n",
    "        max_sequence_length: int\n",
    "            Max length to which captions will be truncated.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, file, download_dir, img_transform, batch_size, mode, \n",
    "                 vocab_threshold, vocab_file, start_token, end_token, unk_token, pad_token, \n",
    "                vocab_from_file, max_sequence_length=0):\n",
    "        \"\"\"\n",
    "        Initialize a dataset instance loading image-caption pairs from the MS COCO Captions dataset.\n",
    "        \"\"\"\n",
    "        self.transform = img_transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        # instantiating the vocab object \n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, \n",
    "                                start_token, end_token, unk_token, file, pad_token, vocab_from_file) \n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # some distinctions below for Train and test mode (root dir and whether there are captions)\n",
    "        if mode == \"train\":\n",
    "            self.image_dir = os.path.join(download_dir, \"train2014\")\n",
    "            self.coco = COCO(file) \n",
    "            _ids = list(self.coco.anns.keys())\n",
    "            shuffle(_ids)\n",
    "            # take 70.000 images from the dataset\n",
    "            self.ids = _ids[:70000]\n",
    "            print('Obtaining caption lengths...')\n",
    "            tokenizer = get_tokenizer(\"basic_english\") \n",
    "            all_tokens = [tokenizer(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in np.arange(len(self.ids))]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "            # get maximum caption length for padding\n",
    "            self.max_caption_length = max(self.caption_lengths)\n",
    "            \n",
    "            # print pretraining IDs for later separation from functional training\n",
    "            with open(\"pretrain_img_IDs.txt\", 'w') as f:\n",
    "                f.write(\",\".join([str(i) for i in self.ids]))\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            self.image_dir = os.path.join(download_dir, \"val2014\")\n",
    "            self.coco = COCO(file) \n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print('Obtaining caption lengths...')\n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            all_tokens = [tokenizer(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in np.arange(len(self.ids))] # tqdm(np.arange(len(self.ids)))\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "            \n",
    "        else:\n",
    "            self.image_dir = os.path.join(download_dir, \"val2014\")\n",
    "            # no annotations here \n",
    "            test_info = json.loads(open(file).read())\n",
    "            self.paths = [item['file_name'] for item in test_info['images']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return number of available data points.\n",
    "        \"\"\"\n",
    "        if self.mode != \"test\":\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return an image-caption tuple. A random caption per images is chosen since the dataset maps captions onto images.\n",
    "        \n",
    "        Arguments:\n",
    "        -------\n",
    "        idx: int\n",
    "            Index of the item to be returned.\n",
    "        Returns:\n",
    "        -----\n",
    "        image: torch.tensor((3,224,224))\n",
    "        caption: torch.tensor((len_caption))\n",
    "        \"\"\"\n",
    "        \n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode != 'test':\n",
    "            ann_id = self.ids[idx]\n",
    "            caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.image_dir, path)).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            tokens = tokenizer(str(caption).lower())\n",
    "            # Convert caption to tensor of word ids, append start and end tokens.\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            \n",
    "            # check if the sequence needs to be truncated\n",
    "            if self.max_sequence_length != 0:\n",
    "                tokens = tokens[:self.max_sequence_length]\n",
    "                \n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[idx]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.image_dir, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "        \n",
    "    def get_train_indices(self):\n",
    "        \"\"\"\n",
    "        Return a list of indices at which the captions have the same length which was sampled at random \n",
    "        for the given batch. To be used for pretraining the speaker (base image-captioning model).\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "            indices: list\n",
    "                List of indices of caption-image pairs to be used in the batch.\n",
    "        \"\"\"\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def get_func_train_indices(self):\n",
    "        \"\"\"\n",
    "        Simple POC function returning two lists on indices for the functional training. \n",
    "        Returns a list of inidces for targets and a list f indices for distractors. \n",
    "        Captions are of same lengths for targets and distractors (will be optimized).\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "            list: (int, int)\n",
    "                List of tuples of target and distractor indices, each for a single reference game iteration.\n",
    "        \"\"\"\n",
    "        \n",
    "        sel_length_t = np.random.choice(self.caption_lengths)\n",
    "\n",
    "        all_indices_t = np.where([self.caption_lengths[i] == sel_length_t for i in np.arange(len(self.caption_lengths))])[0]\n",
    "\n",
    "        indices = list(np.random.choice(all_indices_t, size=(self.batch_size)*2))\n",
    "        indices_t = indices[:self.batch_size]\n",
    "        indices_d = indices[self.batch_size:]\n",
    "        \n",
    "        return list(zip(indices_t, indices_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb205006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility wrapper returning a DataLoader for training \n",
    "def get_loader(transform,\n",
    "               mode='val',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"START\",\n",
    "               end_word=\"END\",\n",
    "               unk_word=\"UNK\",\n",
    "               pad_word=\"PAD\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               download_dir=\"../../../data/val/\",\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Returns the data loader.\n",
    "    Args:\n",
    "    ----\n",
    "      transform: transforms.Compose\n",
    "          Image transform.\n",
    "      mode: str \n",
    "          One of 'train', 'val' or 'test'.\n",
    "      batch_size: int\n",
    "          Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: int\n",
    "          Minimum word count threshold.\n",
    "      vocab_file: str\n",
    "          File containing the vocabulary. \n",
    "      start_word, end_word, unk_word, pad_word: str\n",
    "          Special tokens.\n",
    "      vocab_from_file: bool\n",
    "          If False, create vocab from scratch & override any existing vocab_file.\n",
    "          If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: int\n",
    "          Number of subprocesses to use for data loading \n",
    "      \n",
    "    Returns:\n",
    "    ------\n",
    "        data_loader: torch.DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test', 'val'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train' or mode=='val', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'val':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json')\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"train2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        # TBD\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") #'test2014/'\n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json') \n",
    "\n",
    "    # build COCO caption dataset.\n",
    "    dataset = COCOCaptionsDataset(\n",
    "        file=annotations_file,\n",
    "        download_dir = download_dir, \n",
    "        img_transform=transform,\n",
    "        batch_size=batch_size,\n",
    "        mode=mode,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_token=start_word,\n",
    "        end_token=end_word,\n",
    "        unk_token=unk_word,\n",
    "        pad_token=pad_word, \n",
    "        vocab_from_file=vocab_from_file,\n",
    "        max_sequence_length=15,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=torch.utils.data.sampler.BatchSampler(\n",
    "                                          sampler=initial_sampler,\n",
    "                                          batch_size=dataset.batch_size,\n",
    "                                          drop_last=False),\n",
    "                                                 )\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=dataset.batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc46db",
   "metadata": {},
   "source": [
    "### Agents\n",
    "Next, the agents (speaker and listener) are implemented. Each agent consists of a CNN encoder embedding the images, and an RNN language module. The RNN is decoder in case of the speaker, and an encoder in case of thelistener. All four models are coded separately for purposes of explicitness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b20e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker visual module\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Initialize pretrained Resnet 50 for the speaker. \n",
    "        Put a linear layer on top, mapping ResNet features to desired visual embedding dimensionality.\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # remove the last fully connected layer, adding a Linear one\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.embed.weight.data.normal_(0., 0.02)\n",
    "        self.embed.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Performs a forward step for embedding a batch of images received by the speaker.\n",
    "        Args:\n",
    "        -----\n",
    "            images: torch.tensor((batch_size, 3, 224, 224))\n",
    "            \n",
    "        Returns:\n",
    "        ------\n",
    "            features: torch.tensor((batch_size, visual_embed_size))\n",
    "        \"\"\"\n",
    "        features = self.resnet(images)\n",
    "        # reshape features to shape (batch_size, -1)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285132e",
   "metadata": {},
   "source": [
    "**<-- Please check me below -->**\n",
    "\n",
    "Especially the sampling procedure and the retireval of the action probabilities therein is an important block which is employed in the reference game training and which is part of REINFORCE - quite unsure about the conceptual correctness of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045c7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker language module\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the langauge module consisting of a one-layer LSTM and \n",
    "        trainable embeddings. The image embedding is used as additional context at every step of the training \n",
    "        (prepended at the sentence beginning).\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            embed_size: int\n",
    "                Dimensionality of trainable embeddings.\n",
    "            hidden_size: int\n",
    "                Hidden/ cell state dimensionality of the LSTM.\n",
    "            vocab_size: int\n",
    "                Length of vocabulary.\n",
    "            num_layers: int\n",
    "                Number of LST layers.\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size= embed_size\n",
    "        self.vocabulary_size = vocab_size\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size , self.num_layers, batch_first=True)\n",
    "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size) \n",
    "        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "            \n",
    "        \"\"\" \n",
    "        At the start of training, we need to initialize a hidden state;\n",
    "        Defines a hidden state with all zeroes\n",
    "        The axes are (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n",
    "                torch.zeros((1, batch_size, self.hidden_size), device=device))\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Perform forward step through the LSTM.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            features: torch.tensor((batch_size, embedd_size))\n",
    "                Embeddings of images.\n",
    "            captions: torch.tensor((batch_size, caption_length))\n",
    "                Lists of indices representing tokens of each caption.\n",
    "        Returns:\n",
    "        ------\n",
    "            outputs: torch.tensor((batch_size, caption_length, embedding_dim))\n",
    "                Scores over vocabulary for each token in each caption.\n",
    "        \"\"\"\n",
    "        \n",
    "        embeddings = self.embed(captions)\n",
    "        features = features.unsqueeze(1)\n",
    "        # PREpend the feature embedding as additional context, cut off END token        \n",
    "        embeddings = torch.cat((features, embeddings[:, :-1,:]), dim=1)\n",
    "        hiddens, self.hidden = self.lstm(embeddings)\n",
    "        \n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, inputs, max_sequence_length):\n",
    "        \"\"\"\n",
    "        Function for sampling a caption during functional (reference game) training.\n",
    "        Implements greedy sampling. Sampling stops when END token is sampled or when max_sequence_length is reached.\n",
    "        Also returns the log probabilities of the action (the sampled caption) for REINFORCE.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "            inputs: torch.tensor(1, 1, embed_size)\n",
    "                pre-processed image tensor.\n",
    "            max_sequence_length: int\n",
    "                Max length of sequence which the nodel should generate. \n",
    "        Returns:\n",
    "        ------\n",
    "            output: list\n",
    "                predicted sentence (list of tensor ids). \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        output = []\n",
    "        raw_outputs = [] # for structural loss computation\n",
    "        scores = []\n",
    "        batch_size = inputs.shape[0] # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)\n",
    "        hidden = self.init_hidden(batch_size) # Get initial hidden state of the LSTM\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # below will be optimized\n",
    "        while True:\n",
    "            lstm_out, hidden = self.lstm(inputs, hidden) # lstm_out shape : (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
    "            raw_outputs.extend(outputs)\n",
    "            # get the log probs of the actions\n",
    "            probs = softmax(outputs)\n",
    "            max_probs, max_inds = torch.max(probs, dim=-1)\n",
    "            scores.append(max_probs)\n",
    "            \n",
    "            outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n",
    "            _, max_indice = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)\n",
    "            output.append(max_indice)\n",
    "                          \n",
    "            if (max_indice == 1) or (len(output) == max_sequence_length):\n",
    "                # We predicted the <end> word or reached max length, so there is no further prediction to do\n",
    "                break\n",
    "            \n",
    "            ## Prepare to embed the last predicted word to be the new input of the lstm\n",
    "            inputs = self.embed(max_indice) # inputs shape : (1, embed_size)\n",
    "            inputs = inputs.unsqueeze(1) # inputs shape : (1, 1, embed_size)\n",
    "            \n",
    "        # turn raw scores into log probabilities\n",
    "        log_probs = torch.log(torch.stack(scores))\n",
    "        \n",
    "        if len(output) < max_sequence_length:\n",
    "            # get the embedding and softmax output for pad\n",
    "            pad_input = self.embed(torch.tensor([3])).unsqueeze(1)\n",
    "            lstm_pad, _ = self.lstm(pad_input, hidden)\n",
    "            pad_output = self.linear(lstm_pad)\n",
    "            \n",
    "            while len(output) < max_sequence_length:\n",
    "                output.append(torch.tensor([3])) # pad\n",
    "                raw_outputs.extend(pad_output)\n",
    "        \n",
    "        return output, log_probs, raw_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a16461",
   "metadata": {},
   "source": [
    "**<-- Please check me below -->**\n",
    "\n",
    "The forward steps includes the computation of the dot product based on which the listener makes its guess - not sure if the output scores are correct for cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fda189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listener visual module\n",
    "# has a different forward step than the speaker visual module\n",
    "\n",
    "class ListenerEncoderCNN(EncoderCNN):\n",
    "       \n",
    "    def forward(self, images1, images2, caption):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the listener ResNet 50 CNN.\n",
    "        Computes the dot product between two images and the caption provided by the speaker.\n",
    "        Outputs the index of the image which has the highest dot product with the caption - it is the predicted target.\n",
    "        \n",
    "        Args:\n",
    "        ----\n",
    "        images1: torch.tensor((batch_size, 3, 224, 224))\n",
    "            List of images (potentially containing either targets or distractors).\n",
    "        images2: torch.tensor((batch_size, 3, 224, 224))\n",
    "            List of images (potentially containing either targets or distractors).\n",
    "        caption: torch.tensor((batch_size, sentence_length, embed_size))\n",
    "        \n",
    "        Returns:\n",
    "        ----\n",
    "            indices: torch.tensor(batch_size)\n",
    "                List of predicted target indices. \n",
    "            \n",
    "        \"\"\"\n",
    "        # will be improved\n",
    "        features1 = self.resnet(images1) #[0]\n",
    "        features2 = self.resnet(images2) # [1]\n",
    "        # reshape features to shape (batch_size, -1) - adapt to first dim\n",
    "        features1 = features1.view(features1.size(0), -1)\n",
    "        features1 = self.embed(features1)\n",
    "        features2 = features2.view(features2.size(0), -1)\n",
    "        features2 = self.embed(features2)\n",
    "        # compute dot product between images and caption\n",
    "        # compute mean over words as sentence embedding representation\n",
    "        caption = caption.mean(1)\n",
    "        dot_products_1 = torch.bmm(features1.view(images1.size()[0], 1, features1.size()[1]),\n",
    "                                   caption.view(images1.size()[0], features1.size()[1], 1))\n",
    "        dot_products_2 = torch.bmm(features2.view(images2.size()[0], 1, features2.size()[1]),\n",
    "                                   caption.view(images2.size()[0], features2.size()[1], 1))\n",
    "        # compose targets and distractors dot products\n",
    "        # stack into pairs, assuming dim=0 is the batch dimension\n",
    "        pairs = torch.stack((dot_products_1, dot_products_2), dim=1) \n",
    "        pairs_flat = pairs.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        return pairs_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cdbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listener language module\n",
    "class ListenerEncoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the langauge module consisting of a one-layer LSTM and \n",
    "        trainable embeddings. The image embedding is used as additional context at every step of the training \n",
    "        (prepended at the embedding beginning). \n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            embed_size: int\n",
    "                Dimensionality of trainable embeddings.\n",
    "            hidden_size: int\n",
    "                Hidden/ cell state dimensionality of the LSTM.\n",
    "            vocab_size: int\n",
    "                Length of vocabulary.\n",
    "            num_layers: int\n",
    "                Number of LST layers.\n",
    "        \"\"\"\n",
    "        super(ListenerEncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size= embed_size\n",
    "        self.vocabulary_size = vocab_size\n",
    "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size) \n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size , self.num_layers, batch_first=True)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initialize a hidden state with all zeroes.\n",
    "        The axes are (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n",
    "                torch.zeros((1, batch_size, self.hidden_size), device=device))\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        \"\"\"\n",
    "        Compute forward step through the listener LSTM.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            captions: torch.tensor((1, caption_length))\n",
    "                Caption received from speaker\n",
    "        Returns:\n",
    "        -----\n",
    "            hiddens: torch.tensor((1, caption_length, embed_size))\n",
    "                Hidden sentence representations.\n",
    "        \"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        hiddens, self.hidden = self.lstm(embeddings)\n",
    "        \n",
    "        return hiddens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfcc81",
   "metadata": {},
   "source": [
    "**<-- Please check me below -->**\n",
    "\n",
    "Not sure if it is okay to just treat the LSTM time steps as iterations, and to use no discount factor. Also not sure about the computation of the batch update as sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e606e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def update_policy(rewards, log_probs):\n",
    "    \"\"\"\n",
    "    This function calculates the weight updates accoring to the REINFORCE rule.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        rewards: list\n",
    "            List of rewards of length batch_size\n",
    "        log_probs: torch.tensor((batch_size, caption_length))\n",
    "            Log probabilities of each word in each predicted sentence.\n",
    "    Returns:\n",
    "    -----\n",
    "        policy_gradient: torch.tensor\n",
    "            Update to be applied together with the other loss components to the speaker parameters. \n",
    "    \"\"\"\n",
    "\n",
    "    policy_gradient = []\n",
    "    sentence_prob = log_probs.mean(dim=1)\n",
    "    for log_prob, Gt in zip(sentence_prob, rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    # here, we just sum to get the batch loss - consider average\n",
    "    policy_gradient = torch.stack(policy_gradient).mean(dim=1)\n",
    "    \n",
    "    return policy_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706ecb6",
   "metadata": {},
   "source": [
    "### Pre-training set up\n",
    "\n",
    "First, the speaker pretraining will be completed. The loop below exempliefies how that will be done; the full scale model has not been pretrained yet. The training details like the optimizer are kept maximally simple. For exploration purposes, the entire vocabulary is used.\n",
    "\n",
    "All components are loaded, a training loop is started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d523e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n",
      "Vocab size:  6039\n"
     ]
    }
   ],
   "source": [
    "# pre-training set up \n",
    "\n",
    "batch_size = 128         # batch size\n",
    "vocab_threshold = 11        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 1024           # dimensionality of word embeddings\n",
    "visual_embed_size = 1024   # dimensionality of the image embeding\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 200          # determines window for printing average loss\n",
    "log_file = 'training_log_train.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                         download_dir=\"../../../data/train\", \n",
    "                        )\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"Vocab size: \", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "995b2e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps:  547\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(visual_embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = list(decoder.lstm.parameters()) + list(decoder.linear.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
    "print(\"Total steps: \", total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d40cb45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   37,  470, 1149,  178,  932,   56,    4,   42,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49, 1509,  210,  446,   34, 2630,  407,    4,   20,  270,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   37, 2065,  228,   41,    4, 1293,  121, 1681,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  2587,    22,  4888,  2503,   404, 11953,   197,  1880,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  117,  594,  372,  165,    4,  329, 1362, 3542,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1350, 2481,  132,  133,   79,    4,  213,  214,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,  132,   55,  435,    4,  182,  269, 2054,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75,  273,  113, 8192, 2651,  426,   47,    4,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  465,  363, 4281, 5000,   41,    4,  343,  670,  238,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343,  125,  888, 1022,   56,    4,  312, 2044,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 926, 125, 629, 888, 113,   4, 888, 489,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  465,  174,  196,   79, 2525, 3357,  368,    4, 4769,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 141, 122,  41,   4, 725, 233,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  273,  210,   79,    4, 2223,  315,   81,   74,  419,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258,  94, 736,  41, 113,   4, 949, 592,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1672,    22, 13073,   132,   174,    79,    34,   138,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 617, 362,  79, 104, 386,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  629,  888,  925,  103,    4, 1503,  888,  889,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 4738,  939,  133,   79,    4,   61,  214,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1369, 1486,   79,   34,   80,   60,    4,  269, 1119,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 9170, 1698,   41,   34, 2966,  113,   34, 2858,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 379,  83,   4,  20, 364, 427,   4, 208,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  111,  132,  439, 1979,   79,   34, 4129, 1304,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4652, 7252,   41,  138,   41,    4, 3374,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  888,  925,  537,  188,    4,  322,   14,  888, 2719,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  69,   7,  59,  79,  34, 564,  14,   4, 155,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  379, 1486,   41,    4,   57,  372,  165,   34,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,   16,   62,  939,  801,  256, 3729,   14,  111,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  718,   79,   34,  283,  372,  165,  256, 1867,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  363, 1566,   41,   34, 4281,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 219,  15,  22, 269, 867,   7,  87,   7,  18,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 5794,    7,    4,  173, 2106,  539,  165, 2117,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 260,  41,  34, 406,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  117, 5542,   41,    4,  542,   81,   34, 1431,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,   22,   49,  105,    7,    4,  386, 2297, 1664,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  41,   4, 282, 518,   4, 859, 849,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,  189,  103,  189,  174,  103,  256,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 617, 362,  79, 104, 386,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2663,   32,  228,   41,   34,   42,   81,  284,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   35,   66,  132,  235,   22,  256, 2223, 9654,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   35,   66,  132,  235,   22,  256, 2223, 9654,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 255, 132,   4, 173, 228,  56, 879,  34, 509,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   100,    79, 12194,   718,    41,     4,   669,   719,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,  995,  763,  290,  113,    4,  791,  949, 2740,  592,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  147,  132,    4,   29,   22,  269, 1040,    7,   64,   65,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    79,     4,    37,   883, 23225,   268,   320,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  379,   79,  335,  744,  208, 3025,  210,   37, 4258,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1369, 1486,   79,   34,   80,   60,    4,  269, 1119,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,   61, 3322, 2503,   41,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  888,  925,  537,  188,    4,  322,   14,  888, 2719,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75,   53,  363,  364, 2246,   41,    4,  152,  785,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14,  53, 228, 196,   4,  29, 520,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1530,   14, 5209,    7, 9256,   22,  625,    7,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14,  53, 228, 196,   4,  21, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   35,   66,  132,  235,   22,  256, 2223, 9654,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  575,  117,  228,   41,  140,   22,    4, 1862,  849,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   66,  132, 2205,  446,   81,    4,  185,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  483, 1566, 1652,   79, 5135,   41,   34,  785,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2260,  178,  132,  133,   79,   34,  213,  730,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  173,  757,  165, 1696,  663,  361,   22, 1566, 1653,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 2889,  851,  132, 5543,   22, 3120,    7, 1540,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  132,  957,   22, 3064,   79,  268,  386,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 1415,    4, 4769,  113,    4,  361,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   22,    4, 1566, 1653, 1486,   33,    4,  785,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,  2525,  2818,    55,   372,   165,     4, 23436,  4655,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  888,  925,  537,  188,    4,  322,   14,  888, 2719,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2517,  210,  427,    4,  715,   20,  270,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 9170, 1698,   41,   34, 2966,  113,   34, 2858,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  111,  132,  439, 1979,   79,   34, 4129, 1304,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2835,   14,   62, 8180, 4837,  188,    4, 2952,  251,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75,   53,  363,  364, 2246,   41,    4,  152,  785,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   666,   132,  2694, 12607,   164,   391,   603,   666,  3851,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    79,     4,    37,   883, 23225,   268,   320,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   100,    79, 12194,   718,    41,     4,   669,   719,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   96, 3681,   14,  256,  813,   14,  337,   79,   74,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  817,  368,    4, 3427,   79,    4, 1211, 1031,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  109,   79, 7764,   95,  188,   34,   57,   83,  163,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,  132,  141,    4, 2500,   56,    4,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2260,   29,   22,   76,  706,    7,  631,   79, 2172,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  100,   55, 1060,    4, 1279,  178,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  117,  594,  372,  165,    4,  329, 1362, 3542,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  3069,    22,    49,   295,    14,     4, 10725,    41,\n",
      "           48,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,    5,  791, 3203, 1369,   22,  104, 3836,  520,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2604,  999,   14, 1481,  132,  569,   73, 3455,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 872, 132, 174,  79, 123,  14, 104, 776,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 4738,  939,  133,   79,    4,   61,  214,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 340, 622, 228,  41, 262,  14,   4, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 1415,    4, 4769,  113,    4,  361,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   62,    7,   21,  117,  132, 1001,   41,   34,  735,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   11,   22,    4,   59,   88,   58,    7, 2398,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  588,   22,   52, 3829, 1410, 3868,   79,   34,  261,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  379, 1486,   41,    4,   57,  372,  165,   34,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   460, 10590,   893,  2086,  2938,   165,     4,    11,   155,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  850,   56, 2719,  349,    4,  182,  535,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 5506,  106, 3125, 2260,  296, 1736,   79, 2172,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  259,  132, 1779,    4, 1383,   41,    4,  776,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   61,  378, 1672,   55,   79, 1679,   52, 2407, 4600,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4652, 7252,   41,  138,   41,    4, 3374,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  379, 1486,   41,    4,   57,  372,  165,   34,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 872, 132, 174,  79, 123,  14, 104, 776,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14,  53, 228, 196,   4,  29, 520,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  21, 495, 473, 228, 372, 165,   4,  59,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  213,  972,  107,    4,   61,   32, 2124,  165,   48,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  465,  363, 4281, 5000,   41,    4,  343,  670,  238,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   11,   22,    4,   59,   88,   58,    7, 2398,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258,  94, 736,  41, 113,   4, 949, 592,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1698, 2099,  113,   34, 3133,   41,   34, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  465,  174,  196,   79, 2525, 3357,  368,    4, 4769,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 2889,  851,  132, 5543,   22, 3120,    7, 1540,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 174,  79, 123,  14,  49, 697, 185,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   11,   22,    4,   59,   88,   58,    7, 2398,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 888, 133, 113,   4, 888, 889, 879, 256, 451,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    79,     4,    37,   883, 23225,   268,   320,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  588,   22,   52, 3829, 1410, 3868,   79,   34,  261,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,   352,    54,  1022,    56,     4, 11869,   526,   453,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  213,  972,  107,    4,   61,   32, 2124,  165,   48,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  208,   14, 1369,  493,  500,   14, 1451, 2572,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   79,    4, 1059,  594,   79,    4,   15,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 1953,  550,   22,    4,  621, 8583, 2079,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,  898,   54,   55,  253,   14,   34, 1166,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 617, 362,  79, 104, 386,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  173,  757,  165, 1696,  663,  361,   22, 1566, 1653,   19,    1])\n",
      "Epoch [1/1], Step [1/8], Loss: 10.1247, Perplexity: 24952.7345INDEXED CAPTION PRETRAIN DS:  tensor([    0,   460, 10590,   893,  2086,  2938,   165,     4,    11,   155,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256, 2671,   95,   41,   34, 2123,  427,  256,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,    62,     7,    21,   355, 18096,    41,     4,   332,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   22,    4, 1566, 1653, 1486,   33,    4,  785,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14, 612,  53,  79,   4, 891,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,   79,   49, 1206,  889,   22,   53,  797,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1616, 2659,    7,  268, 7922,  174,   79,    4, 1498,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,    7,  258,  259,  345, 1004,  510,    7, 5014,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4652,   22,  343, 4512, 1131,  471,   34, 1232,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   11,   22,    4,   59,   88,   58,    7, 2398,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 141, 122,  41,   4, 725, 233,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   22,    4, 3707,   55,  299,  256,  468,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   53,    7,  498,   41,   34, 1449,  372,  165, 1356,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   851,    22,   612, 19653,     7,  2710,    41,    48,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75,   53,   54,  739,  616,  165,  554,  256, 1566,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3539,   14, 4652,  174,  196,   79,   49, 1782,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   173,   228,    81,     4,  1953,    14, 14339,   787,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 256, 265,   7, 266,  54, 732,  36,   4, 156,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3427,    7,  256, 5682,   41,    4,   21,  851,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258,  94, 736,  79,  34, 155, 211, 884, 885,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   21,  117,  228,  407,    4, 3982,  499,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 386, 707, 529, 149,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,    7,  258,  259,  345, 1004,  510,    7, 5014,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4, 10533,    41,    17,  6184,   595,    81,     4,    61,\n",
      "          796,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 169, 170,  66, 132, 228,  79,  34, 138,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  363, 1566,   41,   34, 4281,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 663, 299,  14,   4, 100, 368,   4, 391, 220,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,   22,   49,  105,    7,    4,  386, 2297, 1664,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 2889,  851,  132, 5543,   22, 3120,    7, 1540,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 872, 132, 174,  79, 123,  14, 104, 776,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  850,   56, 2719,  349,    4,  182,  535,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343,   20,  178, 1291,  253,  292,    4,   35,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4652,   22,  343, 4512, 1131,  471,   34, 1232,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3539,   14, 4652,  174,  196,   79,   49, 1782,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49, 1509,  210,  446,   34, 2630,  407,    4,   20,  270,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   62,    7,   21,  117,  334,   41,   34, 1334,  542,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,   79,    4,  745, 1782,   22,  182,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  41,   4, 282, 518,   4, 859, 849,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,   61, 3322, 2503,   41,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    79,     4,    37,   883, 23225,   268,   320,\n",
      "           19,     1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  562, 3135,  132, 7570,   41,  999,  171, 6224,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  41,   4, 282, 518,   4, 859, 849,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  851,   22,  256,  813,   14, 2384,   41,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,   55,   41,   34,  189,   14,   34,   35,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  109,   79, 7764,   95,  188,   34,   57,   83,  163,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 169, 170,  66, 132, 228,  79,  34, 138,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   371,  1610,    41,    52,   479, 23249,   113,  3935,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  2587,    22,  4888,  2503,   404, 11953,   197,  1880,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  182, 2659,  174,  372,  165,    4, 4037, 2952,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 386, 707, 529, 149,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  15, 310,  22,  49, 185,   7,   4, 218,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  379,   79,  335,  744,  208, 3025,  210,   37, 4258,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75,   53,   54,  739,  616,  165,  554,  256, 1566,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  588,   22,   52, 3829, 1410, 3868,   79,   34,  261,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 794, 134,  22, 215, 112, 100, 228,  41,  48,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 256, 265,   7, 266,  54, 732,  36,   4, 156,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  100,   55, 1060,    4, 1279,  178,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   21,  117,  228,  407,    4, 3982,  499,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,  465, 1023,    7,  225, 1666,  113,    4,  115,  920,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  123,   14,    4,   20,  575,    7,   21, 1509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  295,   14,    4,   82,   14, 1880,  210,   79,  683,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   35,   66,  132,  235,   22,  256, 2223, 9654,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  197, 2793,   54, 2106,  812, 1355,   79,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 340, 622, 228,  41, 262,  14,   4, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,  465, 1023,    7,  225, 1666,  113,    4,  115,  920,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  872,  959,    4, 1906,   41, 2649,  113,  135,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   22,    4, 3707,   55,  299,  256,  468,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  599,  592,  132,   41,    4, 1698,   22, 9117,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256, 2671,   95,   41,   34, 2123,  427,  256,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  28,  29,  22,  34, 114, 322,  73,   4, 804,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 1953,  550,   22,    4,  621, 8583, 2079,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   66,  132, 2205,  446,   81,    4,  185,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,   79,    4,  745, 1782,   22,  182,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  117, 5542,   41,    4,  542,   81,   34, 1431,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4652, 7252,   41,  138,   41,    4, 3374,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   851,    22,   612, 19653,     7,  2710,    41,    48,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 4600,   54,  363,   79,   34, 1763,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  133, 1770,  256,   53,  286,  441,   41,   48,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  182, 2659,  174,  372,  165,    4, 4037, 2952,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2604,  999,   14, 1481,  132,  569,   73, 3455,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 386, 707, 529, 149,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   22,    4, 3707,   55,  299,  256,  468,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  718,   79,   34,  283,  372,  165,  256, 1867,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 926, 125, 629, 888, 113,   4, 888, 489,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 141, 325,  56,   4, 526, 263, 719,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  109,   79, 7764,   95,  188,   34,   57,   83,  163,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 1415,    4, 4769,  113,    4,  361,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,   6,  15,  22,   4,  58,   7, 278, 732,  56,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2016,   22, 3003,   78,   41,    4, 4355,  263, 1530,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    75, 22309,    41,  5834,   283,   407,     4,   715,   270,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,   61, 3322, 2503,   41,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  290,  299,  113,    4, 1697, 1509,   79,    4,   20,  270,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258,  94, 736,  79,  34, 155, 211, 884, 885,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  943,   22,    4,   31,   32,    7,  256,   35, 1086,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  3427,    22,     4, 19730,     7,  1684,    41,     4,\n",
      "          851,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   21,  117,  228,  407,    4, 3982,  499,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2517,  210,  427,    4,  715,   20,  270,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 1415,    4, 4769, 1060,    4,  361,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258,  94, 736,  79,  34, 155, 211, 884, 885,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   37,  159,   22,    4, 1517,  118,  471,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  62,   7,  21, 117, 407, 105,  79,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   851,    22,   612, 19653,     7,  2710,    41,    48,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258, 872, 132, 777, 964,  41,   4, 862,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,  336,   14,  111,   41,    4,  114,   22, 1919,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93, 7834,    4,  361,   22,    4, 1566, 1653,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,  189,  103,  189,  174,  103,  256,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 872, 132, 174,  79, 123,  14, 104, 776,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1672,    22, 13073,   132,   174,    79,    34,   138,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    22,     4, 10417,  4652,    41,     4,  3919,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1459,    4, 1566,  361,   22,    4, 4074,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   60,   13,   14,    4,  178,    7,   17, 2002,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2016,   22, 3003,   78,   41,    4, 4355,  263, 1530,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  34,  57, 363,  22,   4, 361,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  575,  117,  228,   41,  140,   22,    4, 1862,  849,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,  2525,  2818,    55,   372,   165,     4, 23436,  4655,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343,  233,  178,  850, 1200,    4,  233,   35,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1480, 5258, 2209,  132,  439,  572,   22, 5160,   19,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,   41,    4,  776, 1022,  299,   49, 6760,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,   55,   41,   34,  189,   14,   34,   35,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,   22,   49,  105,    7,    4,  386, 2297, 1664,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  465,  363, 4281, 5000,   41,    4,  343,  670,  238,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,   41,    4,  776, 1022,  299,   49, 6760,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    75, 22309,    41,  5834,   283,   407,     4,   715,   270,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  629,  888,  925,  103,    4, 1503,  888,  889,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  943,   22,    4,   31,   32,    7,  256,   35, 1086,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2835,   14,   62, 8180, 4837,  188,    4, 2952,  251,    1])\n",
      "Epoch [1/1], Step [2/8], Loss: 9.7191, Perplexity: 16631.8565INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  937,   92,  368,  164,  362,  228,   41,    4,\n",
      "        1144,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1611,   14,  133, 2814,  228,   41,  262,   14,  888, 2719,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  518,    4, 4156,  299,  806,  132, 1013,  164, 1389,\n",
      "          41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  671,  107, 2610, 3002,   79,   48,   22, 6099,   41,  262,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   342,    14,     4,   380,    66,  3861,    34,  1268,\n",
      "         6055, 11239,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1433,   56,  372,  165,    4, 2297,  368,   52,  364,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   186,    79,     4,   891,    29, 13483,  4309,    41,\n",
      "            4,   735,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   61,  379,  132,  174,   41,  262,   14,    4,  340, 1431,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  55, 372, 165,   4, 888, 976, 113,   4, 889,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258, 371, 368,  49, 574, 364,  79, 268, 488, 386,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   38,  417,  418,  932,   41,    4,  233,   35,  235,   22, 2572,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   100, 12458,   722,   292,     4,  4101,    22,     4,\n",
      "         1517,  1664,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  3203,   995,  1672,    22,    17, 13073,   103,     4,\n",
      "         1463,   574,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,    7,   52,  109,  110, 1481,   79,    4, 3838,   15,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  518,    4, 4156,  299,  806,  132, 1013,  164, 1389,\n",
      "          41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    29,    66,   107,     4,  1862,   796,     7, 10389,\n",
      "           79,    48,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   700,    14,  3154,   107,  2710,    22, 18764,  1570,\n",
      "           41,   262,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14, 1549,  683, 3337,  210,  663,  520,   79,   34,\n",
      "         270,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1498,  290,   15,   22,    4,   28,  114,   16,  457, 1989,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  633,   57,   22,  729,   14, 1168,   41,  262,   14,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343,   82,   14,  465,   54,   79,    4, 1871,   41, 1369,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   700,    14,  3154,   107,  2710,    22, 18764,  1570,\n",
      "           41,   262,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 112, 379,  41,   4, 749,   7, 537, 379,  41,  34,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2671,   55,   41, 2123,  195,  372,  165, 1119,   79, 1763, 1374,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2671,   55,   41, 2123,  195,  372,  165, 1119,   79, 1763, 1374,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14, 465,   7, 109, 228, 196,  34, 993, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   100, 12458,   722,   292,     4,  4101,    22,     4,\n",
      "         1517,  1664,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   79,   62, 4465,   41,    4,   20,    7,   21,  406,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   700,    14,  3154,   107,  2710,    22, 18764,  1570,\n",
      "           41,   262,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 2260, 1591,  133,   81,    4,  448,   14,  283,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    29,    66,   107,     4,  1862,   796,     7, 10389,\n",
      "           79,    48,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   100, 12458,   722,   292,     4,  4101,    22,     4,\n",
      "         1517,  1664,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  893,   88, 1398,   88, 1400,   88,    7,  419,  759,   54,  569,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  893,   88, 1398,   88, 1400,   88,    7,  419,  759,   54,  569,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   186,    79,     4,   891,    29, 13483,  4309,    41,\n",
      "            4,   735,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 9702, 1542,  334,   79,   34,  539, 3901,   14, 1315,  278,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,    7,   52,  109,  110, 1481,   79,    4, 3838,   15,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  363,    4,  149,   22, 9875, 4281, 3272,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  574,  888,  850,  427, 3765, 4712,  138,  550, 4096,   19,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  566,  291,    4, 1144,   41,    4,  776,  113,    4,  794,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 112, 379,  41,   4, 749,   7, 537, 379,  41,  34,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  363,    4,  149,   22, 9875, 4281, 3272,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   937, 20095,     7,   164,   362,   927,   113,    34,\n",
      "          588,   261,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,  142,  133, 2246,    7,   53,  174,  372,  165,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53,   54,   55,   56,   34, 2642,   41,    4, 1454,  609,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2560,  228,  500,   14,    4,  121,   22, 1429, 2021,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 2119,  165, 1696,    4, 9703,  752,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  132,  627,    4,  251,   22,   49,  648,  269, 1593,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,  142,  133, 2246,    7,   53,  174,  372,  165,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  66, 132, 228,  41,  34, 539,  14,   4, 931,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 132, 260,  41,   4, 464, 211,  48, 132, 715,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 5290,  702, 4652,    7,  379,   41,  827, 1264,   14,    4, 1119,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  132,  627,    4,  251,   22,   49,  648,  269, 1593,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2525, 4607, 1357,   41,  180, 5336, 1536,  165, 1696,   34, 2525,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  213,  972,   79,  123,   14,    4,  343, 1871,   14,   53,   55,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2671,   55,   41, 2123,  195,  372,  165, 1119,   79, 1763, 1374,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  633,   57,   22,  729,   14, 1168,   41,  262,   14,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,   518,     4,   849,     7, 17461,   883,   368,\n",
      "            4,  2053,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  783,   56,  180,  666, 1426,  165, 1696,   34, 2525,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   61, 1698,   66,  132,    5,  663,  165,   34,\n",
      "         200,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  147,  132,   49,  295,   14,    4, 1970,   22,  729,   14,  902,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4351, 3348,  228,  372,  165,   74,  419,   79,    4,  766,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1517,  228,   41,    4, 5327,  372,  165,    4, 1517, 1244,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   38,  417,  418,  932,   41,    4,  233,   35,  235,   22, 2572,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 7870,  939,  107,    4,  671,  952, 2921,  471,   17, 2275,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93, 11127,    56,   103,     4,  1969,   211,   368,\n",
      "            4, 12086,    19,     1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 174,  79,  34, 794,  22,   4, 395,  41,  34, 200,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  343,  700,   14, 5209,    7,    4,  700,   14,\n",
      "        2805,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  518,    4, 7776,  849,    7, 1669,  141,    4, 2500,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 2119,  165, 1696,    4, 9703,  752,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 132, 174, 113, 164, 277, 180, 537, 736,  41,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  562,   62,    7,   21, 1409,   14,  109,   79,    4,  136, 2784,\n",
      "        1865,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  793,    4, 2525, 1652,  113,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1273, 1510,   78,  372,  165,    4, 8005,   56,\n",
      "         178,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  562,   62,    7,   21, 1409,   14,  109,   79,    4,  136, 2784,\n",
      "        1865,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  633,   57,   22,  729,   14, 1168,   41,  262,   14,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  132,  627,    4,  251,   22,   49,  648,  269, 1593,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 1566, 1652,  132,   79, 3971,    7,  514,   73,   34, 1163,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 2671,   54,  174,  372,  165,   74,  419,  407,   34,  796,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   15,   22,    4,   58,   88,  486,   88, 5936,    7,   87,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  937,   92,  368,  164,  362,  228,   41,    4,\n",
      "        1144,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3733,  320,  160,    7,    4, 1189,   41,    4,  958,   14,\n",
      "         457,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 112, 379,  41,   4, 749,   7, 537, 379,  41,  34,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   29,   22,    4,  343,  121,  290,  253,  113,    4, 4119,\n",
      "         535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,   66,  132,   41,    4,  888, 2719,   66,  132,    4,\n",
      "        1968,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 554,  79,  34, 283,   7, 426, 273, 113,  34,  57,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   41,    4, 2500,  777,    4,  405,  446,    4, 1522,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1611,   14,  133, 2814,  228,   41,  262,   14,  888, 2719,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  109,  433,  500,  368,    4, 1304,   22,  111,   79,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3539,   14, 2671,  137,   34,  796,  604,   41,   34, 9659,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  893,   88, 1398,   88, 1400,   88,    7,  419,  759,   54,  569,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   374,   465,    54,   407,     4, 12989, 15716,   228,    56,\n",
      "            7,   345,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  132,  174,  435,   52, 1633,  133,  103,    4,  939,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 6412, 4002,   14,  378,  431, 1219,   22,    4,  583,  416,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   79,    4,  621,  979, 5965,  196,    4,  951,  361,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2281,   22,  164,  369,  299,  739,  616,  165, 3015,    4,\n",
      "         361,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  793,    4, 2525, 1652,  113,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 600, 391, 113,   4, 114,  22, 336,   7, 485,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,  41,   4, 142,  79,   4, 208,  81, 197, 509,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1611,   14,  133, 2814,  228,   41,  262,   14,  888, 2719,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1517,  228,   41,    4, 5327,  372,  165,    4, 1517, 1244,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 794, 134, 228, 372, 165,   4, 796,  79,   4, 794,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 132, 260,  41,   4, 464, 211,  48, 132, 715,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 4370,   14, 3565,   79,  634, 5007,   79,    4,  233,  239,  195,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,   66,  132,   41,    4,  888, 2719,   66,  132,    4,\n",
      "        1968,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  363,    4,  149,   22, 9875, 4281, 3272,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1934, 1935,  173, 1936,   22, 1937, 1938,    7, 1939, 1937, 1730,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14, 465,   7, 109, 228, 196,  34, 993, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   79,    4,   61,  269, 1362,  888,   41,    4, 3669,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14, 465,   7, 109, 228, 196,  34, 993, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  55, 372, 165,   4, 888, 976, 113,   4, 889,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 4370,   14, 3565,   79,  634, 5007,   79,    4,  233,  239,  195,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 4370,   14, 3565,   79,  634, 5007,   79,    4,  233,  239,  195,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  518,    4, 4156,  299,  806,  132, 1013,  164, 1389,\n",
      "          41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  258,  173,  518,    4, 2525, 2182,  211,  368,    4, 4769,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   93,  363, 1566,   41,    4, 1566,  785,   22,   96,   53,  793,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 1566, 1652,  132,   79, 3971,    7,  514,   73,   34, 1163,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  259,  739,  616,  165, 1696,    4,  361,   79,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2281,   22,  164,  369,  299,  739,  616,  165, 3015,    4,\n",
      "         361,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 6412, 4002,   14,  378,  431, 1219,   22,    4,  583,  416,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4263,   41,    4,   86,  136,   22,  467,   79,   34,  302,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  497,   14,  337,    7, 1739,   54,  228,   41,    4,  114,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49, 2699,  522,  939,   79, 2190,  165, 1892,    4,  636,  944,\n",
      "         943,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2525, 4607, 1357,   41,  180, 5336, 1536,  165, 1696,   34, 2525,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   61,  379,  132,  174,   41,  262,   14,    4,  340, 1431,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  738,  173,  228,   41,    4,  134,  372,  165,  164, 1869,  131,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  663,  299,   14,    4, 1698,   22,   34,  578,  195,  120,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  566,  291,    4, 1144,   41,    4,  776,  113,    4,  794,\n",
      "          19,    1])\n",
      "Epoch [1/1], Step [3/8], Loss: 9.2699, Perplexity: 10613.9166INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 554,  79,  34, 283,   7, 426, 273, 113,  34,  57,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 228, 113,   4, 114,  22, 612, 389,  14, 391,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 6412, 4002,   14,  378,  431, 1219,   22,    4,  583,  416,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    29,    66,   107,     4,  1862,   796,     7, 10389,\n",
      "           79,    48,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  57,   7, 407,   4, 125, 105, 793, 273,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  633,   57,   22,  729,   14, 1168,   41,  262,   14,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1734,  739,  283,   36,    4, 2256,  407,    4, 4232,   81,  509,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3733,  320,  160,    7,    4, 1189,   41,    4,  958,   14,\n",
      "         457,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1611,   14,  133, 2814,  228,   41,  262,   14,  888, 2719,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14, 1549,  683, 3337,  210,  663,  520,   79,   34,\n",
      "         270,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1409,    14,     4,  7818,    79,    34,   401, 15451,\n",
      "           34,   135,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   374,   465,    54,   407,     4, 12989, 15716,   228,    56,\n",
      "            7,   345,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   79,    4,  621,  979, 5965,  196,    4,  951,  361,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  663,  299,    4, 3427,   70,  256, 3729,   14, 1630,    7,\n",
      "         337,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   38,  417,  418,  932,   41,    4,  233,   35,  235,   22, 2572,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  783,   56,  180,  666, 1426,  165, 1696,   34, 2525,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53,   54,  174,   88,  793,   49, 1672,   60,   49, 1782,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  574,  888,  850,  427, 3765, 4712,  138,  550, 4096,   19,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  132,  174,  435,   52, 1633,  133,  103,    4,  939,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  55, 372, 165,   4, 888, 976, 113,   4, 889,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  259,  452,   22,  164, 2525, 4769,   79,   34,  539,  544,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 141,   4, 776,  56,  34, 189,  14,   4, 862,  19,   1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  259,  452,   22,  164, 2525, 4769,   79,   34,  539,  544,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  132,  174,  435,   52, 1633,  133,  103,    4,  939,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  991, 2689,    7,    4, 1103,  262,   88,   79,    4, 1828, 1709,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 5082,   93,  748,  268, 1759,  211,  610,    4,  444,   59,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 2671,   54,  174,  372,  165,   74,  419,  407,   34,  796,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2671,   55,   41, 2123,  195,  372,  165, 1119,   79, 1763, 1374,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53,   54,  174,   88,  793,   49, 1672,   60,   49, 1782,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  783,   56,  180,  666, 1426,  165, 1696,   34, 2525,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1409,    14,     4,  7818,    79,    34,   401, 15451,\n",
      "           34,   135,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,   553,    79,   766,   195,    22,   332,   263,    79,\n",
      "        21005,  4623,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1517,  228,   41,    4, 5327,  372,  165,    4, 1517, 1244,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2560,  228,  500,   14,    4,  121,   22, 1429, 2021,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    29,    66,   107,     4,  1862,   796,     7, 10389,\n",
      "           79,    48,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    38,   228,   113,     4, 10324,   211,     4,   888,\n",
      "          960,   103,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 1195,  299,   61,  221,   14,  592,  113,    4, 2121,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  259,  739,  616,  165, 1696,    4,  361,   79,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 5352, 7111,  320,  160, 5234,  211,  112,  173,  736,   41,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,  793,    4, 2525, 1652,  113,    4,  149,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 228, 113,   4, 114,  22, 612, 389,  14, 391,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  132,  627,    4,  251,   22,   49,  648,  269, 1593,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  518,    4, 4156,  299,  806,  132, 1013,  164, 1389,\n",
      "          41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    38,   228,   113,     4, 10324,   211,     4,   888,\n",
      "          960,   103,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  3203,   995,  1672,    22,    17, 13073,   103,     4,\n",
      "         1463,   574,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,    7,   20, 1549,  132,   41,    4, 2966,  113,   49,\n",
      "        2858,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  109,  433,  500,  368,    4, 1304,   22,  111,   79,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1273, 1510,   78,  372,  165,    4, 8005,   56,\n",
      "         178,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,   7,   4, 995, 259, 356, 520,  79,   4, 332,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4263,   41,    4,   86,  136,   22,  467,   79,   34,  302,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,   518,     4,   849,     7, 17461,   883,   368,\n",
      "            4,  2053,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1517,  228,   41,    4, 5327,  372,  165,    4, 1517, 1244,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2560,  228,  500,   14,    4,  121,   22, 1429, 2021,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   937, 20095,     7,   164,   362,   927,   113,    34,\n",
      "          588,   261,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 112, 379,  41,   4, 749,   7, 537, 379,  41,  34,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1498,  290,   15,   22,    4,   28,  114,   16,  457, 1989,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 6412, 4002,   14,  378,  431, 1219,   22,    4,  583,  416,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   79,    4,   61,  269, 1362,  888,   41,    4, 3669,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1273, 1510,   78,  372,  165,    4, 8005,   56,\n",
      "         178,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14, 465,   7, 109, 228, 196,  34, 993, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  3203,   995,  1672,    22,    17, 13073,   103,     4,\n",
      "         1463,   574,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  132,  627,    4,  251,   22,   49,  648,  269, 1593,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  213,  972,   79,  123,   14,    4,  343, 1871,   14,   53,   55,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   29,   22,    4,  343,  121,  290,  253,  113,    4, 4119,\n",
      "         535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1433,   56,  372,  165,    4, 2297,  368,   52,  364,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 9556,  136,   22,  268,  378,  372,  165,    4,  448,   14,\n",
      "         283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1409,    14,     4,  7818,    79,    34,   401, 15451,\n",
      "           34,   135,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  109,  433,  500,  368,    4, 1304,   22,  111,   79,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93, 11127,    56,   103,     4,  1969,   211,   368,\n",
      "            4, 12086,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 174,  79,  34, 794,  22,   4, 395,  41,  34, 200,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1511, 2638,   16,   34, 1044,  165,   34, 5501,   79,   34,  526,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1498,  290,   15,   22,    4,   28,  114,   16,  457, 1989,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  671,  107, 2610, 3002,   79,   48,   22, 6099,   41,  262,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  57,   7, 407,   4, 125, 105, 793, 273,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,  41,   4, 142,  79,   4, 208,  81, 197, 509,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53,   54,  174,   88,  793,   49, 1672,   60,   49, 1782,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  991, 2689,    7,    4, 1103,  262,   88,   79,    4, 1828, 1709,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343,   82,   14,  465,   54,   79,    4, 1871,   41, 1369,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 141,   4, 776,  56,  34, 189,  14,   4, 862,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,   7,   4, 995, 259, 356, 520,  79,   4, 332,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2281,   22,  164,  369,  299,  739,  616,  165, 3015,    4,\n",
      "         361,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   93,  368,    4, 1004,  379,   79,  268,  817,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,    7,   20, 1549,  132,   41,    4, 2966,  113,   49,\n",
      "        2858,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4351, 3348,  228,  372,  165,   74,  419,   79,    4,  766,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  893,   88, 1398,   88, 1400,   88,    7,  419,  759,   54,  569,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  566,  291,    4, 1144,   41,    4,  776,  113,    4,  794,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53, 1108,   79,    4,  549,  446,    4,  708,   14,  111,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1734,  739,  283,   36,    4, 2256,  407,    4, 4232,   81,  509,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1781,   41,   34,  189,   95,  752,    4,  776,  405,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53, 1108,   79,    4,  549,  446,    4,  708,   14,  111,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   29,   22,    4,  343,  121,  290,  253,  113,    4, 4119,\n",
      "         535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  109,  433,  500,  368,    4, 1304,   22,  111,   79,   48,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   79,    4,   61,  269, 1362,  888,   41,    4, 3669,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53,   54,   55,   56,   34, 2642,   41,    4, 1454,  609,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3539,   14, 2671,  137,   34,  796,  604,   41,   34, 9659,\n",
      "          19,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   79,   62, 4465,   41,    4,   20,    7,   21,  406,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   374,   465,    54,   407,     4, 12989, 15716,   228,    56,\n",
      "            7,   345,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 112, 379,  41,   4, 749,   7, 537, 379,  41,  34,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   700,    14,  3154,   107,  2710,    22, 18764,  1570,\n",
      "           41,   262,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1498,  290,   15,   22,    4,   28,  114,   16,  457, 1989,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  937,   92,  368,  164,  362,  228,   41,    4,\n",
      "        1144,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  132,  627,    4,  251,   22,   49,  648,  269, 1593,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 2260, 1591,  133,   81,    4,  448,   14,  283,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   22,    4, 4960,   41,  164,  806,  368,    4,  874,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93, 11127,    56,   103,     4,  1969,   211,   368,\n",
      "            4, 12086,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 6412, 4002,   14,  378,  431, 1219,   22,    4,  583,  416,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2671,   55,   41, 2123,  195,  372,  165, 1119,   79, 1763, 1374,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2281,   22,  164,  369,  299,  739,  616,  165, 3015,    4,\n",
      "         361,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1734,  739,  283,   36,    4, 2256,  407,    4, 4232,   81,  509,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3539,   14, 2671,  137,   34,  796,  604,   41,   34, 9659,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,   553,    79,   766,   195,    22,   332,   263,    79,\n",
      "        21005,  4623,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2671,   55,   41, 2123,  195,  372,  165, 1119,   79, 1763, 1374,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  663,  299,    4, 3427,   70,  256, 3729,   14, 1630,    7,\n",
      "         337,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  893,   88, 1398,   88, 1400,   88,    7,  419,  759,   54,  569,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  343,  700,   14, 5209,    7,    4,  700,   14,\n",
      "        2805,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2560,  228,  500,   14,    4,  121,   22, 1429, 2021,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4351,  635,  332,   22, 3706,   37, 3758,   79,    4,   29,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  343,  700,   14, 5209,    7,    4,  700,   14,\n",
      "        2805,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1517,  228,   41,    4, 5327,  372,  165,    4, 1517, 1244,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   374,   465,    54,   407,     4, 12989, 15716,   228,    56,\n",
      "            7,   345,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 132, 260,  41,   4, 464, 211,  48, 132, 715,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1273, 1510,   78,  372,  165,    4, 8005,   56,\n",
      "         178,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1934, 1935,  173, 1936,   22, 1937, 1938,    7, 1939, 1937, 1730,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 6412, 4002,   14,  378,  431, 1219,   22,    4,  583,  416,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   79,    4, 1059,    7,  574, 1389,  106,  427,    4,\n",
      "         523,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1517,  228,   41,    4, 5327,  372,  165,    4, 1517, 1244,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53,   54,   55,   56,   34, 2642,   41,    4, 1454,  609,\n",
      "          19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 368, 256, 111,  79, 123,  14,  49, 335, 218,  19,   1])\n",
      "Epoch [1/1], Step [4/8], Loss: 8.9254, Perplexity: 7520.4100INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338, 452,  73,   4, 342,  79,  34, 526,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  342,   14,   49, 1509,  739,  616,  165,  362,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2260,   29,   22,   76,  706,    7,  631,   79, 2172,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   460, 10590,   893,  2086,  2938,   165,     4,    11,   155,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,   61, 3322, 2503,   41,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 174,  79, 123,  14,  49, 697, 185,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 5794,    7,    4,  173, 2106,  539,  165, 2117,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14, 612,  53,  79,   4, 891,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,  132,  141,    4, 2500,   56,    4,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,    62,     7,    21,   355, 18096,    41,     4,   332,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  197, 2793,   54, 2106,  812, 1355,   79,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  117,  132,  518,    4,   61,   88, 5444, 2526,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 4764,  380, 4002,   22,    4, 8186,  262,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1530,   14, 5209,    7, 9256,   22,  625,    7,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2663,   32,  228,   41,   34,   42,   81,  284,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75,   53,   54,  739,  616,  165,  554,  256, 1566,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 4891, 1698,  739,  572,  299,   41,    4, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  872,  973,  256,  683,   79,  147, 1079,  310,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  82,  14,  53, 174, 372, 165,  74, 419,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1900,   49, 2157, 6096,   79,    4,   15,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1480, 5258, 2209,  132,  439,  572,   22, 5160,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 169, 170,  66, 132, 228,  79,  34, 138,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,   55,   41,   34,  189,   14,   34,   35,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,    7,    4, 1969,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888,  850,   56, 2719,  349,    4,  182,  535,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4652, 7252,   41,  138,   41,    4, 3374,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  273,  210,   79,    4, 2223,  315,   81,   74,  419,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1833,  291,    4, 2123,  142,   79,   34, 1807,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2016,   22, 3003,   78,   41,    4, 4355,  263, 1530,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    37,  1519, 11577,  2969,     4, 11577,  1966,   889,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4, 11577,   345,    36,     4,  1189,    21,    48,  2753,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  111,  132,  439, 1979,   79,   34, 4129, 1304,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1672,    22, 13073,   132,   174,    79,    34,   138,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4, 15353,    15,  1238,    22,   350,    18,     7,  2382,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 256, 265,   7, 266,  54, 732,  36,   4, 156,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  197, 2793,   54, 2106,  812, 1355,   79,   34,  283,   19,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,   352,    54,  1022,    56,     4, 11869,   526,   453,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1833,  291,    4, 2123,  142,   79,   34, 1807,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  888,  925,  537,  188,    4,  322,   14,  888, 2719,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  347,   41,    4,   20,  931,   79,   34, 2384,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 4600,   54,  363,   79,   34, 1763,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 4738,  939,  133,   79,    4,   61,  214,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1459,    4, 1566,  361,   22,    4, 4074,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 141, 325,  56,   4, 526, 263, 719,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,   79,    4,  745, 1782,   22,  182,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338, 452,  73,   4, 342,  79,  34, 526,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 219,  15,  22, 269, 867,   7,  87,   7,  18,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1616, 2659,    7,  268, 7922,  174,   79,    4, 1498,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  182, 2659,  174,  372,  165,    4, 4037, 2952,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  851,   22,  256,  813,   14, 2384,   41,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 4652, 7252,   41,  138,   41,    4, 3374,  453,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   173,   228,    81,     4,  1953,    14, 14339,   787,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  28,  29,  22,  34, 114, 322,  73,   4, 804,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   926,  1192,   100,   736, 14302,   165,   104,  2264,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 1056,  132,   79,   34,  401,   14,   34, 3484,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  2587,    22,  4888,  2503,   404, 11953,   197,  1880,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1698, 2099,  113,   34, 3133,   41,   34, 2966,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1369, 1486,   79,   34,   80,   60,    4,  269, 1119,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 260,  41,  34, 406,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   62,    7,   21,  117,  132, 1001,   41,   34,  735,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 140, 141, 261,  41, 104, 406,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  123,   14,    4,   20,  575,    7,   21, 1509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  483, 1566, 1652,   79, 5135,   41,   34,  785,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  111,  132,  439, 1979,   79,   34, 4129, 1304,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   666,   132,  2694, 12607,   164,   391,   603,   666,  3851,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  117, 5542,   41,    4,  542,   81,   34, 1431,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  817,  368,    4, 3427,   79,    4, 1211, 1031,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  999, 1325,   79,    4, 3838,  550,   22,  711,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 1459,    4, 1566,  361,   22,    4, 4074,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 3979,   14,    4,  340,  648,  165,    4, 1952,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  465,  363, 4281, 5000,   41,    4,  343,  670,  238,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 1080,  427, 1015, 4842,    4, 9703,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  379,   79,  335,  744,  208, 3025,  210,   37, 4258,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,   79,    4,  745, 1782,   22,  182,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 1056,  132,   79,   34,  401,   14,   34, 3484,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  49, 858,  13,  14,   4,  93, 368,   4, 105,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  599,  592,  132,   41,    4, 1698,   22, 9117,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1566, 1652,  291,  299,  165, 1696,    4,  361,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  408,   14, 2284,  322,  299,   41,    4,  114,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 368,   4, 617, 362,  79, 104, 386,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,  134,  228,   41,    4, 1405,  125, 2117,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2260,   29,   22,   76,  706,    7,  631,   79, 2172,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,  100,   55, 1060,    4, 1279,  178,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  562, 3135,  132, 7570,   41,  999,  171, 6224,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   53,   79,   34, 2354,  228,  113,    4,  114,  345,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  174,   79,  283,  368,    4, 4888, 1178,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2260,   29,   22,   76,  706,    7,  631,   79, 2172,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  34,  57, 363,  22,   4, 361,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1530,   14, 5209,    7, 9256,   22,  625,    7,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 228, 103, 607, 113,  34, 993, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   61,   82,   14,   53,   41,  820, 1169,  325,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  21, 495, 473, 228, 372, 165,   4,  59,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  791,   37,  362,  368,    4, 6624,  290, 2209,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  282,  271,   14, 3954,  394,   41,   34,   80,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   117, 12335,    41,     4,   794,   134,   793,  3505,\n",
      "         1232,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  371, 2774,    4, 9215, 4744,   41,   34, 1915,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  34,  57, 363,  22,   4, 361,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   117, 12335,    41,     4,   794,   134,   793,  3505,\n",
      "         1232,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 4738,  939,  133,   79,    4,   61,  214,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  290,  299,  113,    4, 1697, 1509,   79,    4,   20,  270,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1880,  174,   81,   74,  419,   41,    4,  305,  262,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  34,  57, 363,  22,   4, 361,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 872, 132, 174,  79, 123,  14, 104, 776,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 174,  41,  34,  57, 363,  22,   4, 361,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  562, 3135,  132, 7570,   41,  999,  171, 6224,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,  132,   55,  435,    4,  182,  269, 2054,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2260,  178,  132,  133,   79,   34,  213,  730,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 141, 122,  41,   4, 725, 233,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  147,  132,    4,   29,   22,  269, 1040,    7,   64,   65,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256, 2671,   95,   41,   34, 2123,  427,  256,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 4738,  939,  133,   79,    4,   61,  214,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  174,   79,  283,  368,    4, 4888, 1178,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  117,  132,  518,    4,   61,   88, 5444, 2526,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  465,   54,  113,   34,  617,  794,  617, 1735,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 4600,   54,  363,   79,   34, 1763,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  21, 495, 473, 228, 372, 165,   4,  59,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652, 1080,  427, 1015, 4842,    4, 9703,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  282,  271,   14, 3954,  394,   41,   34,   80,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  282,  271,   14, 3954,  394,   41,   34,   80,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,  995,  763,  290,  113,    4,  791,  949, 2740,  592,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  295,   14,    4,   82,   14, 1880,  210,   79,  683,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,   79,    4,  745, 1782,   22,  182,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 260,  41,  34, 406,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 258,  94, 736,  41, 113,   4, 949, 592,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    22,     4, 10417,  4652,    41,     4,  3919,\n",
      "           19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3634,  334,   56,   41,   34,  719,   73,    4,  590,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3539,   14, 4652,  174,  196,   79,   49, 1782,   19,    1])\n",
      "Epoch [1/1], Step [5/8], Loss: 8.4162, Perplexity: 4519.8324INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 291,   4, 617, 362,  79,  34, 683,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,   4, 931,  22,   4, 629, 289,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 345, 993, 113,  34, 993, 114,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 510, 363,  41,   4, 891,  29,  65,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 621, 169, 170, 334,  79,   4, 138, 208,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 7934,  256, 1437,   36,    4,  408,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 291,   4, 617, 362,  79,  34, 683,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1711,   14,   61, 2503,   79,  448,   14,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75, 1065,   14,  479,  111,  366,  171, 2209,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 8821,   95,  500,   41,    4,  744, 1498,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49, 1672,   41,    4, 2123,  200,   81,    4, 1119,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 7934,  256, 1437,   36,    4,  408,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100,  41, 256, 325,  79,  34, 526,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,     5,   343, 15642,   395,    14,    49,   574,  6665,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 7721,    9, 2016,   54,   41,    4, 4177,  499,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 132,   4,  21,  15,  22,   4, 121,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1098,  355, 2391,   41,    4,  599, 2696,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  132, 1112,    4,  390,  165,    4,  378,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2209,   66,  132,  228,   41,    4,  114,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 197, 265,  54, 628,  47,  34,  15, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  53, 174,  79,   4,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 621, 169, 170, 334,  79,   4, 138, 208,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 7934,  256, 1437,   36,    4,  408,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1665,  228,  372,  165,    4, 3340,  160,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 9093,    4,  282,   56,    4,  276,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    92,   141,     4,   464,    79,     4, 21557,   588,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  562, 1350, 2481,   41,  999,  113,   49, 8404,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  497,   14, 2592,  113,    4,   98, 7898,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   62,    7,   21,   27, 2107,  258, 1548,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 7934,  256, 1437,   36,    4,  408,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53, 2794,    7,   84,   79,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1942,  939,  132,  932,  427,   34,  233,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   52,  258,  109,   95,   22,  120, 1168,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  380, 5792,  103, 4860, 2608,    4,  535,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 374, 465,   7,   4,  93, 363, 529, 848,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14, 1369,  174,  196,  290,   73,  138,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1297,   14,  583, 8180,   22,  479, 1334,   41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 8821,  137,  113,   34, 1763,   41,    4, 1454,  609,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 109, 174, 103,   4, 390, 235, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  25,  22, 126, 342,   7,   4, 134,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   62,    7,   21,   27, 2107,  258, 1548,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1480, 2209, 2042,   41,  165,    4,   57,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1369,   54,  174,   79,    4,  136, 4269,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2742,  290,  253,   14,    4,  690, 1782,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  53, 518, 325,  41,   4, 526, 263, 453,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1665,  228,  372,  165,    4, 3340,  160,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1591,   54,  133,  103,    4,  535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  342,   14,  256, 6043, 1363,    7, 1307,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   59,  132,  500,   79,    4, 2358,  195,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  25,  22, 126, 342,   7,   4, 134,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   59,  132,  500,   79,    4, 2358,  195,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  53, 174,  79,   4,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  34,  52, 296,  54, 363,   4, 529, 149,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 132,   4,  21,  15,  22,   4, 121,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1297,   14,  583, 8180,   22,  479, 1334,   41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,   4, 931,  22,   4, 629, 289,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  256,   53, 2794,    7,   84,   79,   34,  283,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,  2668,    87,    54, 16721,    79,    34,   340,   393,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  673, 1940,  627,   79,   34,  559, 1867,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2065, 1620,   79,    4, 2603,   14, 4514,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 197, 265,  54, 628,  47,  34,  15, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  673, 1940,  627,   79,   34,  559, 1867,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1591,   54,  133,  103,    4,  535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1480, 2209, 2042,   41,  165,    4,   57,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,  141, 1633,   41,    4, 1087,   10,   42,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  132, 1112,    4,  390,  165,    4,  378,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  380, 4002,  107,  125,  278,   41,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173,  79,   4, 853, 783,  41, 164, 931,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    52,  2818, 10771,    79,     4,   149,    14,  2525,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  53, 174,  79,   4,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  497,   14, 2592,  113,    4,   98, 7898,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 9093,    4,  282,   56,    4,  276,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  71,  79,   4,  61,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54, 2503,  925,  879,    4,  128,  251,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  562, 1350, 2481,   41,  999,  113,   49, 8404,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 197, 265,  54, 628,  47,  34,  15, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100,  41, 256, 325,  79,  34, 526,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 291,   4, 617, 362,  79,  34, 683,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,   79,    4,   29,  290,   79,  112, 2155,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 621, 169, 170, 334,  79,   4, 138, 208,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2549,  550,   22,  729,   14,   61, 2503,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   59,  132,  500,   79,    4, 2358,  195,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 663, 299,  14,  49, 574,   7,  49, 159,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,  996,   56,  165, 2687,  926,  138,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 621, 169, 170, 334,  79,   4, 138, 208,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  53, 174,  79,   4,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  995,   94, 1900,   41,    4,  525,  719,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  92, 141,   4, 464,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 132, 260,  41,  49, 588, 464,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,   79,    4,   29,  290,   79,  112, 2155,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  380, 4002,  107,  125,  278,   41,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  502,    4, 1409,   14,  164,  564,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1942,  939,  132,  932,  427,   34,  233,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  784,  888,   41,   49, 2299,   81,    4,  535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 132, 210,   4, 395, 113,  34,  57,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 2008,   41, 2301,   14,    4,   76, 1691,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1591,   54,  133,  103,    4,  535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  49, 574, 228,  41, 262,  14,   4, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,   53,  368, 1566, 6844,   79,  104,  817,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2742,  290,  253,   14,    4,  690, 1782,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 2405,  164,  566, 1479,  164,  885,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 8821,  137,  113,   34, 1763,   41,    4, 1454,  609,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659, 2279, 3109, 3125,  256, 5192,  509,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  132,  174,   22,  256,  111,  165, 1686,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  173, 1781,   34,  862,   22,    4, 2500,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  380, 4002,  107,  125,  278,   41,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   338,    14,   296,   363,    49, 22946,   149,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  132,  174,   22,  256,  111,  165, 1686,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2209,   66,  132,  228,   41,    4,  114,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  502,    4, 1409,   14,  164,  564,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  612, 3325,   14,   52, 3615, 3625,  103, 1169,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  49, 574, 228,  41, 262,  14,   4, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2549,  550,   22,  729,   14,   61, 2503,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,    5,    6,   29,    7,    4, 1213,   14,  571,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 663, 299,  14,  49, 574,   7,  49, 159,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3022,  244,  888,   38, 7098,   22, 5397,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,  1566,  1652, 16396,  5601,    22,    34, 18645,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  342,   14,  256, 6043, 1363,    7, 1307,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 4652,   54,   79,    4,  208, 1248,    4, 5586,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,   53,  368, 1566, 6844,   79,  104,  817,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,    5,    8,  890, 2044, 1041,   41,   34,   35,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  244,   62,    7,   21,   27,   14, 2525, 2818,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54, 2503,  925,  879,    4,  128,  251,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  379,  331,   56, 2385,  263,  103,    4, 4701,   19,    1])\n",
      "Epoch [1/1], Step [6/8], Loss: 7.7928, Perplexity: 2422.9852INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 621,   7, 575, 888, 133, 113,   4, 888, 889,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1566, 1652,  341,    4, 2586,   41,    4, 1566,  785,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 255,  54,  53, 174,  41,  34,  57,  81,  34, 283,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   41,   34,   57,  132, 4842,    4,   37,  364,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 621,   7,  20, 178, 850,  56,   4, 233,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 132,  55,  79,  34, 102,  22,  49, 105,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 132,  49, 295,  14,   4, 231,  15,  22, 350,  18,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 197,  53, 277,  79, 123,  14,   4, 343, 219, 535,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 636,  41,   4, 169, 170,  41,   4, 233, 580,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 368,   4,  20, 364,  79, 164, 488, 386,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 860,  41,   4,  35, 580, 180,   4, 178, 960,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   37, 6709,  888, 1254,   17, 1044,   56,   34, 2719,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2525, 2818,  290,   41,   36,   34, 2308,  417,  418,  134,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    34,   371,   132,   518,     4,  2060,  1615,     7,   132,\n",
      "        15554,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  115,  890, 2338, 1543,   54,   41,    4,  851,   22, 6820,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   53,   54,  141, 1369,   79,   34,   80,   41,   34,   57,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34, 3368,  132, 3871, 2407, 2629,   34,   53,  174,  196,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1056,   79,   34,  683,  777,    4,  539, 3733,   41, 3117,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   485,    14,   111,   228,   189,   103,   189,    41,     4,\n",
      "        16222,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  258,  173,  132, 3482, 4064,   41,  164,   84,  362,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  585,   56,  211, 2769,   41,    4,  320,  160,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1056,   79,   34,  683,  777,    4,  539, 3733,   41, 3117,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   20,    7,  621, 5429,  228,  281,   17, 4631,  500,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 8804, 8805,  379, 3024,    4,  364,   79,   17, 1668,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 134,  81,   4, 448,  14, 283, 372, 165,   4, 535,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  330,   22, 4512,   55,   79,    4,  745,   79,  208,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   20, 5453,  583,  416,  732,  644,   14,    4, 5453, 2696,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 174, 407,  49, 105,  41,   4, 608,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 272, 395, 132, 210,  79,  34,  20, 715, 270,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   255,   132,  2560,    88, 18834,     7,   337,    41,    34,\n",
      "          851,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2517,  210,   81,    4,  343,  448,   14,  283,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  132,  368,    4,   76,  700,    7,    4, 2741,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 174, 407,  49, 105,  41,   4, 608,  35,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   60,   14,    4,  218,   22,  729,   14,  111,    7, 1666,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100,  41,   4, 776, 777,   4, 405, 113, 135,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54,   75, 1633,  113,   34,  189,   14,   34,   42,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  136, 2784, 1865, 1291,    4, 1865,  427,    4,  794,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  140, 1195,  471,    4, 2209,  376,    4, 3483,    7, 1517,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 113,   4, 114, 368,   4, 958,  14, 111,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   20, 5453,  583,  416,  732,  644,   14,    4, 5453, 2696,\n",
      "           1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 3756,   54,   41,  366,    7,  228,   41,    4,  114,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1566, 1652,  341,    4, 2586,   41,    4, 1566,  785,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54,   75, 1065,   14,    4,  258,  259, 4842,    4,  361,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 663, 299,  14,   4, 100, 316,  41,   4, 320, 160,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   20, 5453,  583,  416,  732,  644,   14,    4, 5453, 2696,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   299,   663,   342,    14,   117,   228,  1284,    41,     4,\n",
      "           20, 11346,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1698,  228,   41,   49, 7406,  579,  505, 6487, 8034,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93,  22,   4, 272, 105,  79, 123,  14, 268,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  11, 310, 132, 569,  22,   4,  59,   7,  58,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 113,   4, 114, 368,   4, 958,  14, 111,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,    7,   62,  629,  178,  113,    4, 1790,  821,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 3099,  141,   41,   34,  539,   14,    4,   20,  931,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    75,   337,    22, 11238,    54,    41,   336,     7,    79,\n",
      "          485,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  939,  132,  572,  211,   48, 1406,   56,    4,   42,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 290, 113,   4, 949, 592, 228,  41,   4, 114,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,  132, 1195,    4, 1304, 2209,   22,    4, 1517,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 141,   4, 231, 464,  41,   4,  84, 362,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2659,  345, 2096,   36,   34,  262,   14,    4,  796,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   60,   14,    4,  218,   22,  729,   14,  111,    7, 1666,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   31,   32,    4,    7,   52,   35, 1086,    7,  256,  770,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,  465,  141,  163,  141,    4, 2604,  464,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  197,   53,  407, 1168,   14,  479, 1975,   79,   34,  102,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  140, 1195,  471,    4, 2209,  376,    4, 3483,    7, 1517,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  233,  178,   41,   34,   35,   41,   49, 7203,  609,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14,   53,   55,  196,    4, 1438,   22, 4178,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  585,   56,  211, 2769,   41,    4,  320,  160,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  228,   79,  123,   14,   52, 1306,   14, 1648,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   20,    7,  621, 5429,  228,  281,   17, 4631,  500,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 290, 113,   4, 949, 592, 228,  41,   4, 114,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,    5,  791,   20, 1048, 1623,  939,   41,   34,   42,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 422, 594,  41,  21,  80, 290, 113,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,  258,   53,  368,    7, 3800,    4,  395,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 113,   4, 114, 368,   4, 958,  14, 111,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 3756,   54,   41,  366,    7,  228,   41,    4,  114,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  329,  416,   22,  164,  175,  504,  196,    4,  526, 2127,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   485,    14,   111,   228,   189,   103,   189,    41,     4,\n",
      "        16222,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,    7,   62,  629,  178,  113,    4, 1790,  821,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 2525, 2818,  290,   41,   36,   34, 2308,  417,  418,  134,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 197,  53, 277,  79, 123,  14,   4, 343, 219, 535,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 872, 959,   4, 405, 372, 165,   4, 142, 347,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21,    7,   62,  629,  178,  113,    4, 1790,  821,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49, 1920,   14,  111,   22, 4592,  510,    7, 4318, 6432,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93,  22,   4, 272, 105,  79, 123,  14, 268,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  408,   14, 1825, 2062,   54,  998,   41,    4,  114,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 636,  41,   4, 169, 170,  41,   4, 233, 580,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  888, 2809,   56,   34,  888, 2044,  103,  256,  138,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1969,  132,  174,  969,  435,    4,  497,   14, 3939,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93,  22,   4, 272, 105,  79, 123,  14, 268,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  748,    4,  378,   81,    4,  999,   14, 2671,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  37,   7,  62, 931, 133,  41,   4, 233, 234,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  115,  890, 2338, 1543,   54,   41,    4,  851,   22, 6820,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   79,   62, 4465,  260,   41,    4,   21,  406,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2154,  228,  372,  165,    4,  810,   41,    4, 1431,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 132,  49, 295,  14,   4, 231,  15,  22, 350,  18,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 860,  41,   4,  35, 580, 180,   4, 178, 960,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   22,    4, 1653,  413,  165, 1696,    4,  361,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1011,  380,  228,   41,  262,   14,   49,  330, 4443,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2154,  228,  372,  165,    4,  810,   41,    4, 1431,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14,  258,   53,  368,    7, 3800,    4,  395,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  269,  794,  134,   41,   34,  189,   14,    4, 2263,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  37,   7,  62, 931, 133,  41,   4, 233, 234,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  330,   22, 4512,   55,   79,    4,  745,   79,  208,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  258,  173,  132, 3482, 4064,   41,  164,   84,  362,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  330,   60,    4, 1119,   79,  123,   14,    4,  535,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,    93,    79,     4,  1839,     7, 17279,   748,    49,\n",
      "          105,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 173, 210, 427,  34, 683, 211, 141,   4, 776,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   168,    14,     4,  6099, 17004,  1004,   379,    79,\n",
      "          460,  5179,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54,   75, 1065,   14,    4,  258,  259, 4842,    4,  361,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  851,   14, 2560,    7, 2710,   22,    4,  390,   14, 1637,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  117, 1666,  283,   36,    4,  700,   41,   34,   65,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  198,    7,   62,   15,  548,  407,    4, 1002,  486,  185,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93,  22,   4, 272, 105,  79, 123,  14, 268,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  378, 2659, 2637,    4,  978,   36,   17, 8340,  113,   34, 1763,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  11,  22,   4,  59,  88,  58,  88,   7, 154,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1698,  228,   41,   49, 7406,  579,  505, 6487, 8034,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  228,   79,  123,   14,   52, 1306,   14, 1648,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343,  497,   14, 1467,  228,   79,    4,  111, 5104,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 4919, 2774,   79,   34,  588,   41,    4, 3470,  609,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 6178,  575,  806,   22,  164,  817,   41,  164, 5628,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14, 2525, 2818,  174,   41,    4,  208,  520,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   22,    4, 1653,  413,  165, 1696,    4,  361,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   60,   14,    4,  218,   22,  729,   14,  111,    7, 1666,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   79,    4, 6311,  806,  376,    4,  320,  160,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   168,    14,     4,  6099, 17004,  1004,   379,    79,\n",
      "          460,  5179,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   255,   132,  2560,    88, 18834,     7,   337,    41,    34,\n",
      "          851,    19,     1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  851,   14, 2560,    7, 2710,   22,    4,  390,   14, 1637,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  258,  173, 2637,  616,  165, 1696,    4, 1566,  361,   19,\n",
      "           1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   299,   663,   342,    14,   117,   228,  1284,    41,     4,\n",
      "           20, 11346,     1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [7/8], Loss: 7.3192, Perplexity: 1508.9258INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  428,  623,    4, 1345,   41,    4,  613,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75, 7755,   81,    4,  173,    7,    4,   94,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  15,  22,   4, 548,  49, 185, 256, 632,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  15,  22,   4, 548,  49, 185, 256, 632,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,   53,   54,   41,    4,   20, 1566,  785,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   59,  132,  500,   79,    4, 2358,  195,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 7721,    9, 2016,   54,   41,    4, 4177,  499,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   173,     7,   738,    93,  1023,    22, 14291,     7,   592,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    49, 22754,   622,     7,  3542,    41,     4,   340,  1431,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  502,    4, 1409,   14,  164,  564,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,   4, 931,  22,   4, 629, 289,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,    5,    8,  890, 2044, 1041,   41,   34,   35,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  673, 1940,  627,   79,   34,  559, 1867,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    34,  1463,  4934,    54,    34,  2025, 12161,   111,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1711,   14,   61, 2503,   79,  448,   14,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1762, 1372,  180,    4,  100,  275,    4, 2500,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   35,  116,   22, 1088,   37,  278,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,   4, 931,  22,   4, 629, 289,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   35,  116,   22, 1088,   37,  278,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,  141, 1633,   41,    4, 1087,   10,   42,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 269,  28, 195,  79,   4,   8, 146, 115,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 7721,    9, 2016,   54,   41,    4, 4177,  499,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   49,  562, 1350, 2481,   41,  999,  113,   49, 8404,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  49, 574, 228,  41, 262,  14,   4, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  92, 141,   4, 464,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652,  341,    4, 2586,  113,    4,  361,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   22,    4,  851,  271,   14, 2016,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  502,    4, 1409,   14,  164,  564,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 995, 763, 363,  22, 951,  77, 211, 140, 552,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 510, 363,  41,   4, 891,  29,  65,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   837,  2696, 22522,   299,     4,   891,    29,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93, 1415,    4, 4769,  113,    4, 2525,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  995,   94, 1900,   41,    4,  525,  719,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   35,  116,   22, 1088,   37,  278,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 995, 763, 363,  22, 951,  77, 211, 140, 552,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   52,  258,  109,   95,   22,  120, 1168,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75, 1065,   14,  479,  111,  366,  171, 2209,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   82,   14, 2671, 1641,   79,    4,  208,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2209,   66,  132,  228,   41,    4,  114,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   52,  465,   54,  957,   73,   34, 1232,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,  141, 1633,   41,    4, 1087,   10,   42,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75, 7755,   81,    4,  173,    7,    4,   94,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  114,   22,    4,   37, 1919,    7,   21, 6162,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,   66,  132,  174,  579,    4, 2388,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    34,  1911,  3050,   132, 10113,     4,  1315,  5773,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1665,  228,  372,  165,    4, 3340,  160,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1711,   14,   61, 2503,   79,  448,   14,  283,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   62,    7,   21,   27, 2107,  258, 1548,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 1369,   54,  174,   79,    4,  136, 4269,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 8821,  137,  113,   34, 1763,   41,    4, 1454,  609,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 259, 260,   4, 464,  41, 164, 406,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 2008,   41, 2301,   14,    4,   76, 1691,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  15,  22,   4, 548,  49, 185, 256, 632,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 995, 371,  66, 132, 228,  41,   4, 134,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2742,  290,  253,   14,    4,  690, 1782,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  995,  621,  169, 7440, 2623,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1431,   22,    4,  810,    7,   49, 5255,  622,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  71,  79,   4,  61,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 338,  14,  53, 174,  79,   4,  29,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  343, 2034,   14, 3711, 1194,   41,  999,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  784,  888,   41,   49, 2299,   81,    4,  535,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 2008,   41, 2301,   14,    4,   76, 1691,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   62,  117, 1001,   41,    4, 1046, 3043,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  380, 5792,  103, 4860, 2608,    4,  535,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  185,  107,    4,  169, 4865,   60,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652,  341,    4, 2586,  113,    4,  361,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,   52,  258,  109,   95,   22,  120, 1168,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93, 1415,    4, 4769,  113,    4, 2525,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  995,  621,  169, 7440, 2623,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 132,   4,  21,  15,  22,   4, 121,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  379,  331,   56, 2385,  263,  103,    4, 4701,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100, 2008,   41, 2301,   14,    4,   76, 1691,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,   4, 776,  41,   4, 862,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173,  132, 1013,  164,  931,  853,   41,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  410,   53,   55,   41,    4, 2384,  568,  195,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  114,   22,    4,   37, 1919,    7,   21, 6162,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100,  41, 256, 325,  79,  34, 526,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  49, 574, 228,  41, 262,  14,   4, 202,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    53, 22130,     7, 17641,   446,    34,   588,   283,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  92, 141,   4, 464,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 100, 141,   4, 776,  41,   4, 862,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52, 2671,  996,   56,  165, 2687,  926,  138,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,    5,    8,  890, 2044, 1041,   41,   34,   35,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  173,  141,  325,   22, 1732,   41,   34,  397,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1771, 4571, 4572,  165, 2731,   49,  295,   47,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  995,  621,  169, 7440, 2623,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1431,   22,    4,  810,    7,   49, 5255,  622,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54, 2503,  925,  879,    4,  128,  251,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   21, 1507, 1508,   58,   79,    4,   11,   19,    1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 1297,   14,  583, 8180,   22,  479, 1334,   41,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  52, 465, 363, 364,  79, 123,  14,   4, 523,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,   53,   54,   41,    4,   20, 1566,  785,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   75, 1065,   14,  479,  111,  366,  171, 2209,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  342,   14,  256, 6043, 1363,    7, 1307,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 131, 783, 691,   4, 134, 113, 491,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  995, 8821,   54, 1486,  427,   34,  526,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2525, 1652,   79, 5135, 4842,    4,  361,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,   22,    4,  851,  271,   14, 2016,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  995,  621,  169, 7440, 2623,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,   93,  132,  174,   22,  256,  111,  165, 1686,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  410,   53,   55,   41,    4, 2384,  568,  195,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,  995, 8821,   54, 1486,  427,   34,  526,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,   54, 2503,  925,  879,    4,  128,  251,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  173, 9093,    4,  282,   56,    4,  276,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 893,  79,   4, 906,  79,   4,  11,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  92, 141,   4, 464,  79,  34, 588,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,  141, 1633,   41,    4, 1087,   10,   42,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,    34,  1911,  3050,   132, 10113,     4,  1315,  5773,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   34,  380, 4002,  107,  125,  278,   41,   48,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4, 2549,  550,   22,  729,   14,   61, 2503,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   173,     7,   738,    93,  1023,    22, 14291,     7,   592,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,  34,  52, 296,  54, 363,   4, 529, 149,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 132,   4,  21,  15,  22,   4, 121,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   173,     7,   738,    93,  1023,    22, 14291,     7,   592,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  338,   14, 1369,   54,  174,   79,    4,  208,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,   79,    4,   29,  290,   79,  112, 2155,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,   173,     7,   738,    93,  1023,    22, 14291,     7,   592,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,    4,  100,  174,  299,  363, 5693,   79,    4,  904,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4, 995, 371,  66, 132, 228,  41,   4, 134,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,   52,   35, 1086, 1177,   81,    4,  219,  535,   19,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0, 147, 621, 169, 170, 334,  79,   4, 138, 208,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([    0,     4,   837,  2696, 22522,   299,     4,   891,    29,    19,\n",
      "            1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  255,  132,    4,   35,  116,   22, 1088,   37,  278,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([  0,   4,  93, 174, 372, 165,   4, 114, 478,  19,   1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0,  374,   53,  141, 1633,   41,    4, 1087,   10,   42,    1])\n",
      "INDEXED CAPTION PRETRAIN DS:  tensor([   0, 1771, 4571, 4572,  165, 2731,   49,  295,   47,   19,    1])\n",
      "Epoch [1/1], Step [8/8], Loss: 7.1373, Perplexity: 1258.0421"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "csv_out = \"pretraining_losses_\"\n",
    "\n",
    "speaker_losses=[]\n",
    "perplexities = []\n",
    "steps = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "                \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients (reset).\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        \n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.reshape(-1))\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), torch.exp(loss))\n",
    "        \n",
    "        speaker_losses.append(loss.item())\n",
    "        steps.append(i_step)\n",
    "        perplexities.append(torch.exp(loss).item())\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'func-decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'func-encoder-%d.pkl' % epoch))\n",
    "    \n",
    "    # save the training metrics\n",
    "    df_out = pd.DataFrame({\n",
    "        \"steps\": steps,\n",
    "        \"losses\": speaker_losses,\n",
    "        \"perplexities\": perplexities\n",
    "    })\n",
    "    df_out.to_csv(csv_out + \"epoch_\" + str(epoch) + \".csv\", index=False )\n",
    "    \n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eae7c1",
   "metadata": {},
   "source": [
    "### Reference game setup\n",
    "\n",
    "Here, the actually interesting steps are hapenning. Both agents, the speaker and the listener, are trained in the reference game set up. \n",
    "\n",
    "For this, the speaker is initialized with a pretrained model and then further fine-tuned. To do so, a functional training dataset object is created which samples image pairs rather than single image-caption pairs as for pretraining the speaker. A respective data_loader is defined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d66933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional training dataset\n",
    "\n",
    "class COCOCaptionsDataset_functional(COCOCaptionsDataset):\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Sample a training instance: a target image, a distractor image, the corresponding target image caption\n",
    "        and the index of the target in the pair of the input idx. \n",
    "\n",
    "        Arguments:\n",
    "        -------\n",
    "        idx: (int, int)\n",
    "            Index of the target and distractor items to be returned.\n",
    "            \n",
    "        Returns:\n",
    "        ------\n",
    "            target_image: torch.tensor(3,224,224) \n",
    "            distractor_image: torch.tensor(3,224,224)\n",
    "            target_caption: torch.tensor(caption_length) \n",
    "            target: int (0 or 1)\n",
    "                Index of the target within idx\n",
    "        \"\"\"\n",
    "        if self.mode != 'test':\n",
    "            # sample target index\n",
    "            target = np.random.choice([0,1])\n",
    "            target_idx = idx[target]\n",
    "            distractor_idx = idx[(1-target)]\n",
    "            \n",
    "            ann_id = self.ids[target_idx]\n",
    "            target_caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            target_path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # get distarctor\n",
    "            dist_id = self.ids[distractor_idx]\n",
    "            dist_img_id = self.coco.anns[dist_id]['image_id']\n",
    "            distractor_path = self.coco.loadImgs(dist_img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            target_image = Image.open(os.path.join(self.image_dir, target_path)).convert('RGB')\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "            distractor_image = Image.open(os.path.join(self.image_dir, distractor_path)).convert('RGB')\n",
    "            distractor_image = self.transform(distractor_image)\n",
    "\n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            tokens = tokenizer(str(target_caption).lower())\n",
    "            # Convert caption to tensor of word ids, append start and end tokens.\n",
    "            target_caption = []\n",
    "            target_caption.append(self.vocab(self.vocab.start_word))\n",
    "\n",
    "            # check if the sequence needs to be padded or truncated\n",
    "            if self.max_sequence_length != 0:\n",
    "                tokens = tokens[:self.max_sequence_length]\n",
    "\n",
    "            target_caption.extend([self.vocab(token) for token in tokens])\n",
    "            target_caption.append(self.vocab(self.vocab.end_word))\n",
    "            target_caption = torch.Tensor(target_caption).long()\n",
    "            return target_image, distractor_image, target_caption, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591700ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function wrapping a DataLoader around the functional dataset class\n",
    "def get_loader_functional(transform,\n",
    "               mode='val',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"START\",\n",
    "               end_word=\"END\",\n",
    "               unk_word=\"UNK\",\n",
    "               pad_word=\"PAD\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               download_dir=\"../../../data/val/\",\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----\n",
    "      transform: Image transform.\n",
    "      mode: One of 'train' or 'test'.\n",
    "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: Minimum word count threshold.\n",
    "      vocab_file: File containing the vocabulary. \n",
    "      start_word: Special word denoting sentence start.\n",
    "      end_word: Special word denoting sentence end.\n",
    "      unk_word: Special word denoting unknown words.\n",
    "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
    "                       If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: Number of subprocesses to use for data loading \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        data_loader: DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test', 'val'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train' or mode=='val', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'val':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json')\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"train2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") #'test2014/'\n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json') #image_info_test2014\n",
    "\n",
    "    # COCO caption dataset for the reference game setting\n",
    "    dataset = COCOCaptionsDataset_functional(\n",
    "        file=annotations_file,\n",
    "        download_dir = download_dir, \n",
    "        img_transform=transform,\n",
    "        batch_size=batch_size,\n",
    "        mode=mode,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_token=start_word,\n",
    "        end_token=end_word,\n",
    "        unk_token=unk_word,\n",
    "        pad_token=pad_word, \n",
    "        vocab_from_file=vocab_from_file,\n",
    "        max_sequence_length=15,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=torch.utils.data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False),\n",
    "                                                 )\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                           \n",
    "           batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                                 )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f5efb",
   "metadata": {},
   "source": [
    "Below, all the training parameters are instantiated, the pretrained speaker model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b1129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "vocab_threshold = 1        # minimum word count threshold\n",
    "listener_embed_size = 512\n",
    "\n",
    "# NOTE: embed_size for speaker is 1024, different from listener\n",
    "speaker_embed_size = 1024\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 1          # determines window for printing average loss\n",
    "log_file = 'reference_game_log_train.txt'       # name of file with saved training loss and perplexity\n",
    "lambda_s = 0.1 # weight of the structural loss from the original paper\n",
    "\n",
    "# define a data loader returning two images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader_pairs = get_loader_functional(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                         download_dir=\"../../../data/train\", \n",
    "                        )\n",
    "vocab_size = len(data_loader_pairs.dataset.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae23584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps:  70000\n"
     ]
    }
   ],
   "source": [
    "speaker_encoder = EncoderCNN(speaker_embed_size)\n",
    "speaker_decoder = DecoderRNN(speaker_embed_size, hidden_size, vocab_size)\n",
    "speaker_encoder.load_state_dict(torch.load(os.path.join('./models', 'func-encoder-1.pkl')))\n",
    "speaker_decoder.load_state_dict(torch.load(os.path.join('./models', 'func-decoder-1.pkl')))\n",
    "\n",
    "\n",
    "listener_encoder = ListenerEncoderCNN(listener_embed_size)\n",
    "listener_rnn = ListenerEncoderRNN(listener_embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "speaker_encoder.to(device)\n",
    "speaker_encoder.eval()\n",
    "speaker_decoder.to(device)\n",
    "listener_encoder.to(device)\n",
    "listener_rnn.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "# different sets of params are defined because they are optimized with different losses\n",
    "speaker_params = list(speaker_decoder.embed.parameters()) + list(speaker_decoder.lstm.parameters()) + list(speaker_decoder.linear.parameters()) + list(speaker_encoder.embed.parameters()) \n",
    "listener_params = list(listener_rnn.lstm.parameters()) + list(listener_encoder.embed.parameters()) # + list(listener_rnn.linear.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "speaker_optimizer = torch.optim.Adam(speaker_params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "listener_optimizer = torch.optim.Adam(listener_params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "total_step = math.ceil(len(data_loader_pairs.dataset.caption_lengths) / data_loader_pairs.batch_sampler.batch_size)\n",
    "print(\"Total steps: \", total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09911d2e",
   "metadata": {},
   "source": [
    "Below, the model is trained. The conceptual steps are:\n",
    "* sample (target, distractor, target_index) tuples\n",
    "* pass target images to the speaker which samples corresponding captions\n",
    "* pass the produced captions to the listener LSTM\n",
    "* pass both images and corresponding captions through the listener CNN\n",
    "* compute speaker loss consisting of the structural (cross entropy) and the functional (REINFORCE) loss\n",
    "* compute listener loss consisting of the cross entropy loss (predicted vs target image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c275b9",
   "metadata": {},
   "source": [
    "**<-- Please check me below -->**\n",
    "\n",
    "Really not sure if the training loop is technically correct for computing all the loss components and training the model as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b66ae00e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_s:  tensor(7.3210, grad_fn=<NllLossBackward0>)  L_f:  tensor([[-0.3694]], grad_fn=<MeanBackward1>)\n",
      "Epoch [1/1], Step [1/70000], Speaker loss: 0.3627, Listener loss: 0.7641, Perplexity: 1.4372\n",
      "L_s:  tensor(7.2530, grad_fn=<NllLossBackward0>)  L_f:  tensor([[0.3237]], grad_fn=<MeanBackward1>)\n",
      "Epoch [1/1], Step [2/70000], Speaker loss: 1.0490, Listener loss: 0.5791, Perplexity: 2.8547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6ec80b13a2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistener_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistener_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m# retrieve the index of the larger dot product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mpredicted_max_dots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3794fe043f96>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images1, images2, caption)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# will be improved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mfeatures1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mfeatures2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# reshape features to shape (batch_size, -1) - adapt to first dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mfeatures1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "csv_out = \"functional_training_losses_\"\n",
    "\n",
    "speaker_losses_structural = []\n",
    "speaker_losses_functional = []\n",
    "listener_losses = []\n",
    "perplexities = []\n",
    "steps = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices_pairs = data_loader_pairs.dataset.get_func_train_indices()\n",
    "        \n",
    "        # Create and assign a batch sampler to retrieve a target batch with the sampled indices.\n",
    "        new_sampler_pairs = torch.utils.data.sampler.SubsetRandomSampler(indices=indices_pairs)\n",
    "        \n",
    "        data_loader_pairs.batch_sampler.sampler = new_sampler_pairs\n",
    "        # Obtain the target batch.\n",
    "        images1, images2, captions, targets = next(iter(data_loader_pairs))\n",
    "        \n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images1 = images1.to(device)\n",
    "        images2 = images2.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Zero the gradients (reset).\n",
    "        speaker_encoder.zero_grad()\n",
    "        speaker_decoder.zero_grad()\n",
    "        listener_encoder.zero_grad()\n",
    "        listener_rnn.zero_grad()\n",
    "        \n",
    "        ###### Pass the targets through the speaker model.\n",
    "        \n",
    "        # sample caption from speaker \n",
    "        # zip images and target indices such that we can input correct image into speaker\n",
    "        train_pairs = list(zip(zip(images1, images2), targets))\n",
    "        target_images = [im[j] for im, j in train_pairs]\n",
    "        preds_out = []\n",
    "        log_probs_batch = []\n",
    "        speaker_features_batch = []\n",
    "        speaker_raw_output = []\n",
    "        # get predicted captions for each image in the batch (to be made more efficient)\n",
    "        for i, im in enumerate(target_images): \n",
    "            speaker_features = speaker_encoder(im.unsqueeze(0))\n",
    "            speaker_features_batch.append(speaker_features)\n",
    "            # get predicted caption and its log probability\n",
    "            captions_pred, log_probs, raw_outputs = speaker_decoder.sample(speaker_features.unsqueeze(1), max_sequence_length=captions[i].shape[0])\n",
    "            \n",
    "            preds_out.append(captions_pred)\n",
    "            log_probs_batch.append(log_probs)\n",
    "            speaker_raw_output.extend(raw_outputs)\n",
    "            \n",
    "        # transform predicted word indices to tensor\n",
    "        preds_out_tensor = [torch.stack(x) for x in preds_out]\n",
    "        preds_out = torch.stack(preds_out_tensor).squeeze(-1)    \n",
    "                \n",
    "        #######\n",
    "        # pass images and generated message form speaker through listener\n",
    "        hiddens = listener_rnn(preds_out)\n",
    "        \n",
    "        predictions = listener_encoder(images1, images2, hiddens) \n",
    "        # retrieve the index of the larger dot product\n",
    "        predicted_max_dots, predicted_inds = torch.max(predictions, dim = 1)\n",
    "\n",
    "        ######\n",
    "        # RL step\n",
    "        log_probs = torch.stack(log_probs_batch)\n",
    "        # if target index and output index match, 1, else -1\n",
    "        rewards = [1 if i == j else -1 for i, j in list(zip(predicted_inds, targets))]\n",
    "        # compute REINFORCE update\n",
    "        rl_grads = update_policy(rewards,  log_probs)\n",
    "        \n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        \n",
    "        # REINFORCE for functional part, applied to speaker LSTM weights (maybe also Linear ones)\n",
    "        # cross entropy for Listener\n",
    "        # and also cross entropy for Speaker params, optimizing against target caption of the target image\n",
    "        # (last implemented just like for pretraining), this is the structural loss component\n",
    "        \n",
    "        # combine structural loss and functional loss for the speaker\n",
    "        loss_structural = criterion(torch.stack(speaker_raw_output).contiguous().view(-1, vocab_size), captions.reshape(-1)) \n",
    "        speaker_loss =  lambda_s*loss_structural + rl_grads\n",
    "        \n",
    "        print(\"L_s: \", loss_structural, \" L_f: \", rl_grads)\n",
    "        \n",
    "        # listener loss\n",
    "        listener_loss = criterion(predictions, targets)\n",
    "        \n",
    "        \n",
    "        # Backward pass.\n",
    "        speaker_loss.backward(retain_graph=True)\n",
    "        listener_loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Update the parameters in the respective optimizer.\n",
    "        speaker_optimizer.step()\n",
    "        listener_optimizer.step()\n",
    "        \n",
    "        # Get training statistics.\n",
    "        # perplexity computation questionable\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Speaker loss: %.4f, Listener loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, speaker_loss.item(), listener_loss.item(), torch.exp(speaker_loss))\n",
    "        \n",
    "        speaker_losses_structural.append(loss_structural.item())\n",
    "        speaker_losses_functional.append(rl_grads.item())\n",
    "        listener_losses.append(listener_loss.item())\n",
    "        perplexities.append(torch.exp(speaker_loss).item())\n",
    "        steps.append(i_step)\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "        \n",
    "    # save the training metrics\n",
    "    df_out = pd.DataFrame({\n",
    "        \"steps\": steps,\n",
    "        \"speaker_s\": speaker_losses_structural,\n",
    "        \"speaker_f\": speaker_losses_functional,\n",
    "        \"listener\": listener_losses,\n",
    "        \"perplexities\": perplexities\n",
    "    })\n",
    "    df_out.to_csv(csv_out + \"epoch_\" + str(epoch) + \".csv\", index=False )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1e4affa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, speaker_losses_structural, label=\"Speaker structural\")\n",
    "plt.plot(steps, speaker_losses_functional, label=\"Speaker functional\")\n",
    "plt.plot(steps, listener_losses, label=\"Listener\")\n",
    "plt.plot(steps, perplexities, label=\"Perplexity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166982c5",
   "metadata": {},
   "source": [
    "### Testing the pretrained sepaker\n",
    "\n",
    "Below, some initial code for testing the pretrained speaker (basic image captioning model) is provided. Ignore for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "719a2729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "# transforms like in train\n",
    "transform_test = transforms.Compose([transforms.Resize((224, 224)), \\\n",
    "                                     transforms.ToTensor(), \\\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), \\\n",
    "                                                          (0.229, 0.224, 0.225))])\n",
    "data_loader_test = get_loader(transform=transform_test,\n",
    "                         mode='val',\n",
    "                         batch_size=1,\n",
    "                         vocab_threshold=11,\n",
    "                         vocab_from_file=True,\n",
    "                         download_dir=\"../../../data/val\", \n",
    "#                          vocab_file='../../../data/vocab.pkl'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c296557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize sample image, before pre-processing.\n",
    "orig_image, image = next(iter(data_loader_test))\n",
    "\n",
    "plt.imshow(orig_image.squeeze(0).permute(1,2,0))\n",
    "plt.title('example image')\n",
    "plt.show()\n",
    "# orig_image.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db3f2641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d6a148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB:  6039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (lstm): LSTM(1024, 512, batch_first=True)\n",
       "  (embed): Embedding(6039, 1024)\n",
       "  (linear): Linear(in_features=512, out_features=6039, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_sentence(output):\n",
    "    \"\"\"\n",
    "    Transform list of indices to a sentence.\n",
    "    \"\"\"\n",
    "    list_string = []\n",
    "    \n",
    "    for idx in output:\n",
    "        list_string.append(data_loader_test.dataset.vocab.idx2word[idx.item()])\n",
    "    \n",
    "    list_string = list_string[1:-1] # Discard <start> and <end> words\n",
    "    sentence = ' '.join(list_string) # Convert list of string to full string\n",
    "    sentence = sentence.capitalize()  # Capitalize the first letter of the first word\n",
    "    return sentence\n",
    "\n",
    "\n",
    "encoder_file = 'encoder-earlystoppiing-4.pkl' \n",
    "decoder_file = 'decoder-earlystopping-4.pkl'\n",
    "\n",
    "embed_size = 512\n",
    "visual_embed_size = 1024\n",
    "hidden_size = 512\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader_test.dataset.vocab)\n",
    "print(\"VOCAB: \", vocab_size)\n",
    "# Initialize the encoder and decoder, and set each to inference mode.\n",
    "encoder = EncoderCNN(visual_embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(visual_embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "\n",
    "# Load the trained weights.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./../models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./../models', decoder_file)))\n",
    "\n",
    "# Move models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18547c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImGAE torch.Size([1, 15])\n",
      "example output: [tensor([0]), tensor([4]), tensor([805]), tensor([14]), tensor([110]), tensor([22]), tensor([4]), tensor([217]), tensor([14]), tensor([279]), tensor([19]), tensor([1]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([3])]\n",
      "example sentence:\n",
      " A plate of food with a glass of water . end pad pad pad pad pad pad pad pad pad pad pad pad pad pad\n"
     ]
    }
   ],
   "source": [
    "image = image.to(device)\n",
    "print(\"ImGAE\", image.shape)\n",
    "# Obtain the embedded image features.\n",
    "features = encoder(orig_image).unsqueeze(1)\n",
    "\n",
    "# Pass the embedded image features through the model to get a predicted caption.\n",
    "output, log_p, raw = decoder.sample(features, 27)\n",
    "print('example output:', output)\n",
    "sentence = clean_sentence(output)\n",
    "print('example sentence:\\n', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e812213",
   "metadata": {},
   "source": [
    "Ideally, the pretrained speaker model should be evaluated in order to estimate the quality of the base performance, since it is critical for any kinds of changes that could follow from functional training. The evaluation should contain some standard methods like BLEU, and maybe one or two language drift explorations. Some examples should be inspected manually, too.  \n",
    "In case the performance is bad, the following aspects could be tweaked: train longer (larger early stopping delta), different NN weight initialization, more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a61475",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "The code was created with the help of [this](https://medium.com/@deepeshrishu09/automatic-image-captioning-with-pytorch-cf576c98d319) and [this](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38294f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
