{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9169dd8",
   "metadata": {},
   "source": [
    "## Evaluating the pretrained speaker: MS COCO Captions evaluation\n",
    "\n",
    "The following notebook is to set up / re-use public code for image caption evaluation following the pipeline suggested in the original paper on MS COCO Captions. Additionally, given the specifics of the downstream task (namely, sampling from the model and minimizing the CCE loss against that), a baseline for the validation PPL on sampled captions is computed as well.  \n",
    "\n",
    "Due to installation difficulties, and because it was not a part of the original paper, the SPICE score computation is commented out in the cloned code and not performed in the present evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d0b6d",
   "metadata": {},
   "source": [
    "#### Utils\n",
    "In order to compute the standard image caption evaluation metrics, the code provided in [this](https://github.com/daqingliu/coco-caption) repo is used. Since it requires the results to be formatted in a specific syntax, script below performs some utility mapping of validation annotation IDs to  validation image IDs. \n",
    "\n",
    "--> i need to produce {'image_id': XXX, 'caption': 'lower cased string'} items when validating the model. When iterating over items with my data loader, I get annotation IDs. So i need to map ann IDs to img IDs. I can do that via th COCO.loadAnns(annIds) and then retrieve 'image_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d53259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducing the desired results format\n",
    "import json\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da86c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264048\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "264048\n"
     ]
    }
   ],
   "source": [
    "# just creating a file for my entire val split\n",
    "val_ids = torch.load(\"val_split_IDs_from_COCO_train.pt\")\n",
    "print(len(val_ids))\n",
    "coco = COCO(\"../../../data/train/annotations/captions_train2014.json\")\n",
    "val_imgIDs = [coco.loadAnns(i)[0]['image_id'] for i in val_ids]\n",
    "print(len(val_imgIDs))\n",
    "# torch.save(val_imgIDs, \"val_split_imgIDs_from_COCO_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c1dc8",
   "metadata": {},
   "source": [
    "A further restriction is that there must be only one produced caption per image so the evaluation happens on\n",
    "one annID per unique image only -- the metrics are computed relative to all 5 ground truth captions anyways, but they are retrieved within the pipeline. Below, the respective annotation ID and image ID files are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3652b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i also need to create a new test split which only contains one annotation per unique image\n",
    "# get images i know werent used for pretraining\n",
    "with open(\"imgID2annID.json\", \"r\") as fp:\n",
    "    f = json.load(fp)\n",
    "imgIDs4val = list(f.keys())[30000:]\n",
    "\n",
    "ann4val_unqIm = [f[i][0] for i in imgIDs4val]\n",
    "# torch.save(torch.tensor(ann4val_unqIm), \"val_split_annIDs_singular_from_COCO_train_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6231f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52783\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "52783\n"
     ]
    }
   ],
   "source": [
    "# load unique data\n",
    "val_ids = torch.load(\"val_split_annIDs_singular_from_COCO_train_tensor.pt\").tolist()\n",
    "print(len(val_ids))\n",
    "coco = COCO(\"../../../data/train/annotations/captions_train2014.json\")\n",
    "val_imgIDs = [coco.loadAnns(i)[0]['image_id'] for i in val_ids]\n",
    "print(len(val_imgIDs))\n",
    "# torch.save(val_imgIDs, \"val_split_imgIDs_singular_from_COCO_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437cbad",
   "metadata": {},
   "source": [
    "#### Evaluation function\n",
    "The wrapper below takes in a trained model and performs the evaluation on a set number of validation images (e.g., a set including the images used in the reference game, or images used neither in pretraining nor experiments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3f4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import agent modules from the actual repo\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902cd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.speaker import DecoderRNN\n",
    "from utils.build_dataset import get_loader\n",
    "from reference_game_utils.update_policy import clean_sentence\n",
    "from utils.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813a360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_caption.pycocotools.coco import COCO \n",
    "from coco_caption.pycocoevalcap.eval import COCOEvalCap # TODO in readme add point about renaming!\n",
    "import skimage.io as io\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc414267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speaker(\n",
    "    model_path: str,\n",
    "    num_val_imgs: int,\n",
    "    res_path: str,\n",
    "    val_ppl_path: str,\n",
    "    metrics_res_path: str,\n",
    "    vocab_file: str,\n",
    "    download_dir: str,\n",
    "    val_file: str, \n",
    "    val_imgIDs_file: str,\n",
    "    vocab_threshold: int = 25,\n",
    "    batch_size: int = 1,\n",
    "    embed_size: int = 512,\n",
    "    visual_embed_size: int = 512,\n",
    "    hidden_size: int = 512,\n",
    "    decoding_strategy: str = \"greedy\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a pretrained speaker (image captioner) model for MS COCO by computing: \n",
    "    1) validation loss + perplexity given a paricular decoding strategy\n",
    "    2) image captioning evaluation metrics from MS COCO Captions:\n",
    "        BLEU, ROUGE, METEOR and CIDEr.\n",
    "    All results including produced captions are saved.\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    model_path: str\n",
    "        Path to speaker model weights.\n",
    "    num_val_imgs: int\n",
    "        Number of validation images to be used.\n",
    "    res_path: str\n",
    "        Path and name of file where produced captions will be saved.\n",
    "    val_ppl_path: str\n",
    "        Path and name of file where batch-wise validation loss and PPL will be written to.\n",
    "    metrics_res_path: str\n",
    "        Path and name of file where COCO Captions metrics will be written to.\n",
    "    vocab_file: str\n",
    "        Path to vocab file.\n",
    "    download_dir: str\n",
    "        Directory with annotations.\n",
    "    val_file: str\n",
    "        Path to file holding UNIQUE per image annotation IDs from validation set.\n",
    "    val_imgIDs_file: str\n",
    "        Path to image IDs corresponding to the validation annotation IDs above.\n",
    "    vocab_threshold: int = 25\n",
    "        Minimal token count used in vocabulary construction.\n",
    "    batch_size: int = 1\n",
    "        Must be 1.\n",
    "    embed_size: int = 512\n",
    "        Dimensionality of embeddings. Must correspond to pretraining settings.\n",
    "    visual_embed_size: int = 512\n",
    "        Dimensionality of image vectors. Must correspond to pretraining settings.\n",
    "    hidden_size: int = 512\n",
    "        Dimensionality of the hidden layer. Must correspond to pretraining settings.\n",
    "    decoding_strategy: str = \"greedy\"\n",
    "        Decoding strategy to be used in sampling. \n",
    "        Available options: \"greedy\", \"exp\", \"pure\", \"topk_temperature\", \"encoding\".\n",
    "        If \"encode\" is used, ground truth captions are just passed through the LSTM in training mode,\n",
    "        no decoding is taking place.\n",
    "    \"\"\"\n",
    "    assert batch_size == 1, \"Only batch_size=1 evaluations are supported!\"\n",
    "    \n",
    "    # data loader\n",
    "    transform_test = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                         transforms.ToTensor(), \\\n",
    "                                         transforms.Normalize((0.485, 0.456, 0.406), \\\n",
    "                                                          (0.229, 0.224, 0.225))])\n",
    "    data_loader_test = get_loader(transform=transform_test,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=True,\n",
    "                         download_dir=download_dir, \n",
    "                         vocab_file=vocab_file,\n",
    "                         dataset_path=val_file, \n",
    "                         num_imgs=num_val_imgs,\n",
    "                         embedded_imgs=torch.load(\"../train_logs/COCO_train_ResNet_features_reshaped_dict.pt\"),\n",
    "                        )\n",
    "    # add img IDs\n",
    "    data_loader_test.dataset._img_ids_flat = torch.load(val_imgIDs_file)[:num_val_imgs]\n",
    "    val_imgIDs = torch.load(val_imgIDs_file)[:num_val_imgs]\n",
    "    \n",
    "    vocab_size = len(data_loader_test.dataset.vocab)\n",
    "    # load model\n",
    "    decoder = DecoderRNN(\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        visual_embed_size,\n",
    "    )\n",
    "    decoder.load_state_dict(torch.load(model_path))\n",
    "    decoder.eval()\n",
    "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    \n",
    "    # instantiate results \n",
    "    results = []\n",
    "    val_running_loss = 0.0\n",
    "    val_running_ppl = 0.0\n",
    "    losses_list = []\n",
    "    ppl_list = []\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    \n",
    "    num_steps = math.ceil(len(data_loader_test.dataset)/batch_size)\n",
    "    \n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        counter += 1\n",
    "        # manually construct indices to avoid duplications bc of length of examples or random repetitions\n",
    "        indices = [(i, 0)]\n",
    "        \n",
    "        new_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader_test.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        targets, distractors, target_features, distractor_features, target_captions, distractor_captions = next(iter(data_loader_test)) \n",
    "        print(\"target caption \", clean_sentence(target_captions, data_loader_test))\n",
    "        both_images = torch.cat((target_features.unsqueeze(1), distractor_features.unsqueeze(1)), dim=1)\n",
    "        # retrieve image IDs\n",
    "        batch_img_ids = [val_imgIDs[i[0]] for i in indices]\n",
    "        print(\"Image id \", batch_img_ids)\n",
    "        \n",
    "        max_seq_len = target_captions.shape[1]-1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get prediction\n",
    "            if decoding_strategy == \"encoding\":\n",
    "                outputs, _ = decoder(both_images, target_captions, hidden)\n",
    "                norm_outputs = softmax(outputs)\n",
    "                _, captions_pred = torch.max(norm_outputs, dim = -1)\n",
    "                \n",
    "            else: \n",
    "                captions_pred, log_probs, outputs, entropies = decoder.sample(\n",
    "                    both_images, \n",
    "                    max_sequence_length=max_seq_len, \n",
    "                    decoding_strategy=decoding_strategy\n",
    "                )\n",
    "            # transform to natural language\n",
    "            nl_captions_pred = clean_sentence(captions_pred, data_loader_test)\n",
    "            print(\"NL captions pred \", nl_captions_pred)\n",
    "            # append to results list together with img ID\n",
    "            for i, c in list(zip(batch_img_ids, nl_captions_pred)):\n",
    "                \n",
    "                if \"end\" in c.split(\" \"):\n",
    "                    len_c = sum([1 for x in c.split(\" \")[:c.split(\" \").index(\"end\")] if x != \"end\" ])\n",
    "                else:\n",
    "                    len_c = len(c.split(\" \"))            \n",
    "                \n",
    "                results.append({\"image_id\": i, \"caption\": \" \".join(c.split()[:len_c])})\n",
    "                \n",
    "            # compute val PPL\n",
    "            loss = criterion(outputs.transpose(1,2), target_captions[:, 1:]) \n",
    "            losses_list.append(loss.item())\n",
    "            ppl = np.exp(loss.item())\n",
    "            ppl_list.append(ppl)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "            val_running_ppl += ppl\n",
    "    \n",
    "    print(\"Final average loss: \", val_running_loss / counter)\n",
    "    print(\"Final average PPL: \", val_running_ppl / counter)\n",
    "        \n",
    "    # check if results dir exists\n",
    "    os.makedirs(\"../../../data/speaker_eval_results/\", exist_ok=True)\n",
    "    \n",
    "    # write out results file\n",
    "    with open(res_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    # write out validation PPLs\n",
    "    df_out = pd.DataFrame({\n",
    "        \"loss\": losses_list,\n",
    "        \"PPL\": ppl_list,\n",
    "    })\n",
    "    df_out.to_csv(val_ppl_path)\n",
    "    \n",
    "    # now compute the evaluations, as proposed in the notebook from the repo referenced above \n",
    "    cocoRes = data_loader_test.dataset.coco.loadRes(res_path)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(data_loader_test.dataset.coco, cocoRes)\n",
    "\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()\n",
    "    coco_metrics = cocoEval.eval.items()\n",
    "    # construct out file \n",
    "    metrics_df = pd.DataFrame(cocoEval.eval, index = [0]).round(3)\n",
    "    metrics_df.to_csv(\n",
    "        metrics_res_path\n",
    "    )\n",
    "    print(\"Final coco metrics: \", coco_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacc49e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 37150.61it/s]\n",
      "100%|██████████| 3700/3700 [00:00<00:00, 100936.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target caption  ['Start a train is coming up on another train next to it . end']\n",
      "Image id  [341245]\n",
      "target caption  ['Start a passenger train moves along rails next to apartment buildings . end']\n",
      "Image id  [521150]\n",
      "target caption  ['Start this train was photographed from an aerial view . end']\n",
      "Image id  [568955]\n",
      "target caption  ['Start a train with a brown engine is on a train track . end']\n",
      "Image id  [393602]\n",
      "target caption  ['Start trains passing on tracks near city area on cloudy day . end']\n",
      "Image id  [576543]\n",
      "target caption  ['Start check the phone to see if unk has called . end']\n",
      "Image id  [92639]\n",
      "target caption  ['Start intersection street signs displaying plaza drive and unk avenue . end']\n",
      "Image id  [521132]\n",
      "target caption  ['Start a street sign on the edge of a curb has a sign with a monkey end']\n",
      "Image id  [573988]\n",
      "target caption  ['Start a road sign hangs from an old rusted pole . end']\n",
      "Image id  [86831]\n",
      "target caption  ['Start a traffic light next to a street sign that says unk . end']\n",
      "Image id  [127575]\n",
      "Final average loss:  8.351852226257325\n",
      "Final average PPL:  4243.300323686942\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "Img Ids in BLEU:  [86831, 92639, 127575, 341245, 393602, 521132, 521150, 568955, 573988, 576543]\n",
      "Hypo  ['engine engine pin pin pin pin pin pin pin pin pin']\n",
      "Ref  ['a road sign hangs from an old rusted pole', 'a sign of a street name taken crocked', 'wooden street sign with arrow finial that identifies cecilia st.', 'a blue street sign that says cecilia st.', 'a street sign indicating cecilia st. or 800 s']\n",
      "Hypo  ['engine alcohol alcohol alcohol alcohol alcohol alcohol headphones alcohol headphones alcohol']\n",
      "Ref  ['check the phone to see if anyone has called', 'a man with a hat sitting with a cell phone', 'a man wearing a hat sitting down looking at his cell phone', 'man in a black suit and hat looking happy while checking his cell phone', 'a guy wearing glasses and all black holding a white device']\n",
      "Hypo  ['engine screens alcohol alcohol pin pin pin pin pin pin pin pin pin']\n",
      "Ref  ['a traffic light next to a street sign that says hollywood', 'a streetsign and stop light with an advertisement on a building neaby', \"there 's a hollywood street sign on a traffic light\", 'a street light and road sign that says hollywood', 'the blue sign next to the stone and glass building says hollywood']\n",
      "Hypo  ['engine screens alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol']\n",
      "Ref  ['a train is coming up on another train next to it', 'a train engine with steam coming from it on a track', 'a green and red train with a number six on it', 'two green trains with numbers on them pouring out steam', 'two old fashioned steam railroad trains are on the tracks']\n",
      "Hypo  ['engine alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol']\n",
      "Ref  ['a train with a brown engine is on a train track', 'a brown and yellow train engine pulling train cars down the track', 'a railroad track coming down a set of tracks', 'a train is on a track near the grass', 'a red and yellow striped train is on the tracks']\n",
      "Hypo  ['reach screens alcohol alcohol pin dark pin pin pin dark pin']\n",
      "Ref  ['intersection street signs displaying plaza drive and coe avenue', 'a sign that reads plaza drive is being displayed', 'a street sign sits high and names the street', 'a street sign indicates where plaza drive is located', 'a black and white street sign against a blue sky']\n",
      "Hypo  ['reach alcohol alcohol alcohol pin pin pin pin pin pin pin pin']\n",
      "Ref  ['a passenger train moves along rails next to apartment buildings', 'a train on a track under a blue sky with some buildings', 'a train on the tracks in what seems to be a deserted area of town', 'a gray train some buildings and some train tracks', 'a train going along a track near apartments']\n",
      "Hypo  ['engine alcohol alcohol alcohol alcohol pin pin pin pin pin']\n",
      "Ref  ['this train was photographed from an aerial view', 'a train is riding on tracks in front of a building', 'looking down on a train that is on the tracks', 'looking down at a commuter train passing a building', 'a long train travels on the tracks in the countryside']\n",
      "Hypo  ['engine alcohol alcohol alcohol pin pin pin pin pin pin pin pin pin pin pin pin']\n",
      "Ref  ['a street sign on the edge of a curb has a sign with a monkey in spanish a handicap sign and a parking sign', 'a sign sitting on the side of a road on top of a sidewalk', 'there is a street pole with many different signs on it', 'a set of parking signs with a monkey sign below it', 'various street signs on brick sidewalk next to street']\n",
      "Hypo  ['engine alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol alcohol']\n",
      "Ref  ['trains passing on tracks near city area on cloudy day', 'two trains are going in opposite directions on parallel tracks', 'maroon train setting on tracks next to other train', 'a couple of trains traveling past each other on tracks', 'a train pulls up to another train on the tracks']\n",
      "{'testlen': 122, 'reflen': 112, 'guess': [122, 112, 102, 92], 'correct': [2, 0, 0, 0]}\n",
      "ratio:1.089286\n",
      "Bleu_1: 0.016\n",
      "Bleu_2: 0.000\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.009\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.017\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.003\n",
      "Final coco metrics:  dict_items([('Bleu_1', 0.016393442622816452), ('Bleu_2', 3.825833548609991e-10), ('Bleu_3', 1.1279347846691441e-12), ('Bleu_4', 6.284432064237135e-14), ('METEOR', 0.009090909090909092), ('ROUGE_L', 0.01692094313453537), ('CIDEr', 0.0033146595486712796)])\n"
     ]
    }
   ],
   "source": [
    "# evaluate scheduled sampling speaker\n",
    "evaluate_speaker(\n",
    "    model_path=\"../models/decoder-coco-512dim-scheduled_sampling_wGreedyDecoding_k150-1.pkl\",\n",
    "    num_val_imgs=10,\n",
    "    res_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_results.json\",\n",
    "    val_ppl_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_val.json\",\n",
    "    metrics_res_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_COCO_metrics.csv\",\n",
    "    vocab_file=\"vocab4000.pkl\",\n",
    "    download_dir=\"../../../data/train\",\n",
    "    val_file=\"val_split_annIDs_singular_from_COCO_train_tensor.pt\", \n",
    "    val_imgIDs_file=\"val_split_imgIDs_singular_from_COCO_train.pt\",\n",
    "    batch_size=1,\n",
    "    decoding_strategy=\"encoding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea63c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 28244.47it/s]\n",
      "100%|██████████| 3700/3700 [00:00<00:00, 112677.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target caption  ['Start a train is coming up on another train next to it . end']\n",
      "Image id  [341245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL captions pred  ['A train traveling down train tracks . end . end . end .']\n",
      "target caption  ['Start a passenger train moves along rails next to apartment buildings . end']\n",
      "Image id  [521150]\n",
      "NL captions pred  ['A train traveling down tracks in a city . end end .']\n",
      "target caption  ['Start this train was photographed from an aerial view . end']\n",
      "Image id  [568955]\n",
      "NL captions pred  ['A train traveling down the train tracks . end .']\n",
      "target caption  ['Start a train with a brown engine is on a train track . end']\n",
      "Image id  [393602]\n",
      "NL captions pred  ['A train is coming down the tracks in the middle of a train']\n",
      "target caption  ['Start trains passing on tracks near city area on cloudy day . end']\n",
      "Image id  [576543]\n",
      "NL captions pred  ['A train is coming down the tracks in a train station .']\n",
      "target caption  ['Start check the phone to see if unk has called . end']\n",
      "Image id  [92639]\n",
      "NL captions pred  ['A man in a suit and tie sitting on a chair']\n",
      "target caption  ['Start intersection street signs displaying plaza drive and unk avenue . end']\n",
      "Image id  [521132]\n",
      "NL captions pred  ['A street sign with a unk sign and street sign .']\n",
      "target caption  ['Start a street sign on the edge of a curb has a sign with a monkey end']\n",
      "Image id  [573988]\n",
      "NL captions pred  ['A sign that is on the side of the street . end end end end .']\n",
      "target caption  ['Start a road sign hangs from an old rusted pole . end']\n",
      "Image id  [86831]\n",
      "NL captions pred  ['A street sign with a unk unk unk unk unk unk']\n",
      "target caption  ['Start a traffic light next to a street sign that says unk . end']\n",
      "Image id  [127575]\n",
      "NL captions pred  ['A street sign with broadway and unk unk unk unk unk unk unk']\n",
      "Final average loss:  7.62620906829834\n",
      "Final average PPL:  35546.62524029962\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "Img Ids in BLEU:  [86831, 92639, 127575, 341245, 393602, 521132, 521150, 568955, 573988, 576543]\n",
      "Hypo  ['a street sign with a unk unk unk unk unk unk']\n",
      "Ref  ['a road sign hangs from an old rusted pole', 'a sign of a street name taken crocked', 'wooden street sign with arrow finial that identifies cecilia st.', 'a blue street sign that says cecilia st.', 'a street sign indicating cecilia st. or 800 s']\n",
      "Hypo  ['a man in a suit and tie sitting on a chair']\n",
      "Ref  ['check the phone to see if anyone has called', 'a man with a hat sitting with a cell phone', 'a man wearing a hat sitting down looking at his cell phone', 'man in a black suit and hat looking happy while checking his cell phone', 'a guy wearing glasses and all black holding a white device']\n",
      "Hypo  ['a street sign with broadway and unk unk unk unk unk unk unk']\n",
      "Ref  ['a traffic light next to a street sign that says hollywood', 'a streetsign and stop light with an advertisement on a building neaby', \"there 's a hollywood street sign on a traffic light\", 'a street light and road sign that says hollywood', 'the blue sign next to the stone and glass building says hollywood']\n",
      "Hypo  ['a train traveling down train tracks']\n",
      "Ref  ['a train is coming up on another train next to it', 'a train engine with steam coming from it on a track', 'a green and red train with a number six on it', 'two green trains with numbers on them pouring out steam', 'two old fashioned steam railroad trains are on the tracks']\n",
      "Hypo  ['a train is coming down the tracks in the middle of a train']\n",
      "Ref  ['a train with a brown engine is on a train track', 'a brown and yellow train engine pulling train cars down the track', 'a railroad track coming down a set of tracks', 'a train is on a track near the grass', 'a red and yellow striped train is on the tracks']\n",
      "Hypo  ['a street sign with a unk sign and street sign']\n",
      "Ref  ['intersection street signs displaying plaza drive and coe avenue', 'a sign that reads plaza drive is being displayed', 'a street sign sits high and names the street', 'a street sign indicates where plaza drive is located', 'a black and white street sign against a blue sky']\n",
      "Hypo  ['a train traveling down tracks in a city']\n",
      "Ref  ['a passenger train moves along rails next to apartment buildings', 'a train on a track under a blue sky with some buildings', 'a train on the tracks in what seems to be a deserted area of town', 'a gray train some buildings and some train tracks', 'a train going along a track near apartments']\n",
      "Hypo  ['a train traveling down the train tracks']\n",
      "Ref  ['this train was photographed from an aerial view', 'a train is riding on tracks in front of a building', 'looking down on a train that is on the tracks', 'looking down at a commuter train passing a building', 'a long train travels on the tracks in the countryside']\n",
      "Hypo  ['a sign that is on the side of the street']\n",
      "Ref  ['a street sign on the edge of a curb has a sign with a monkey in spanish a handicap sign and a parking sign', 'a sign sitting on the side of a road on top of a sidewalk', 'there is a street pole with many different signs on it', 'a set of parking signs with a monkey sign below it', 'various street signs on brick sidewalk next to street']\n",
      "Hypo  ['a train is coming down the tracks in a train station']\n",
      "Ref  ['trains passing on tracks near city area on cloudy day', 'two trains are going in opposite directions on parallel tracks', 'maroon train setting on tracks next to other train', 'a couple of trains traveling past each other on tracks', 'a train pulls up to another train on the tracks']\n",
      "{'testlen': 100, 'reflen': 100, 'guess': [100, 90, 80, 70], 'correct': [62, 27, 8, 1]}\n",
      "ratio:1.000000\n",
      "Bleu_1: 0.620\n",
      "Bleu_2: 0.431\n",
      "Bleu_3: 0.265\n",
      "Bleu_4: 0.128\n",
      "computing METEOR score...\n",
      "METEOR: 0.212\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.417\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.324\n",
      "Final coco metrics:  dict_items([('Bleu_1', 0.6199999999875999), ('Bleu_2', 0.4312771730480913), ('Bleu_3', 0.26495430563176403), ('Bleu_4', 0.1276743707359539), ('METEOR', 0.21248796782497917), ('ROUGE_L', 0.417337811246854), ('CIDEr', 0.3236962406166407)])\n"
     ]
    }
   ],
   "source": [
    "# evaluate original teacher forced speaker\n",
    "# evaluate_speaker(\n",
    "#     model_path=\"../models/decoder-noEnc-prepend-512dim-4000vocab-rs1234-wEmb-cont-7.pkl\",\n",
    "#     num_val_imgs=1000,\n",
    "#     res_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_greedyDecoding_results.json\",\n",
    "#     val_ppl_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_greedyDecoding_val.json\",\n",
    "#     metrics_res_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_greedyDecoding_COCO_metrics.csv\",\n",
    "#     vocab_file=\"vocab4000.pkl\",\n",
    "#     download_dir=\"../../../data/train\",\n",
    "#     val_file=\"val_split_annIDs_singular_from_COCO_train_tensor.pt\", \n",
    "#     val_imgIDs_file=\"val_split_imgIDs_singular_from_COCO_train.pt\",\n",
    "#     batch_size=1,\n",
    "#     decoding_strategy=\"greedy\",\n",
    "# )\n",
    "# evaluate_speaker(\n",
    "#     model_path=\"../models/decoder-noEnc-prepend-512dim-4000vocab-rs1234-wEmb-cont-7.pkl\",\n",
    "#     num_val_imgs=1000,\n",
    "#     res_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_encoding_results.json\",\n",
    "#     val_ppl_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_encoding_val.json\",\n",
    "#     metrics_res_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_encoding_COCO_metrics.csv\",\n",
    "#     vocab_file=\"vocab4000.pkl\",\n",
    "#     download_dir=\"../../../data/train\",\n",
    "#     val_file=\"val_split_annIDs_singular_from_COCO_train_tensor.pt\", \n",
    "#     val_imgIDs_file=\"val_split_imgIDs_singular_from_COCO_train.pt\",\n",
    "#     batch_size=1,\n",
    "#     decoding_strategy=\"encoding\",\n",
    "# )\n",
    "evaluate_speaker(\n",
    "    model_path=\"../models/decoder-noEnc-prepend-512dim-4000vocab-rs1234-wEmb-cont-7.pkl\",\n",
    "    num_val_imgs=10,\n",
    "    res_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_pureDecoding_results.json\",\n",
    "    val_ppl_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_pureDecoding_val.json\",\n",
    "    metrics_res_path=\"../../../data/speaker_eval_results/decoder_orig_teacher_forced_pureDecoding_COCO_metrics.csv\",\n",
    "    vocab_file=\"vocab4000.pkl\",\n",
    "    download_dir=\"../../../data/train\",\n",
    "    val_file=\"val_split_annIDs_singular_from_COCO_train_tensor.pt\", \n",
    "    val_imgIDs_file=\"val_split_imgIDs_singular_from_COCO_train.pt\",\n",
    "    batch_size=1,\n",
    "    decoding_strategy=\"greedy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1aff10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
