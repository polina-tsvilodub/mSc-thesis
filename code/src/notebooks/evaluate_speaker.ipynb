{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9169dd8",
   "metadata": {},
   "source": [
    "## Evaluating the pretrained speaker: MS COCO Captions evaluation\n",
    "\n",
    "The following notebook is to set up / re-use public code for image caption evaluation following the pipeline suggested in the original paper on MS COCO Captions. Additionally, given the specifics of the downstream task (namely, sampling from the model and minimizing the CCE loss against that), a baseline for the validation PPL on sampled captions is computed as well.  \n",
    "\n",
    "Due to installation difficulties, and because it was not a part of the original paper, the SPICE score computation is commented out in the cloned code and not performed in the present evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d0b6d",
   "metadata": {},
   "source": [
    "#### Utils\n",
    "In order to compute the standard image caption evaluation metrics, the code provided in [this](https://github.com/daqingliu/coco-caption) repo is used. Since it requires the results to be formatted in a specific syntax, script below performs some utility mapping of validation annotation IDs to  validation image IDs. \n",
    "\n",
    "--> i need to produce {'image_id': XXX, 'caption': 'lower cased string'} items when validating the model. When iterating over items with my data loader, I get annotation IDs. So i need to map ann IDs to img IDs. I can do that via th COCO.loadAnns(annIds) and then retrieve 'image_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d53259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducing the desired results format\n",
    "import json\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da86c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264048\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "264048\n"
     ]
    }
   ],
   "source": [
    "# just creating a file for my entire val split\n",
    "val_ids = torch.load(\"val_split_IDs_from_COCO_train.pt\")\n",
    "print(len(val_ids))\n",
    "coco = COCO(\"../../../data/train/annotations/captions_train2014.json\")\n",
    "val_imgIDs = [coco.loadAnns(i)[0]['image_id'] for i in val_ids]\n",
    "print(len(val_imgIDs))\n",
    "# torch.save(val_imgIDs, \"val_split_imgIDs_from_COCO_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d717e9b",
   "metadata": {},
   "source": [
    "A further restriction is that there must be only one produced caption per image so the evaluation happens on\n",
    "one annID per unique image only -- the metrics are computed relative to all 5 ground truth captions anyways, but they are retrieved within the pipeline. Below, the respective annotation ID and image ID files are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3652b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i also need to create a new test split which only contains one annotation per unique image\n",
    "# get images i know werent used for pretraining\n",
    "with open(\"imgID2annID.json\", \"r\") as fp:\n",
    "    f = json.load(fp)\n",
    "imgIDs4val = list(f.keys())[30000:]\n",
    "\n",
    "ann4val_unqIm = [f[i][0] for i in imgIDs4val]\n",
    "# torch.save(torch.tensor(ann4val_unqIm), \"val_split_annIDs_singular_from_COCO_train_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6231f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52783\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "52783\n"
     ]
    }
   ],
   "source": [
    "# load unique data\n",
    "val_ids = torch.load(\"val_split_annIDs_singular_from_COCO_train_tensor.pt\").tolist()\n",
    "print(len(val_ids))\n",
    "coco = COCO(\"../../../data/train/annotations/captions_train2014.json\")\n",
    "val_imgIDs = [coco.loadAnns(i)[0]['image_id'] for i in val_ids]\n",
    "print(len(val_imgIDs))\n",
    "# torch.save(val_imgIDs, \"val_split_imgIDs_singular_from_COCO_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437cbad",
   "metadata": {},
   "source": [
    "#### Evaluation function\n",
    "The wrapper below takes in a trained model and performs the evaluation on a set number of validation images (e.g., a set including the images used in the reference game, or images used neither in pretraining nor experiments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3f4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import agent modules from the actual repo\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902cd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.speaker import DecoderRNN\n",
    "from utils.build_dataset import get_loader\n",
    "from reference_game_utils.update_policy import clean_sentence\n",
    "from utils.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813a360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_caption.pycocotools.coco import COCO \n",
    "from coco_caption.pycocoevalcap.eval import COCOEvalCap # TODO in readme add point about renaming!\n",
    "import skimage.io as io\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc414267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speaker(\n",
    "    model_path: str,\n",
    "    num_val_imgs: int,\n",
    "    res_path: str,\n",
    "    val_ppl_path: str,\n",
    "    metrics_res_path: str,\n",
    "    vocab_file: str,\n",
    "    download_dir: str,\n",
    "    val_file: str, \n",
    "    val_imgIDs_file: str,\n",
    "    vocab_threshold: int = 25,\n",
    "    batch_size: int = 1,\n",
    "    embed_size: int = 512,\n",
    "    visual_embed_size: int = 512,\n",
    "    hidden_size: int = 512,\n",
    "    decoding_strategy: str = \"greedy\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a pretrained speaker (image captioner) model for MS COCO by computing: \n",
    "    1) validation loss + perplexity given a paricular decoding strategy\n",
    "    2) image captioning evaluation metrics from MS COCO Captions:\n",
    "        BLEU, ROUGE, METEOR and CIDEr.\n",
    "    All results including produced captions are saved.\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    model_path: str\n",
    "        Path to speaker model weights.\n",
    "    num_val_imgs: int\n",
    "        Number of validation images to be used.\n",
    "    res_path: str\n",
    "        Path and name of file where produced captions will be saved.\n",
    "    val_ppl_path: str\n",
    "        Path and name of file where batch-wise validation loss and PPL will be written to.\n",
    "    metrics_res_path: str\n",
    "        Path and name of file where COCO Captions metrics will be written to.\n",
    "    vocab_file: str\n",
    "        Path to vocab file.\n",
    "    download_dir: str\n",
    "        Directory with annotations.\n",
    "    val_file: str\n",
    "        Path to file holding UNIQUE per image annotation IDs from validation set.\n",
    "    val_imgIDs_file: str\n",
    "        Path to image IDs corresponding to the validation annotation IDs above.\n",
    "    vocab_threshold: int = 25\n",
    "        Minimal token count used in vocabulary construction.\n",
    "    batch_size: int = 1\n",
    "        Must be 1.\n",
    "    embed_size: int = 512\n",
    "        Dimensionality of embeddings. Must correspond to pretraining settings.\n",
    "    visual_embed_size: int = 512\n",
    "        Dimensionality of image vectors. Must correspond to pretraining settings.\n",
    "    hidden_size: int = 512\n",
    "        Dimensionality of the hidden layer. Must correspond to pretraining settings.\n",
    "    decoding_strategy: str = \"greedy\"\n",
    "        Decoding strategy to be used in sampling. \n",
    "        Available options: \"greedy\", \"exp\", \"pure\", \"topk_temperature\", \"encoding\".\n",
    "        If \"encode\" is used, ground truth captions are just passed through the LSTM in training mode,\n",
    "        no decoding is taking place.\n",
    "    \"\"\"\n",
    "    assert batch_size == 1, \"Only batch_size=1 evaluations are supported!\"\n",
    "    \n",
    "    # data loader\n",
    "    transform_test = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                         transforms.ToTensor(), \\\n",
    "                                         transforms.Normalize((0.485, 0.456, 0.406), \\\n",
    "                                                          (0.229, 0.224, 0.225))])\n",
    "    data_loader_test = get_loader(transform=transform_test,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=True,\n",
    "                         download_dir=download_dir, \n",
    "                         vocab_file=vocab_file,\n",
    "                         dataset_path=val_file, \n",
    "                         num_imgs=num_val_imgs,\n",
    "                         embedded_imgs=torch.load(\"../train_logs/COCO_train_ResNet_features_reshaped_dict.pt\"),\n",
    "                        )\n",
    "    # add img IDs\n",
    "    data_loader_test.dataset._img_ids_flat = torch.load(val_imgIDs_file)[:num_val_imgs]\n",
    "    val_imgIDs = torch.load(val_imgIDs_file)[:num_val_imgs]\n",
    "    \n",
    "    vocab_size = len(data_loader_test.dataset.vocab)\n",
    "    # load model\n",
    "    decoder = DecoderRNN(\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        visual_embed_size,\n",
    "    )\n",
    "    decoder.eval()\n",
    "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    \n",
    "    # instantiate results \n",
    "    results = []\n",
    "    val_running_loss = 0.0\n",
    "    val_running_ppl = 0.0\n",
    "    losses_list = []\n",
    "    ppl_list = []\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    \n",
    "    num_steps = math.ceil(len(data_loader_test.dataset)/batch_size)\n",
    "    \n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        counter += 1\n",
    "        # manually construct indices to avoid duplications bc of length of examples or random repetitions\n",
    "        indices = [(i, 0)]\n",
    "        \n",
    "        new_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader_test.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        targets, distractors, target_features, distractor_features, target_captions, distractor_captions = next(iter(data_loader_test)) \n",
    "        \n",
    "        both_images = torch.cat((target_features.unsqueeze(1), distractor_features.unsqueeze(1)), dim=1)\n",
    "        # retrieve image IDs\n",
    "        batch_img_ids = [val_imgIDs[i[0]] for i in indices]\n",
    "        \n",
    "        max_seq_len = target_captions.shape[1]-1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get prediction\n",
    "            if decoding_strategy == \"encoding\":\n",
    "                outputs, _ = decoder(both_images, target_captions, hidden)\n",
    "                norm_outputs = softmax(outputs)\n",
    "                _, captions_pred = torch.max(norm_outputs, dim = -1)\n",
    "                \n",
    "            else: \n",
    "                captions_pred, log_probs, outputs, entropies = decoder.sample(\n",
    "                    both_images, \n",
    "                    max_sequence_length=max_seq_len, \n",
    "                    decoding_strategy=decoding_strategy\n",
    "                )\n",
    "            # transform to natural language\n",
    "            nl_captions_pred = clean_sentence(captions_pred, data_loader_test)\n",
    "            \n",
    "            # append to results list together with img ID\n",
    "            for i, c in list(zip(batch_img_ids, nl_captions_pred)):\n",
    "                \n",
    "                if \"end\" in c.split(\" \"):\n",
    "                    len_c = sum([1 for x in c.split(\" \")[:c.split(\" \").index(\"end\")] if x != \"end\" ])\n",
    "                else:\n",
    "                    len_c = len(c.split(\" \"))            \n",
    "                \n",
    "                results.append({\"image_id\": i, \"caption\": \" \".join(c.split()[:len_c])})\n",
    "                \n",
    "            # compute val PPL\n",
    "            loss = criterion(outputs.transpose(1,2), target_captions[:, 1:]) \n",
    "            losses_list.append(loss.item())\n",
    "            ppl = np.exp(loss.item())\n",
    "            ppl_list.append(ppl)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "            val_running_ppl += ppl\n",
    "    \n",
    "    print(\"Final average loss: \", val_running_loss / counter)\n",
    "    print(\"Final average PPL: \", val_running_ppl / counter)\n",
    "        \n",
    "    # check if results dir exists\n",
    "    os.makedirs(\"../../../data/speaker_eval_results/\", exist_ok=True)\n",
    "    \n",
    "    # write out results file\n",
    "    with open(res_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    # write out validation PPLs\n",
    "    df_out = pd.DataFrame({\n",
    "        \"loss\": losses_list,\n",
    "        \"PPL\": ppl_list,\n",
    "    })\n",
    "    df_out.to_csv(val_ppl_path)\n",
    "    \n",
    "    # now compute the evaluations, as proposed in the notebook from the repo referenced above \n",
    "    cocoRes = coco.loadRes(res_path)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(data_loader_test.dataset.coco, cocoRes)\n",
    "\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()\n",
    "    coco_metrics = cocoEval.eval.items()\n",
    "    # construct out file \n",
    "    metrics_df = pd.DataFrame(cocoEval.eval, index = [0]).round(3)\n",
    "    metrics_df.to_csv(\n",
    "        metrics_res_path\n",
    "    )\n",
    "    print(\"Final coco metrics: \", coco_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eacc49e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 74153.44it/s]\n",
      "100%|██████████| 3700/3700 [00:00<00:00, 104442.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader ids  64\n",
      "Loader ids  64\n",
      "IMG IDS  64\n",
      "IMG IDS  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final average loss:  8.345450565218925\n",
      "Final average PPL:  4216.196820365262\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Len img ids:  64\n",
      "[341245, 521150, 568955, 393602, 576543, 92639, 521132, 573988, 86831, 127575, 138859, 31673, 322670, 353357, 422423, 461236, 38435, 14824, 25644, 577685, 316622, 315808, 392404, 209261, 190723, 525891, 265209, 321213, 59622, 344969, 159683, 64751, 451312, 68764, 83815, 53957, 157434, 552159, 173515, 122343, 324901, 47293, 474653, 420487, 62821, 515186, 64818, 469644, 519696, 200404, 289204, 451381, 113812, 70411, 386074, 443432, 239559, 132617, 318815, 530040, 417528, 69758, 296696, 553719]\n",
      "Len imgIds  64\n",
      "64\n",
      "64\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "Img Ids in BLEU:  [14824, 25644, 31673, 38435, 47293, 53957, 59622, 62821, 64751, 64818, 68764, 69758, 70411, 83815, 86831, 92639, 113812, 122343, 127575, 132617, 138859, 157434, 159683, 173515, 190723, 200404, 209261, 239559, 265209, 289204, 296696, 315808, 316622, 318815, 321213, 322670, 324901, 341245, 344969, 353357, 386074, 392404, 393602, 417528, 420487, 422423, 443432, 451312, 451381, 461236, 469644, 474653, 515186, 519696, 521132, 521150, 525891, 530040, 552159, 553719, 568955, 573988, 576543, 577685]\n",
      "Hypo ['national bicycles bicycles farmers farmers farmers celery celery celery celery farmers']\n",
      "ref  ['people dressed up as zombies on a city street', 'a group of people dressed in zombie costumes', 'a group of people in makeup and dye with a stop sign', 'a couple of people standing next to a stop sign', 'a group of people dressed as zombies near a stop sign']\n",
      "Hypo ['at bicycles bicycles aprons aprons candy candy candy aprons aprons aprons']\n",
      "ref  ['there is a white and black plane on the tracks', 'a commuter train in a city advancing down the train track', 'a white and black passenger train on rail road tracks', 'a train that is sitting on the tracks under some wires', 'a train moves along some tracks in a city']\n",
      "Hypo ['at focused bicycles bicycles fedora aprons portrait portrait portrait portrait portrait portrait portrait']\n",
      "ref  ['a very funny sign by a dead squirrel in the road', 'ironic sign pointing out flattened squirrel on a street', 'their is a dead squirrel someone is trying to give away', 'a homemade sign pointing to a free dead squirrel', \"a murdered squirrel laying lifeless in a street with a cardboard sign mocking it 's death sitting to it 's right\"]\n",
      "Hypo ['at focused bicycles artwork artwork artwork artwork artwork artwork artwork artwork artwork']\n",
      "ref  ['a sign mounted to a pole that reads bus stop', 'a bus stop sign attached to a pole', 'a bus stop sign with a couple of stickers on it', 'a weathered bus stop sign on a pole', 'a older sign for people to stand to catch the bus']\n",
      "Hypo ['at focused ones ones ones ones ones ones ones ones ones ones ones']\n",
      "ref  ['a photo looking at a car mirror at the traffic behind', 'a car mirror looking back at mountains and traffic', 'looking in the rear-view mirror of a car travelling on the interstate', 'a driver looking at traffic through the rearview mirror of a car', 'a rear view mirror on the side of a car reflecting a mountain range']\n",
      "Hypo ['national bicycles bicycles bicycles farmers farmers farmers lounge live live live']\n",
      "ref  ['a man working on a train in a station', 'a guy dressed in an orange and white work suit is fueling up a train', 'an attendant attaches a hose to a train', 'a worker in an orange safety suit fueling a train', 'a person is attaching a pipe to a train']\n",
      "Hypo ['national bicycles bicycles bicycles ones ones ones ones ones ones ones ones']\n",
      "ref  ['a train passing through a crossing gate and a kid', 'a white and blue train pulling multiple train cars on tracks', 'a train comes towards a street crossing on a cloudy day', 'a train is going down the tracks during a clear day', 'a train traveling down a train track next to a green field']\n",
      "Hypo ['at bicycles bicycles bicycles bicycles built built built built']\n",
      "ref  ['some dump trucks dirt and lights at night', 'a couple of trucks in a field during the night', 'at a construction sites with some trucks working', 'a few trucks at night with their headlights on', 'a man standing near four dump trucks in a quarry at night']\n",
      "Hypo ['at focused bicycles cherry cherry cherry cherry ones cherry portrait portrait portrait portrait portrait']\n",
      "ref  ['a train parked inside of a train station next to a platform', 'a passenger train is on the tracks in a station', 'a close up of a train on a track', 'the metro train has yellow painted on the front of it', 'a train stopped in a covered railway station']\n",
      "Hypo ['at citrus stuff fit fit fit fit fit fit garbage garbage garbage fit fit fit fit']\n",
      "ref  ['a large brown cow standing next to a small pug on the side of a hill', 'a cow standing in a field looking at a dog', 'a cow in a grassy field looking at another animal', 'a cow looking at a small dog in a grassy field', 'a small dog and a cow in a field']\n",
      "Hypo ['national bicycles bicycles cherry cherry cherry cherry cherry cherry cherry cherry']\n",
      "ref  ['a picture of a parking meter on a sidewalk', 'closeup of parking meter that still has some time on it', 'a parking meter sitting by the side of the road', 'black meter sitting on the side of a road with time up', 'a gray parking meter showing amount of time left on it']\n",
      "Hypo ['at focused bicycles bicycles alley farmers farmers icy icy icy farmers']\n",
      "ref  ['a couch with a cat sitting on the back', 'a cat laying on the back of a couch in front of a window', 'a cat sleeping on top of the back of a chair on a pink blanket', 'the cat is laying on the pink blanket by a window', 'a fluffy cat laying on a blanket on the couch']\n",
      "Hypo ['at sits frisbee frisbee alley alley alley automobile automobile automobile put automobile automobile automobile']\n",
      "ref  ['a very cute black cat laying on a bed with a suitcase', 'black cat lying on suitcase lid on bed in sleeping area', 'a black cat is sitting on a suitcase on a bed', 'a cat with a serious look sitting on a suit case flap on a bed', 'a black cat lays on the open lid of a suitcase']\n",
      "Hypo ['at bicycles bicycles bicycles candy candy candy candy candy candy']\n",
      "ref  ['two trains at night are going down their tracks', 'a couple of trains that are going through the night', 'two trains passing each other in a train station', 'a late night train arriving at a station', 'a photo taken at night of two trains on train tracks']\n",
      "Hypo ['at bicycles salon salon salon salon ones ones salon salon ones']\n",
      "ref  ['a road sign hangs from an old rusted pole', 'a sign of a street name taken crocked', 'wooden street sign with arrow finial that identifies cecilia st.', 'a blue street sign that says cecilia st.', 'a street sign indicating cecilia st. or 800 s']\n",
      "Hypo ['national bicycles bicycles farmers farmers farmers farmers farmers farmers leopard farmers']\n",
      "ref  ['check the phone to see if anyone has called', 'a man with a hat sitting with a cell phone', 'a man wearing a hat sitting down looking at his cell phone', 'man in a black suit and hat looking happy while checking his cell phone', 'a guy wearing glasses and all black holding a white device']\n",
      "Hypo ['at focused focused cherry aprons aprons aprons fit aprons aprons aprons aprons']\n",
      "ref  ['a large blue truck parked next to a red truck', 'a semi truck parked at a rest stop', 'a blue semi truck next to a red semi truck', 'two trucks parked on dirt beneath a dark sky', 'a blue truck is going down an on ramp']\n",
      "Hypo ['at focused sits aprons aprons aprons aprons aprons aprons aprons aprons']\n",
      "ref  ['some trucks are being parked on a parking lot', 'three sixteen wheeler trucks are parked close together', 'a red truck and a yellow truck in a parking lot', 'three trucks are parked next to each other', 'three large trucks parked beside each other on the cement']\n",
      "Hypo ['national bicycles bicycles bicycles aprons farmers aprons aprons aprons aprons aprons aprons aprons']\n",
      "ref  ['a traffic light next to a street sign that says hollywood', 'a streetsign and stop light with an advertisement on a building neaby', \"there 's a hollywood street sign on a traffic light\", 'a street light and road sign that says hollywood', 'the blue sign next to the stone and glass building says hollywood']\n",
      "Hypo ['at focused focused alley alley alley alley alley alley alley hose hose alley']\n",
      "ref  ['the man holding a can of beer is petting a cat', 'a man sitting down while holding a beer and petting a cat', 'a cat sitting on a chair being petted by a man', 'this man is drinking beer and touching his cat', 'a man that is petting his cat that is in a chair']\n",
      "Hypo ['at bicycles bicycles bicycles bicycles bicycles spots spots spots spots spots spots spots spots spots']\n",
      "ref  [\"a sign saying do n't honk $ 350 penalty on a pole\", 'a street sign warning of a fine for honking', 'traffic signs on a pole in front of buildings', \"a sign that reads do n't honk as it has a penalty of $ 350\", 'a sign warns of a $ 350 fine for honking a horn']\n",
      "Hypo ['at citrus shrubbery aprons aprons aprons leopard leopard leopard leopard']\n",
      "ref  ['a train traveling down a road in the snow', 'a train approaches on a set of snowy tracks', 'a train passing down a track in the snow', 'a train traveling on a snowy train track', 'a train traveling along a snow covered country']\n",
      "Hypo ['at bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles']\n",
      "ref  ['a picture of a long train traveling away', 'there is a city train that is parked along side the curb', 'grey commuter train at platform of covered station', 'a closeup side view of a passenger train', 'a train parked inside of a train station next to a platform']\n",
      "Hypo ['national focused focused aprons aprons aprons aprons aprons aprons aprons aprons aprons aprons aprons aprons']\n",
      "ref  ['a white truck driving by a street sign with an arrow on it', 'various items in the bed of a white truck on a highway', 'miscellaneous tools in the bed of a white truck', 'back of a white pick up truck going down the highway', 'this truck is carrying cargo in its bed']\n",
      "Hypo ['national focused cherry cherry cherry cherry cherry ones cherry cherry']\n",
      "ref  ['a commuter train pulling into a train station', 'modern train parked in an underground train station', 'a subway train is waiting on the tracks', 'a subway train parked at a passenger platform', 'the train looks as though it needs to be fixed and washed']\n",
      "Hypo ['at focused bicycles fedora fedora aprons fedora aprons aprons aprons aprons aprons aprons aprons aprons aprons']\n",
      "ref  ['a woman and child stand on roadside while two men are at back of vehicle', 'people on the side of the road with a couple of vehicles', 'a boy and a woman are standing next to a roadway', 'a couple of people standing next to a yellow tow truck', 'the mother and son have come to meet the father']\n",
      "Hypo ['at focused bicycles ones ones ones ones ones ones ones']\n",
      "ref  ['a modern train is pulling along the tracks', 'a train parked inside of a station next to a loading platform', 'a yellow and white subway train sitting on some tracks', 'a yellow and white train is being serviced', 'dirty subway control car sitting in the subway']\n",
      "Hypo ['at focused bicycles fedora fedora fedora ones ones ones fedora']\n",
      "ref  ['a silver truck parked next to another truck', 'two trucks in a clearing parked in the grass', 'two pickup trucks parked on grass in a wooded area', 'two off road type pick up trucks parked in a grassy field', 'two pickup trucks parked in the woods in a haze']\n",
      "Hypo ['national bicycles bicycles bicycles bicycles bicycles bicycles bicycles microwaves microwaves bicycles microwaves']\n",
      "ref  ['a train is on train tracks reflecting low sunlight', 'a view of the sunset over a train', 'the sun shining on the top of a train on train tracks', 'a sun glaring train drives next to a building', 'an upper shot of a train traveling down the tracks']\n",
      "Hypo ['at focused focused fedora fedora fedora ones ones ones ones ones aprons ones ones ones ones']\n",
      "ref  ['a blue food truck is parked in a shopping center by a clothing store', 'a blue truck sits parked alongside the curb in a parking lot', 'a truck parked next to a store in the parking lot', 'there is a blue truck that is parked in front of a store', 'blue commercial truck parked in lot next to urban businesses']\n",
      "Hypo ['at focused alley alley alley alley alley alley alley leopard celery automobile']\n",
      "ref  ['a pink bag contains a laptop and some other things', 'a large pink bag with a laptop on a chair', 'a large bag with a lap top sitting inside it', 'a laptop is packed away in a purse with kleenex', \"there 's a laptop in the pink floral bag and a package of tissues in a pocket\"]\n",
      "Hypo ['at bicycles bicycles bicycles leopard leopard leopard leopard leopard leopard ones']\n",
      "ref  ['a train moving across the tracks outside if a stadium', 'train tracks near a stadium with a train going by', 'a train is moving along a stretch of track', 'a train on a track near many trees', 'a passenger train moving along one of several tracks that are side by side']\n",
      "Hypo ['at focused obstacle cherry cherry cherry cherry ones cherry ones cherry aprons']\n",
      "ref  ['a white and yellow train pulling into a train station', 'there is a yellow and whit train on the tracks', 'a man drives a train down a track', 'a yellow and white train on the tracks', 'a train on the tracks parked in a train station']\n",
      "Hypo ['national focused shrubbery alley everywhere material material material material corner celery celery members']\n",
      "ref  ['a blanket and backpack sitting on the beach next to a surfboard', 'a white surfboard is laying in the sand', 'a towel surfboard and backpack laying in the sand', 'a large rock with a surf board and a surfers belongings sprawled out', 'a surfboard and some other items on a rock']\n",
      "Hypo ['at focused focused cherry cherry cherry cherry cherry cherry cherry']\n",
      "ref  ['the freight train is traveling through the flatlands', 'a blue and black train going down railroad tracks', 'the train is moving fast on the tracks', 'a train on a train tracks with power lines above', 'a blue and silver train sitting on the track']\n",
      "Hypo ['at focused bicycles bicycles salon salon salon salon salon salon salon placing']\n",
      "ref  ['a street sign labelled sex streer near a dont honk sign', 'a sign that says no honking on the corner of sex st.', 'a street called sex street on the corner of a road', 'a street sign and a traffic sign on a street pole', \"a sign on sex st that says do n't honk $ 350 penalty\"]\n",
      "Hypo ['at bicycles bicycles bicycles bicycles bicycles bicycles aprons aprons bicycles bicycles bicycles portrait']\n",
      "ref  ['a truck at a stop light with its brake lights on', 'a photograph of an unusual license plate on a truck', 'a truck stopped on a street at a red light', 'a truck that is out on the street', 'a blue truck stopped at a traffic light']\n",
      "Hypo ['national focused ones ones ones ones ones ones ones ones ones ones ones']\n",
      "ref  ['a train is coming up on another train next to it', 'a train engine with steam coming from it on a track', 'a green and red train with a number six on it', 'two green trains with numbers on them pouring out steam', 'two old fashioned steam railroad trains are on the tracks']\n",
      "Hypo ['at bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles leopard leopard bicycles bicycles']\n",
      "ref  ['trains stopped on the tracks at the railroad at a train station', 'the red train has stopped for possible passengers', 'a long train is coming down some tracks', 'a train traveling past a train station near other tracks', 'a commuter train pulling up to the station to deliver and pick up passengers']\n",
      "Hypo ['national focused bicycles bicycles candy candy candy candy candy candy candy']\n",
      "ref  ['a yellow and brown train passing under a bridge', 'a train traveling down train tracks under a bridge', 'the union pacific train has as its slogan building america', 'a locomotive engine parked on a train track', 'a long yellow train driving down the tracks']\n",
      "Hypo ['at shrubbery shrubbery ones ones ones ones ones ones ones ones ones']\n",
      "ref  ['a group of animals walking across a wave covered beach', 'a small herd of cows walks along a beach towards people', 'a line of animals walking in a line on the beach', 'animals walking down the beach near the water', 'lol cows walking down the beach and people walking up the beach']\n",
      "Hypo ['at focused focused ones ones ones vests vests vests ones ones ones ones hovering']\n",
      "ref  ['a stop sign is placed on a street by a wooded area', 'a stop sign sitting next to a street near a forest', 'a road that is right by of the forest', 'the sun shines on some trees on a country road', 'a rurel country road with a stop sign']\n",
      "Hypo ['national focused bicycles aprons aprons aprons aprons aprons aprons aprons aprons aprons aprons']\n",
      "ref  ['a train with a brown engine is on a train track', 'a brown and yellow train engine pulling train cars down the track', 'a railroad track coming down a set of tracks', 'a train is on a track near the grass', 'a red and yellow striped train is on the tracks']\n",
      "Hypo ['at focused bicycles bicycles ones ones ones ones ones ones ones ones automobile ones']\n",
      "ref  ['a blue truck parked in a wet parking lot near tall buildings', 'an old dump truck sits in front of a back street in a large city', 'a light blue dump truck parked on the side of a road', 'the older truck is waiting for the next load', 'a dump truck parked on muddy ground in an empty lot']\n",
      "Hypo ['national focused alley alley alley alley leopard leopard leopard leopard leopard leopard']\n",
      "ref  ['the girl is running through the grass in a costume', 'a girl wearing a hat and dress while holding a purse that looks like a clock', 'girl with white and pink dress with hat and clock bag', 'a young girl in a dress and top hat running in some grass', 'a woman in a costume inspired by the white rabbit from alice in wonderland']\n",
      "Hypo ['at focused focused ones ones ones ones ones ones ones aprons']\n",
      "ref  ['an old fashioned train is displayed beside a mountain', 'a steam engine in the middle of a park', 'a train is parked on the side of the road', 'a vintage locomotive on display next to a mountain', 'a train is parked next to the mountain side']\n",
      "Hypo ['at citrus ones ones ones ones ones ones ones ones ones ones ones hose ones']\n",
      "ref  ['a mama cow walking with two baby cows on top of a beach', 'tree cows walk on the sand at a beach', 'cows walking on a beach with boats behind them', 'two little cows following a bigger one on the beach', 'a cow and her two calves walking along a beach']\n",
      "Hypo ['at focused bicycles bicycles candy candy candy candy candy candy']\n",
      "ref  ['a couple of trains are sitting on some tracks', 'a station that has several train sections in it', 'a photo of several trains in a warehouse', 'a bunch of train cars in a garage', 'trams are stopped on the tracks in a subway station']\n",
      "Hypo ['national bicycles bicycles bicycles bronze spanish spanish spanish spanish spanish spanish spanish spanish spanish']\n",
      "ref  ['a grey and white striped tabby cat laying inside a black suitcase', 'a fat cat sitting on luggage while staring at something', 'a brown cat is partly nestled inside a black bag', 'a furry cat sits inside of an open black suitcase', 'a cat that is laying on top of some luggage']\n",
      "Hypo ['national bicycles bicycles bicycles bicycles bicycles bicycles ones bicycles ones portrait bicycles portrait']\n",
      "ref  ['a train station with an amtrak train on the middle track', 'a train pulls up to a station and platform', 'a passenger trails sits in the train yard on tracks', 'an amtrak train slows down as it enters an empty station', 'a photo of a train station with a train at the platform']\n",
      "Hypo ['national bicycles bicycles alley lounge lounge lounge lounge lounge']\n",
      "ref  ['a spotted cat pawing at a strange light', 'a cat pawing at a lighting fixture', 'a curious adult cat paws at a light fixture', 'a cat is reaching for a white umbrella', 'mischievous cat playing with an umbrella with its paw']\n",
      "Hypo ['at focused mountains fedora aprons ones ones portrait portrait portrait portrait portrait portrait portrait portrait']\n",
      "ref  ['a double tractor trailer is driving on the highway through a snow storm', 'the truck is traveling down the road in really bad weather conditions', 'a tractor-trailer truck drives on a road in snowy conditions', 'a truck riding through a puddle of water near trees with snow on them', 'a tractor trailer is moving along a road']\n",
      "Hypo ['at walk farmers ones ones ones ones ones ones ones ones ones ones ones ones']\n",
      "ref  ['a woman is holding a dog to view a cow through a window', 'a woman and a dog are looking out a window at a cow and flowers', 'there is a woman holding a small dog out of a car window', 'a cow is on the side of a road as a woman in a car holds up her black dog to the window', 'a woman and a dog looking at a cow through a window']\n",
      "Hypo ['at bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles bicycles']\n",
      "ref  ['a man with a camera is walking a calf down a street', 'a man with a camera around his neck walking in the street with a large dog', 'a man with a camera walks with a dog on a street', 'a couple walks down the street with a young cow', 'two people with cameras walking with their dog']\n",
      "Hypo ['national focused bicycles desk desk fit desk fit fit fit desk']\n",
      "ref  ['intersection street signs displaying plaza drive and coe avenue', 'a sign that reads plaza drive is being displayed', 'a street sign sits high and names the street', 'a street sign indicates where plaza drive is located', 'a black and white street sign against a blue sky']\n",
      "Hypo ['at bicycles bicycles bicycles ones ones ones ones ones salon salon ones']\n",
      "ref  ['a passenger train moves along rails next to apartment buildings', 'a train on a track under a blue sky with some buildings', 'a train on the tracks in what seems to be a deserted area of town', 'a gray train some buildings and some train tracks', 'a train going along a track near apartments']\n",
      "Hypo ['at focused motorboat motorboat motorboat motorboat motorboat motorboat ones motorboat ones ones ones motorboat']\n",
      "ref  ['a red stop sign next to a large bush with red flowers', 'a stop sign stands behind a large display of flowers', 'a stop sign some red flowers a building and some trees', 'a stop sign on a street beside some red flowers', 'a stop sign in font of a plant with red flowers']\n",
      "Hypo ['at bicycles bicycles bicycles alley deliver deliver deliver deliver deliver']\n",
      "ref  ['a brown and black cat lying on a bed', \"a multi-colored cat that is laying on a piece of furniture with it 's head up and eyes open\", 'a cat is sitting on a red sofa', 'this is a calico cat sitting on a red piece of furniture', 'a cat laying on a couch in a room']\n",
      "Hypo ['at bicycles scenic scenic farmers farmers farmers fedora fedora portrait aprons aprons aprons aprons']\n",
      "ref  ['several soldiers with weapons posing in front of a truck and trailer', 'group of army soldiers posing for an image on a truck', 'a group of service men posing in front of a truck', 'a national guard truck and trailer and soldiers with their weapons', 'there are many men and woman standing next to a national guard truck']\n",
      "Hypo ['at sides farmers farmers automobile automobile automobile automobile automobile automobile']\n",
      "ref  ['a cat lying in the sun on a couch', 'a fluffy cat is sitting on a poka dot couch with his tail wrapped under him', 'a cat sits in the dark on a couch while the area next to it is brightly lit', 'a cat sitting on top of a polka doted couch', 'a cat sits on a couch leaning on a pillow']\n",
      "Hypo ['at bicycles bicycles bicycles bicycles candy candy built candy candy']\n",
      "ref  ['this train was photographed from an aerial view', 'a train is riding on tracks in front of a building', 'looking down on a train that is on the tracks', 'looking down at a commuter train passing a building', 'a long train travels on the tracks in the countryside']\n",
      "Hypo ['at bicycles bicycles farmers farmers farmers leopard leopard leopard hose hose hose hose live live live']\n",
      "ref  ['a street sign on the edge of a curb has a sign with a monkey in spanish a handicap sign and a parking sign', 'a sign sitting on the side of a road on top of a sidewalk', 'there is a street pole with many different signs on it', 'a set of parking signs with a monkey sign below it', 'various street signs on brick sidewalk next to street']\n",
      "Hypo ['national focused bicycles fedora fedora candy candy candy salon salon salon salon']\n",
      "ref  ['trains passing on tracks near city area on cloudy day', 'two trains are going in opposite directions on parallel tracks', 'maroon train setting on tracks next to other train', 'a couple of trains traveling past each other on tracks', 'a train pulls up to another train on the tracks']\n",
      "Hypo ['at bicycles bicycles bicycles candy candy candy candy candy candy candy aprons candy candy']\n",
      "ref  ['a white trolly is parked at a platform station in the city', 'electric trolley no. 701 pulling into the station', 'a white train merging towards a platform to make a stop', 'a train coming into a train station in an urban area', 'a white train with few people inside entering into the station']\n",
      "{'testlen': 788, 'reflen': 734, 'guess': [788, 724, 660, 596], 'correct': [13, 0, 0, 0]}\n",
      "ratio:1.073569\n",
      "Bleu_1: 0.016\n",
      "Bleu_2: 0.000\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.007\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.019\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.001\n",
      "Final coco metrics:  dict_items([('Bleu_1', 0.016497461928913076), ('Bleu_2', 1.509521434943508e-10), ('Bleu_3', 3.256203541872574e-13), ('Bleu_4', 1.5513939494595848e-14), ('METEOR', 0.0069023193987303765), ('ROUGE_L', 0.0189564366156952), ('CIDEr', 0.0012282727200121127)])\n"
     ]
    }
   ],
   "source": [
    "evaluate_speaker(\n",
    "    model_path=\"../models/decoder-coco-512dim-scheduled_sampling_wGreedyDecoding_k150-1.pkl\",\n",
    "    num_val_imgs=64,\n",
    "    res_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_results.json\",\n",
    "    val_ppl_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_val.json\",\n",
    "    metrics_res_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_COCO_metrics.csv\",\n",
    "    vocab_file=\"vocab4000.pkl\",\n",
    "    download_dir=\"../../../data/train\",\n",
    "    val_file=\"val_split_annIDs_singular_from_COCO_train_tensor.pt\", \n",
    "    val_imgIDs_file=\"val_split_imgIDs_singular_from_COCO_train.pt\",\n",
    "    batch_size=1,\n",
    "    decoding_strategy=\"encoding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddf73e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
