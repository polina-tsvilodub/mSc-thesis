{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9169dd8",
   "metadata": {},
   "source": [
    "## Evaluating the pretrained speaker: MS COCO Captions evaluation\n",
    "\n",
    "The following notebook is to set up / re-use public code for image caption evaluation following the pipeline suggested in the original paper on MS COCO Captions. Additionally, given the specifics of the downstream task (namely, sampling from the model and minimizing the CCE loss against that), a baseline for the validation PPL on sampled captions is computed as well.  \n",
    "\n",
    "Due to installation difficulties, and because it was not a part of the original paper, the SPICE score computation is commented out in the cloned code and not performed in the present evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d0b6d",
   "metadata": {},
   "source": [
    "#### Utils\n",
    "In order to compute the standard image caption evaluation metrics, the code provided in [this](https://github.com/daqingliu/coco-caption) repo is used. Since it requires the results to be formatted in a specific syntax, script below performs some utility mapping of validation annotation IDs to  validation image IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76d53259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducing the desired results format\n",
    "import json\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da86c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264048\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "264048\n"
     ]
    }
   ],
   "source": [
    "# --> i need to produce {'image_id': XXX, 'caption': 'lower cased string'} items when validating. \n",
    "# when i iterate over items with my data loader, i get annotation ids. \n",
    "# so i need to map ann IDs to img IDs. I can do that via th COCO .loadAnns(annIds) and then retrieve 'image_id', \n",
    "\n",
    "# actually i'll just create a file for my entire val split\n",
    "val_ids = torch.load(\"val_split_IDs_from_COCO_train.pt\")\n",
    "print(len(val_ids))\n",
    "coco = COCO(\"../../../data/train/annotations/captions_train2014.json\")\n",
    "val_imgIDs = [coco.loadAnns(i)[0]['image_id'] for i in val_ids]\n",
    "print(len(val_imgIDs))\n",
    "# torch.save(val_imgIDs, \"val_split_imgIDs_from_COCO_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "220aa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further, it is necessary to create an annotation subset matching the val set in length\n",
    "with open(\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_results.json\", \"r\") as f_in:\n",
    "    coco_all = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14e0e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 549810,\n",
       " 'caption': 'Safari safari car print print rusted print rusted print though print though print'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(coco_all[\"images\"])\n",
    "# len(coco_all[\"annotations\"])\n",
    "coco_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437cbad",
   "metadata": {},
   "source": [
    "#### Evaluation function\n",
    "The wrapper below takes in a trained model and performs the evaluation on a set number of validation images (set including the images used in the reference game) and on the test set (images used neither in pretraining nor experiments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3f4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import agent modules from the actual repo\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902cd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.speaker import DecoderRNN\n",
    "from utils.build_dataset import get_loader\n",
    "from reference_game_utils.update_policy import clean_sentence\n",
    "from utils.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813a360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_caption.pycocotools.coco import COCO # not sure if this is needed bc of the loadRes method\n",
    "from coco_caption.pycocoevalcap.eval import COCOEvalCap # TODO in readme add point about renaming!\n",
    "# import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "# import pylab\n",
    "# pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc414267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_speaker(\n",
    "    model_path: str,\n",
    "    num_val_imgs: int,\n",
    "    res_path: str,\n",
    "    val_ppl_path: str,\n",
    "    vocab_file: str,\n",
    "    download_dir: str,\n",
    "    val_file: str, \n",
    "    vocab_threshold: int = 25,\n",
    "    batch_size: int = 64,\n",
    "    embed_size: int = 512,\n",
    "    visual_embed_size: int = 512,\n",
    "    hidden_size: int = 512,\n",
    "    decoding_strategy: str = \"greedy\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a pretrained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data loader\n",
    "    transform_test = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                         transforms.ToTensor(), \\\n",
    "                                         transforms.Normalize((0.485, 0.456, 0.406), \\\n",
    "                                                          (0.229, 0.224, 0.225))])\n",
    "    data_loader_test = get_loader(transform=transform_test,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=True,\n",
    "                         download_dir=download_dir, \n",
    "                         vocab_file=vocab_file,\n",
    "                         dataset_path=val_file, \n",
    "                         num_imgs=num_val_imgs,\n",
    "                         embedded_imgs=torch.load(\"../train_logs/COCO_train_ResNet_features_reshaped_dict.pt\"),\n",
    "                        )\n",
    "    # add img IDs\n",
    "    data_loader_test.dataset._img_ids_flat = val_imgIDs[:num_val_imgs]\n",
    "    \n",
    "#     print(\"ids \", data_loader_test.dataset.ids)\n",
    "#     print(\"Features \", data_loader_test.dataset.embedded_imgs.shape)\n",
    "    \n",
    "    vocab_size = len(data_loader_test.dataset.vocab)\n",
    "    # load model\n",
    "    decoder = DecoderRNN(\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        visual_embed_size,\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "    \n",
    "    # instantiate results \n",
    "    results = []\n",
    "    val_running_loss = 0.0\n",
    "    val_running_ppl = 0.0\n",
    "    losses_list = []\n",
    "    ppl_list = []\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    \n",
    "    num_steps = math.ceil(len(data_loader_test.dataset)/batch_size)\n",
    "    \n",
    "    # configs for the caption evaluations\n",
    "#     dataDir='.'\n",
    "#     dataType='val2014'\n",
    "#     algName = 'fakecap'\n",
    "#     annFile='%s/annotations/captions_%s.json'%(dataDir,dataType)\n",
    "#     subtypes=['results', 'evalImgs', 'eval']\n",
    "#     [resFile, evalImgsFile, evalFile]= \\\n",
    "#     ['%s/results/captions_%s_%s_%s.json'%(dataDir,dataType,algName,subtype) for subtype in subtypes]\n",
    "    \n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        counter += 1\n",
    "        \n",
    "        indices = data_loader_test.dataset.get_func_train_indices()\n",
    "        \n",
    "        new_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader_test.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        targets, distractors, target_features, distractor_features, target_captions, distractor_captions = next(iter(data_loader_test)) \n",
    "        \n",
    "        both_images = torch.cat((target_features.unsqueeze(1), distractor_features.unsqueeze(1)), dim=1)\n",
    "        # retrieve image IDs\n",
    "        batch_img_ids = [val_imgIDs[i[0]] for i in indices]\n",
    "        \n",
    "        max_seq_len = target_captions.shape[1]-1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get prediction\n",
    "            captions_pred, log_probs, outputs, entropies = decoder.sample(\n",
    "                both_images, \n",
    "                max_sequence_length=max_seq_len, \n",
    "                decoding_strategy=decoding_strategy\n",
    "            )\n",
    "            # transform to natural language\n",
    "            nl_captions_pred = clean_sentence(captions_pred, data_loader_test)\n",
    "            \n",
    "            # append to results list together with img ID\n",
    "            for i, c in list(zip(batch_img_ids, nl_captions_pred)):\n",
    "#                 print(\"I and C \", i, c)\n",
    "                \n",
    "                if \"end\" in c.split(\" \"):\n",
    "                    len_c = sum([1 for x in c.split(\" \")[:c.split(\" \").index(\"end\")] if x != \"end\" ])\n",
    "                else:\n",
    "                    len_c = len(c.split(\" \"))            \n",
    "                \n",
    "                results.append({\"image_id\": i, \"caption\": \" \".join(c.split()[:len_c])})\n",
    "                \n",
    "            # compute val PPL\n",
    "            loss = criterion(outputs.transpose(1,2), target_captions[:, 1:]) \n",
    "            losses_list.append(loss.item())\n",
    "            ppl = np.exp(loss.item())\n",
    "            ppl_list.append(ppl)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "            val_running_ppl += ppl\n",
    "    \n",
    "    print(\"Final average loss: \", val_running_loss / counter)\n",
    "    print(\"Final average PPL: \", val_running_ppl / counter)\n",
    "    \n",
    "    # check if results dir exists\n",
    "    os.makedirs(\"../../../data/speaker_eval_results/\", exist_ok=True)\n",
    "    \n",
    "    # write out results file\n",
    "    with open(res_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    # write out validation PPLs\n",
    "    df_out = pd.DataFrame({\n",
    "        \"loss\": losses_list,\n",
    "        \"PPL\": ppl_list,\n",
    "    })\n",
    "    df_out.to_csv(val_ppl_path)\n",
    "    \n",
    "    # now compute the evaluations, as proposed in the notebook from the repo referenced above \n",
    "    cocoRes = coco.loadRes(res_path)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    coco_truth = COCO(\"../../../data/train/annotations/captions_train2014.json\")\n",
    "    cocoEval = COCOEvalCap(coco_truth, cocoRes) # data_loader_test.dataset.coco\n",
    "\n",
    "    # evaluate on a subset of images by setting\n",
    "    # cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()\n",
    "    coco_metrics = cocoEval.eval.items()\n",
    "    # TODO maybe write these smh too\n",
    "    print(\"Final coco metrics: \", coco_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eacc49e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 114730.13it/s]\n",
      "100%|██████████| 3700/3700 [00:00<00:00, 88550.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final average loss:  8.310003161430359\n",
      "Final average PPL:  4064.401759388544\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-909c83d0503b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vocab4000.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../../data/train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mval_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_split_IDs_from_COCO_train_tensor.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     vocab_threshold: int = 25,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     batch_size: int = 64,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6e1ac4029bfa>\u001b[0m in \u001b[0;36mevaluate_speaker\u001b[0;34m(model_path, num_val_imgs, res_path, val_ppl_path, vocab_file, download_dir, val_file, vocab_threshold, batch_size, embed_size, visual_embed_size, hidden_size, decoding_strategy)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# evaluate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mcoco_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# TODO maybe write these smh too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/thesis_MSc/mSc-thesis/code/src/notebooks/coco_caption/pycocoevalcap/eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'computing %s score...'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/thesis_MSc/mSc-thesis/code/src/notebooks/coco_caption/pycocoevalcap/bleu/bleu.py\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Sanity check.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_speaker(\n",
    "    model_path=\"../models/decoder-coco-512dim-scheduled_sampling_wGreedyDecoding_k150-1.pkl\",\n",
    "    num_val_imgs=500,\n",
    "    res_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_results.json\",\n",
    "    val_ppl_path=\"../../../data/speaker_eval_results/decoder_scheduled_sampling_wGreedyDecoding_k150-1_val.json\",\n",
    "    vocab_file=\"vocab4000.pkl\",\n",
    "    download_dir=\"../../../data/train\",\n",
    "    val_file=\"val_split_IDs_from_COCO_train_tensor.pt\", \n",
    "#     vocab_threshold: int = 25,\n",
    "#     batch_size: int = 64,\n",
    "#     embed_size: int = 512,\n",
    "#     visual_embed_size: int = 512,\n",
    "#     hidden_size: int = 512,\n",
    "#     decoding_strategy: str = \"greedy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2081ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
