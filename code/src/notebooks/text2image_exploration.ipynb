{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509d4e9a",
   "metadata": {},
   "source": [
    "## Explore Text2Image models\n",
    "\n",
    "This notebook explores the computation of image similarity as the dot product of embeddings computed based on the pretrained ResNet50 model (pretrained on ImageNet).\n",
    "This similarity computation is a necessary component for potential Text2Image based language drift metrics / models.\n",
    "\n",
    "More specifically, as a proof of concept, I checked whether the dot product of ResNet based embeddings for images from the same categories is higher than for images from different categories. This is important in order to check whether simple dot product computations based on ResNet embeddings are viable as a similarity metric.\n",
    "\n",
    "The exploration is as follows:\n",
    "* sample images are retrieved from the COCO dataset\n",
    "* based on captions similar to the original captions, images are generated from the CogView Text2Imgae transformer model (a demonstration can be found [here](https://wudao.aminer.cn/CogView/index.html) and was used to generate the images) (Ding et al., 2021, see [here](https://pythonawesome.com/a-pretrained-transformer-for-text-to-image-generation-in-general-domain/)). \n",
    "    * this demo only works for Chinese text, so the sample input texts were translated into simplified Chinese using Google translate.\n",
    "* images from texts with unrelated categories were generated as distractors    \n",
    "* a ResNet instance pretrained on ImageNet was loaded and embeddings were created for all the images.\n",
    "* dot products were computed for all pairs of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947b531",
   "metadata": {},
   "source": [
    "First, a COCO image containing the categories \"skateboard\", \"person\", \"dog\" was retrieved. Then, two sample images were generated from the CogView model. Then, the dot product for both embeddings is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68948646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1fd7e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate pretrained ResNet model\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling=max)\n",
    "\n",
    "# load image from unrelated category\n",
    "img_path = 'cog-view-apple.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "# compute embedding\n",
    "features_apple = model.predict(x)\n",
    "# flatten the embedding\n",
    "features_apple_flat = tf.reshape(features_apple, -1)\n",
    "features_apple_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45866d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load CogView sample for skateboard+person+dog\n",
    "img2_path = 'cog-view-sample2.jpg'\n",
    "img2 = image.load_img(img2_path, target_size=(224, 224))\n",
    "x2 = image.img_to_array(img2)\n",
    "x2 = np.expand_dims(x2, axis=0)\n",
    "x2 = preprocess_input(x2)\n",
    "# compute embedding\n",
    "features_cog_skateboard = model.predict(x2)\n",
    "features_cog_skateboard.shape\n",
    "features_cog_skateboard_flat = tf.reshape(features_cog_skateboard, -1)\n",
    "features_cog_skateboard_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e09d0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load original COCO image for skateboard+person+dog\n",
    "img3_path = 'person-skateboard-dog-coco.png'\n",
    "img3 = image.load_img(img3_path, target_size=(224, 224))\n",
    "x3 = image.img_to_array(img3)\n",
    "x3 = np.expand_dims(x3, axis=0)\n",
    "x3 = preprocess_input(x3)\n",
    "# compute embedding\n",
    "features_coco_skateboard = model.predict(x3)\n",
    "features_coco_skateboard.shape\n",
    "features_coco_skateboard_flat = tf.reshape(features_coco_skateboard, -1)\n",
    "features_coco_skateboard_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2b46995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=31308.82>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute dot product of the original COCO image and the CogView category-related image\n",
    "tf.tensordot(features_coco_skateboard_flat, features_cog_skateboard_flat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11cfecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=19613.654>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute dot product of the original COCO image and the CogView category-UNRELATED image\n",
    "tf.tensordot(features_coco_skateboard_flat, features_apple_flat, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b14e16",
   "metadata": {},
   "source": [
    "--> We can see that the dor product is higher for the category related images.\n",
    "\n",
    "Now, the same procedure is applied to a second example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "067e07b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load original COCO image for apple+sandwich+cup\n",
    "img4_path = 'apple-sandwich-cup-coco1.png'\n",
    "img4 = image.load_img(img4_path, target_size=(224, 224))\n",
    "x4 = image.img_to_array(img4)\n",
    "x4 = np.expand_dims(x4, axis=0)\n",
    "x4 = preprocess_input(x4)\n",
    "# compute embedding\n",
    "features_coco_food = model.predict(x4)\n",
    "features_coco_food_flat = tf.reshape(features_coco_food, -1)\n",
    "features_coco_food_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fb30f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load another COCO image for apple+sandwich+cup\n",
    "img5_path = 'apple-sandwich-cup-coco2.png'\n",
    "img5 = image.load_img(img5_path, target_size=(224, 224))\n",
    "x5 = image.img_to_array(img5)\n",
    "x5 = np.expand_dims(x5, axis=0)\n",
    "x5 = preprocess_input(x5)\n",
    "# compute embedding\n",
    "features_coco_food2 = model.predict(x5)\n",
    "features_coco_food2_flat = tf.reshape(features_coco_food2, -1)\n",
    "features_coco_food2_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa32c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load images generated from CogView for the respective categories\n",
    "# load original COCO image for apple+sandwich+cup\n",
    "img6_path = 'cog-view-food-sample1.jpg'\n",
    "img6 = image.load_img(img6_path, target_size=(224, 224))\n",
    "x6 = image.img_to_array(img6)\n",
    "x6 = np.expand_dims(x6, axis=0)\n",
    "x6 = preprocess_input(x6)\n",
    "# compute embedding\n",
    "features_cog_food1 = model.predict(x6)\n",
    "features_cog_food1_flat = tf.reshape(features_cog_food1, -1)\n",
    "features_cog_food1_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef5e493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load images generated from CogView for the respective categories\n",
    "# load original COCO image for apple+sandwich+cup\n",
    "img7_path = 'cog-view-food-sample2.jpg'\n",
    "img7 = image.load_img(img7_path, target_size=(224, 224))\n",
    "x7 = image.img_to_array(img7)\n",
    "x7 = np.expand_dims(x7, axis=0)\n",
    "x7 = preprocess_input(x7)\n",
    "# compute embedding\n",
    "features_cog_food2 = model.predict(x7)\n",
    "features_cog_food2_flat = tf.reshape(features_cog_food2, -1)\n",
    "features_cog_food2_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ca50a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(61500.562, shape=(), dtype=float32)\n",
      "tf.Tensor(66899.2, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# compute dot product between first COCO food image and both generated images\n",
    "print(tf.tensordot(features_coco_food_flat, features_cog_food1_flat, 1))\n",
    "print(tf.tensordot(features_coco_food_flat, features_cog_food2_flat, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "546fa604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(43730.094, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# compute dot product between first COCO food image and skateboard+person+dog COCO image\n",
    "print(tf.tensordot(features_coco_food_flat, features_coco_skateboard_flat, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e066a9",
   "metadata": {},
   "source": [
    "--> We can see that again within category images have higher dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dot product between second COCO food image and "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mac-thesis",
   "language": "python",
   "name": "mac-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
