{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ca0979",
   "metadata": {},
   "source": [
    "## Explore Text2Image models\n",
    "\n",
    "This notebook explores the computation of image similarity as the dot product of embeddings computed based on the pretrained ResNet50 model (pretrained on ImageNet).\n",
    "This similarity computation is a necessary component for potential Text2Image based language drift metrics / models.\n",
    "\n",
    "More specifically, as a proof of concept, I checked whether the dot product of ResNet based embeddings for images from the same categories is higher than for images from different categories. This is important in order to check whether simple dot product computations based on ResNet embeddings are sensible as a similarity measure (in a loose sense of measure). However, this proof of concept is still contingent on the choice and pretraining of the Text2Image model.\n",
    "\n",
    "The exploration is as follows:\n",
    "* sample images are retrieved from the COCO dataset (the ones saved in the dataset_exploration notebook).\n",
    "* based on captions similar to the original captions, images are generated from the CogView Text2Image transformer model (a demonstration can be found [here](https://wudao.aminer.cn/CogView/index.html) and was used to generate the images) (Ding et al., 2021, see [here](https://pythonawesome.com/a-pretrained-transformer-for-text-to-image-generation-in-general-domain/)). \n",
    "    * this demo only works for Chinese text, so the sample input texts were translated into simplified Chinese using Google translate.\n",
    "* images from texts with unrelated categories were generated as distractors.    \n",
    "* a ResNet instance pretrained on ImageNet was loaded and embeddings were created for all the images.\n",
    "* the embeddings were normalized.\n",
    "* dot products were computed for different pairs of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8ced4",
   "metadata": {},
   "source": [
    "First, a COCO image containing the categories \"skateboard\", \"person\", \"dog\" was retrieved. Then, two sample images were generated from the CogView model. Then, the dot product for both embeddings is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40509311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b147b143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate pretrained ResNet model\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling=max)\n",
    "\n",
    "# load image from unrelated category\n",
    "# generated with input text \"a red bowl and an apple\"\n",
    "img_path = 'cog-view-apple.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "# compute embedding\n",
    "features_apple = model.predict(x)\n",
    "# flatten the embedding\n",
    "features_apple_flat = tf.reshape(features_apple, -1)\n",
    "features_apple_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075947ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load CogView sample for skateboard+person+dog\n",
    "# both generated from \"a man riding a skateboard with his dog\"\n",
    "img2_path = 'cog-view-sample2.jpg'\n",
    "img2 = image.load_img(img2_path, target_size=(224, 224))\n",
    "x2 = image.img_to_array(img2)\n",
    "x2 = np.expand_dims(x2, axis=0)\n",
    "x2 = preprocess_input(x2)\n",
    "# compute embedding\n",
    "features_cog_skateboard = model.predict(x2)\n",
    "features_cog_skateboard.shape\n",
    "features_cog_skateboard_flat = tf.reshape(features_cog_skateboard, -1)\n",
    "features_cog_skateboard_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f65052d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100352])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load original COCO image for skateboard+person+dog\n",
    "img3_path = 'person-skateboard-dog-coco.png'\n",
    "img3 = image.load_img(img3_path, target_size=(224, 224))\n",
    "x3 = image.img_to_array(img3)\n",
    "x3 = np.expand_dims(x3, axis=0)\n",
    "x3 = preprocess_input(x3)\n",
    "# compute embedding\n",
    "features_coco_skateboard = model.predict(x3)\n",
    "features_coco_skateboard.shape\n",
    "features_coco_skateboard_flat = tf.reshape(features_coco_skateboard, -1)\n",
    "features_coco_skateboard_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209e2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize embeddings to remove magnitude dependency\n",
    "features_coco_skateboard_norm, _ = tf.linalg.normalize(features_coco_skateboard_flat)\n",
    "features_cog_skateboard_norm, _ = tf.linalg.normalize(features_cog_skateboard_flat)\n",
    "features_apple_norm, _ = tf.linalg.normalize(features_apple_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35a8cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.08191117>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute dot product of the original COCO image and the CogView category-related image\n",
    "tf.tensordot(features_coco_skateboard_norm, features_cog_skateboard_norm, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71401ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.07616195>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute dot product of the original COCO image and the CogView category-UNRELATED image\n",
    "tf.tensordot(features_coco_skateboard_norm, features_apple_norm, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bfdba63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.9999972>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "tf.tensordot(features_coco_skateboard_norm, features_coco_skateboard_norm, 1)\n",
    "tf.tensordot(features_coco_skateboard_norm, -features_coco_skateboard_norm, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a8a26",
   "metadata": {},
   "source": [
    "--> We can see that the dor product is higher for the category related images.\n",
    "\n",
    "Now, the same procedure is applied to a second example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c58a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original COCO image for apple+sandwich+cup\n",
    "img4_path = 'apple-sandwich-cup-coco1.png'\n",
    "img4 = image.load_img(img4_path, target_size=(224, 224))\n",
    "x4 = image.img_to_array(img4)\n",
    "x4 = np.expand_dims(x4, axis=0)\n",
    "x4 = preprocess_input(x4)\n",
    "# compute embedding\n",
    "features_coco_food = model.predict(x4)\n",
    "features_coco_food_flat = tf.reshape(features_coco_food, -1)\n",
    "features_coco_food_norm, _ = tf.linalg.normalize(features_coco_food_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4f7c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load another COCO image for apple+sandwich+cup\n",
    "img5_path = 'apple-sandwich-cup-coco2.png'\n",
    "img5 = image.load_img(img5_path, target_size=(224, 224))\n",
    "x5 = image.img_to_array(img5)\n",
    "x5 = np.expand_dims(x5, axis=0)\n",
    "x5 = preprocess_input(x5)\n",
    "# compute embedding\n",
    "features_coco_food2 = model.predict(x5)\n",
    "features_coco_food2_flat = tf.reshape(features_coco_food2, -1)\n",
    "features_coco_food2_norm, _ = tf.linalg.normalize(features_coco_food2_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91245b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images generated from CogView for the respective categories\n",
    "# both generated from \"A meal with two plates and a cup of coffee\"\n",
    "img6_path = 'cog-view-food-sample1.jpg'\n",
    "img6 = image.load_img(img6_path, target_size=(224, 224))\n",
    "x6 = image.img_to_array(img6)\n",
    "x6 = np.expand_dims(x6, axis=0)\n",
    "x6 = preprocess_input(x6)\n",
    "# compute embedding\n",
    "features_cog_food1 = model.predict(x6)\n",
    "features_cog_food1_flat = tf.reshape(features_cog_food1, -1)\n",
    "features_cog_food1_norm, _ = tf.linalg.normalize(features_cog_food1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f15511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images generated from CogView for the respective categories\n",
    "img7_path = 'cog-view-food-sample2.jpg'\n",
    "img7 = image.load_img(img7_path, target_size=(224, 224))\n",
    "x7 = image.img_to_array(img7)\n",
    "x7 = np.expand_dims(x7, axis=0)\n",
    "x7 = preprocess_input(x7)\n",
    "# compute embedding\n",
    "features_cog_food2 = model.predict(x7)\n",
    "features_cog_food2_flat = tf.reshape(features_cog_food2, -1)\n",
    "features_cog_food2_norm, _ = tf.linalg.normalize(features_cog_food2_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa20a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.17764406, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15680446, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# compute dot product between first COCO food image and both generated images\n",
    "print(tf.tensordot(features_coco_food_norm, features_cog_food1_norm, 1))\n",
    "print(tf.tensordot(features_coco_food_norm, features_cog_food2_norm, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a726816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.12372634, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# compute dot product between first COCO food image and skateboard+person+dog COCO image\n",
    "print(tf.tensordot(features_coco_food_norm, features_coco_skateboard_norm, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1968e",
   "metadata": {},
   "source": [
    "--> We can see that again within category images have higher dot products. However, compared to to the previous example, we can see that the product of COCO images is realtively high, hinting at potentially high influence of low-level features like image quality etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc29b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.11234939, shape=(), dtype=float32)\n",
      "tf.Tensor(0.12537794, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# compute dot product between second COCO food image and both generated images\n",
    "print(tf.tensordot(features_coco_food2_norm, features_cog_food1_norm, 1))\n",
    "print(tf.tensordot(features_coco_food2_norm, features_cog_food2_norm, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdc401",
   "metadata": {},
   "source": [
    "--> We also see that the dot products are lower for the second COCO image, as this is the one where the searched categories play a secondary role while they are the main point of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87636b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mac-thesis",
   "language": "python",
   "name": "mac-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
