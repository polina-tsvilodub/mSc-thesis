{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0029df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets as dset\n",
    "from torchvision import transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9eb3b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd6788b1df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badac689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 300])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.empty((1, 25, 300))\n",
    "test_tensor[:, :test_tensor.shape[1]-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494757be",
   "metadata": {},
   "source": [
    "It seems that we do sort of a sliding window processing of the text examples because we can only do fixed length sequence processing in the LSTM, we use words 0 until k-1 as input, and want to predict words 1 until k as target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea82a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('../../../data/val/annotations/captions_val2014.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "802d6cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'images', 'licenses', 'annotations'])\n",
      "{}\n",
      "202654\n",
      "13155\n",
      "40504\n"
     ]
    }
   ],
   "source": [
    "print(coco.dataset.keys())\n",
    "print(coco.cats)\n",
    "print(len(coco.anns.keys()))\n",
    "print(list(coco.anns.keys())[:1000][549])\n",
    "print(len(coco.imgs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67b9fb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COCO_val2014_000000478766.jpg'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_id = coco.anns[157]['image_id']\n",
    "path = coco.loadImgs(img_id)[0]['file_name']\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bc56bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 480)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Required crop size (450, 450) is larger then input image size (256, 341)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-26156df5cf32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                          (0.229, 0.224, 0.225))])\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# transforms.Compose([transforms.Resize(256)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# print(image2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(img, output_size)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Required crop size {(th, tw)} is larger then input image size {(h, w)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtw\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Required crop size (450, 450) is larger then input image size (256, 341)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import numpy as np \n",
    "\n",
    "image1 = Image.open(os.path.join('../../../data/val/val2014', path)).convert('RGB')\n",
    "print(image1.size)\n",
    "# print(np.asarray(image1))\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, squeeze=False)\n",
    "ax = axs[0, 0]\n",
    "ax.imshow(np.asarray(image1))\n",
    "    \n",
    "# plt.axis('off')\n",
    "# plt.imshow(image)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.RandomCrop(450), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model, tuples for means and std for the three img channels\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "# transforms.Compose([transforms.Resize(256)])\n",
    "image2 = transform(image1)\n",
    "print(image2.size())\n",
    "# print(image2)\n",
    "# plt.imshow(image)\n",
    "ax = axs[0,1]\n",
    "ax.imshow(image2.permute(1, 2, 0))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c92f6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 31984,\n",
       " 'id': 13155,\n",
       " 'caption': 'A model posing next to a motorbike at a motorcycle show.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.anns[13155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b20e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Initialize pretrained Resnet 50.\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # remove the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.batch= nn.BatchNorm1d(embed_size,momentum = 0.01)\n",
    "        self.embed.weight.data.normal_(0., 0.02)\n",
    "        self.embed.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        print(features.size)\n",
    "        # reshape features to shape (300, -1) - adapt to first dim\n",
    "        features = features.view(features.size(0), -1)\n",
    "        print(\"features size after view: \", features.size)\n",
    "        print(features.shape)\n",
    "        features = self.batch(self.embed(features))\n",
    "        print(\"features after applying batch norm: \", features.size)\n",
    "        print(features.shape)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93caa8b",
   "metadata": {},
   "source": [
    "#### New architectural points / divergence from Lazaridou\n",
    "- the batchnorm layer on top of the resnet outputs\n",
    "- potentially: pretrained embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "045c7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the langauge module consisting of a one-layer LSTM, a dropout layer and \n",
    "        trainable embeddings. The image embedding is used as additional context at every step of the training \n",
    "        (prepended at the embedding beginning). \n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "#         _ = \"~/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\"\n",
    "#         self.fasttext = gensim.models.KeyedVectors.load_word2vec_format(_)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size= embed_size\n",
    "#         self.drop_prob= 0.2\n",
    "        self.vocabulary_size = vocab_size\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size , self.num_layers, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size) # .from_pretrained(\n",
    "#                             torch.FloatTensor(self.fasttext.vectors)\n",
    "#                         )\n",
    "        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    # how is the lstm initialized?\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" At the start of training, we need to initialize a hidden state;\n",
    "        there will be none because the hidden state is formed based on previously seen data.\n",
    "        So, this function defines a hidden state with all zeroes\n",
    "        The axes semantics are (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n",
    "                torch.zeros((1, batch_size, self.hidden_size), device=device))\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        # initialize hidden layer\n",
    "        # check if this isn't reinitializing the hidden state in the middle of the sequence\n",
    "#         self.hidden = self.init_hidden(self.hidden_size)\n",
    "        \n",
    "        embeddings = self.embed(captions)\n",
    "        print(\"LM embeddings shape original: \", embeddings.shape)\n",
    "        features = features.unsqueeze(1)\n",
    "        print(\"unsqueezed features: \", features.shape)\n",
    "        # take embeddings up to last word -- WHY --> apparently to avoid predicting further when END encountered\n",
    "        # PREpend the feature embedding as additional context\n",
    "        \n",
    "        \n",
    "        embeddings = torch.cat((features, embeddings[:, :-1,:]), dim=1) # embeddings[:, :-1,:]\n",
    "        print(\"embeddings shape after cat: \", embeddings.shape)\n",
    "        # where is the previous hidden state coming from?\n",
    "        hiddens, self.hidden = self.lstm(embeddings) # , self.hidden\n",
    "        print(\"Hidden LSTM shape: \", hiddens.shape)\n",
    "        print(\"Hidden LSTM unsqueezed shape: \", hiddens.unsqueeze(1).shape)\n",
    "        \n",
    "        outputs = self.linear(hiddens)\n",
    "        print(\"outputs shape: \", outputs.shape)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "        \n",
    "        \n",
    "        output = []\n",
    "        batch_size = inputs.shape[0] # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)\n",
    "        hidden = self.init_hidden(batch_size) # Get initial hidden state of the LSTM\n",
    "    \n",
    "        while True:\n",
    "            print(\"predicting next word\")\n",
    "            lstm_out, hidden = self.lstm(inputs, hidden) # lstm_out shape : (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
    "            outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n",
    "            _, max_indice = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)\n",
    "            print(\"Sampled max index: \", max_indice)\n",
    "#             output.append(max_indice.cpu().numpy()[0].item()) # storing the word predicted\n",
    "            output.append(max_indice)\n",
    "                          \n",
    "            if (max_indice == 1) or (len(output) == 10):\n",
    "                # We predicted the <end> word, so there is no further prediction to do\n",
    "                break\n",
    "            \n",
    "            ## Prepare to embed the last predicted word to be the new input of the lstm\n",
    "            inputs = self.embed(max_indice) # inputs shape : (1, embed_size)\n",
    "            inputs = inputs.unsqueeze(1) # inputs shape : (1, 1, embed_size)\n",
    "            print(\"Inputs at end of sampling step: \", inputs.shape, \" \", inputs)\n",
    "            \n",
    "        return output  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1a3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[7, 8],\n",
       "         [1, 2],\n",
       "         [3, 4]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1,2],[3,4],[5,6]]])\n",
    "y = torch.tensor([[[7,8]]])\n",
    "print(x.shape)\n",
    "print(x[:, :-1, :])\n",
    "\n",
    "torch.cat((y, x[:, :-1, :]), dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506ea47",
   "metadata": {},
   "source": [
    "Evaluate BPE tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741503a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4ab90f90f33c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfasttext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fasttext-wiki-news-subwords-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "# load pretrained gensim fasttext embeddings (try these because taking into account subword info might be important here)\n",
    "# import gensim.downloader\n",
    "\n",
    "# fasttext = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f041c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(fasttext)\n",
    "len(fasttext.key_to_index)\n",
    "fasttext.index_to_key[:10]\n",
    "fasttext_dict = dict()\n",
    "fasttext_dict = dict(zip(np.arange(len(fasttext.index_to_key)), fasttext.index_to_key))\n",
    "type(fasttext_dict)\n",
    "# fasttext[\"Hi\"]\n",
    "# .get_vecattr(key, attr), .set_vecattr(key,attr,new_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4db86e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext_path = \"~/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\"\n",
    "# fasttext_loaded = gensim.models.KeyedVectors.load_word2vec_format(fasttext_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04efd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0203, -0.0123, -0.0076,  ..., -0.0154,  0.0306,  0.0047],\n",
       "        [ 0.0242,  0.0048,  0.0184,  ..., -0.0350,  0.0184,  0.0035],\n",
       "        [ 0.0049, -0.0030,  0.0672,  ..., -0.0458, -0.0115,  0.0011],\n",
       "        ...,\n",
       "        [-0.0054,  0.0103,  0.0093,  ...,  0.0111,  0.0201,  0.0094],\n",
       "        [-0.0342, -0.0159, -0.0326,  ..., -0.0076, -0.0235,  0.0224],\n",
       "        [ 0.0364,  0.0182, -0.0025,  ..., -0.0063,  0.0009,  0.0487]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_loaded.vectors.shape\n",
    "torch_fasttext = torch.FloatTensor(fasttext_loaded.vectors)\n",
    "torch_fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8aa378",
   "metadata": {},
   "source": [
    "At which point do i decrease the vocab size? at pretraining already?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134c2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import pickle\n",
    "import os.path\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter\n",
    "from torchtext.data import get_tokenizer\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self,\n",
    "        vocab_threshold,\n",
    "        vocab_file='./vocab.pkl',\n",
    "        start_word=\"START\",\n",
    "        end_word=\"END\",\n",
    "        unk_word=\"UNK\",         \n",
    "        annotations_file=\"captions_val2014.json\",\n",
    "        pad_word=\"PAD\",\n",
    "        vocab_from_file=False):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "          vocab_threshold: Minimum word count threshold.\n",
    "          vocab_file: File containing the vocabulary.\n",
    "          start_word: Special word denoting sentence start.\n",
    "          end_word: Special word denoting sentence end.\n",
    "          unk_word: Special word denoting unknown words.\n",
    "          annotations_file: Path for train annotation file.\n",
    "          vocab_from_file: If False, create vocab from scratch & override any existing vocab_file\n",
    "                           If True, load vocab from from existing vocab_file, if it exists\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.pad_word = pad_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "#         _fasttext_path = \"~/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\"\n",
    "#         self._fasttext = gensim.models.KeyedVectors.load_word2vec_format(_fasttext_path)\n",
    "        \n",
    "#         self.fasttext_word2idx = _fasttext.key_to_index\n",
    "#         self.fasttext_idx2word = dict(zip(np.arange(len(_fasttext.index_to_key)), _fasttext.index_to_key))\n",
    "        \n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.init_vocab()\n",
    "        # everything below irrelevant, if using pretrained vocab and embeddings \n",
    "        \n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_word(self.pad_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.word2idx = {} #self._fasttext.key_to_index #{}\n",
    "        self.idx2word = {} #self._fasttext.index_to_key #{}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "#             try:\n",
    "#                 self.word2idx[word] = self.fasttext_word2idx[word]\n",
    "#                 self.idx2word[self.fasttext_word2idx[word]] = word\n",
    "#             except KeyError:\n",
    "#                 self.word2idx[word] = self.fasttext_word2idx[\"UNK\"]\n",
    "#                 self.idx2word[self.fasttext_word2idx[\"UNK\"]] = \"UNK\"\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(ids):\n",
    "            caption = str(coco.anns[id]['caption'])\n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            tokens = tokenizer(caption.lower()) # nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7999f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "# build a batch generator which takes ones caption out of the five at random\n",
    "class COCOCaptionsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom class for preprocessing datapoints and sampling a random caption per image.\n",
    "    \"\"\"\n",
    "    def __init__(self, file, download_dir, img_transform, text_transform, batch_size, mode, \n",
    "                 vocab_threshold, vocab_file, start_token, end_token, unk_token, pad_token, \n",
    "                vocab_from_file, max_sequence_length=0):\n",
    "        self.transform = img_transform\n",
    "        self.mode = mode # train or test or val\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, \n",
    "                                start_token, end_token, unk_token, file, pad_token, vocab_from_file) # class for instantiating the vocab object elsewhere\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # some distinctions below for Train and test mode\n",
    "        if mode == \"train\":\n",
    "            self.image_dir = os.path.join(download_dir, \"train2014\")\n",
    "            self.coco = COCO(file) # os.path.join(download_dir, file)\n",
    "            _ids = list(self.coco.anns.keys())\n",
    "            shuffle(_ids)\n",
    "            self.ids = _ids[:1000]\n",
    "            print('Obtaining caption lengths...')\n",
    "            tokenizer = get_tokenizer(\"basic_english\") # nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower())\n",
    "            all_tokens = [tokenizer(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in np.arange(len(self.ids))] # tqdm(np.arange(len(self.ids)))\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "            # get maximum caption length for padding\n",
    "            self.max_caption_length = max(self.caption_lengths)\n",
    "            \n",
    "            # print pretraining IDs for later separation from functional training\n",
    "            with open(\"pretrain_img_IDs.txt\", 'w') as f:\n",
    "                f.write(\",\".join([str(i) for i in self.ids]))\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            self.image_dir = os.path.join(download_dir, \"val2014\")\n",
    "            self.coco = COCO(file) # os.path.join(download_dir, file)\n",
    "            self.ids = list(self.coco.anns.keys())#[:1000]\n",
    "            print('Obtaining caption lengths...')\n",
    "            tokenizer = get_tokenizer(\"basic_english\") # nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower())\n",
    "            all_tokens = [tokenizer(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in np.arange(len(self.ids))] # tqdm(np.arange(len(self.ids)))\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "            # get maximum caption length for padding\n",
    "        else:\n",
    "            \n",
    "            # no annotations here \n",
    "            test_info = json.loads(open(file).read()) # os.path.join(download_dir, file)\n",
    "            self.paths = [item['file_name'] for item in test_info['images']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.mode != \"test\":\n",
    "            return len(self.ids) # TODO: check if this is what we want -- it returns the number of captions, not unique imgs\n",
    "        else:\n",
    "            print(\"Test ds length: \", len(self.paths))\n",
    "            return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return an image-caption tuple. The caption is sampled from the list of five captions/image at random.\n",
    "        \n",
    "        Arguments:\n",
    "        -------\n",
    "        idx: int\n",
    "            Index of the item to be returned.\n",
    "        \"\"\"\n",
    "        # TODO watch out that the same image-caption pair isn't used too often\n",
    "        \n",
    "        # TODO: below is rough logic, but check how to get the item at a specific instance since this is a Dataset object\n",
    "#         selected_file_img, selected_file_cap = self.data[idx]\n",
    "        # as can be seen above, the Dataset loader already given img-caption tuples -- check how to combine this with an index\n",
    "#         cap_idx = random.sample(list(range(0,len(self.data))), 1)[0]\n",
    "#         target_cap = selected_file_cap[cap_idx]\n",
    "#         return (selected_file_img, target_cap)\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode != 'test':\n",
    "            ann_id = self.ids[idx]\n",
    "            caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.image_dir, path)).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # TODO check if any other preprocessing of the caption needs to be performed\n",
    "            \n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            # TODO possibly shorten too long captions\n",
    "            tokens = tokenizer(str(caption).lower())\n",
    "            # Convert caption to tensor of word ids, append start and end tokens.\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            \n",
    "            # check if the sequence needs to be padded or truncated\n",
    "#             if self.max_sequence_length == 0:\n",
    "#                 while len(tokens) < self.max_caption_length:\n",
    "#                     tokens.append(self.pad_token)\n",
    "#             else:\n",
    "#                 while len(tokens) < self.max_sequence_length:\n",
    "#                     tokens.append(self.pad_token)\n",
    "#                 tokens = tokens[:self.max_sequence_length]\n",
    "                \n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "#             print(\"Indexed caption: \", caption)\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[idx]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.image_dir, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "        \n",
    "    def get_train_indices(self):\n",
    "        \"\"\"\n",
    "        Return a list of indices at which the captions have the same length which was sampled at random \n",
    "        for the given batch.\n",
    "        \"\"\"\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "#         print(\"Sel length: \", sel_length)\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "#         print(\"all indices: \", all_indices)\n",
    "#         all_indices = list(np.arange(len(self.caption_lengths)))\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        \n",
    "#         print(\"Indices: \", indices)\n",
    "        return indices\n",
    "    \n",
    "    def get_func_train_indices(self):\n",
    "        \"\"\"\n",
    "        Simple POC function returning two lists on indices, with captions of different lengths for  \n",
    "        targets and distractors.\n",
    "        \"\"\"\n",
    "        sel_length_t = np.random.choice(self.caption_lengths)\n",
    "#         sel_length_d = np.random.choice(self.caption_lengths)\n",
    "#         while sel_length_d == sel_length_t:\n",
    "#             sel_length_d = np.random.choice(self.caption_lengths)\n",
    "#         print(\"Sel length: \", sel_length)\n",
    "        all_indices_t = np.where([self.caption_lengths[i] == sel_length_t for i in np.arange(len(self.caption_lengths))])[0]\n",
    "#         print(\"all indices: \", all_indices)\n",
    "#         all_indices = list(np.arange(len(self.caption_lengths)))\n",
    "        indices = list(np.random.choice(all_indices_t, size=(self.batch_size)*2))\n",
    "        indices_t = indices[:self.batch_size]\n",
    "        indices_d = indices[self.batch_size:]\n",
    "        print(\"LENGTHS: \", len(indices_t), \" \", len(indices_d))\n",
    "#         all_indices_d = np.where([self.caption_lengths[i] == sel_length_d for i in np.arange(len(self.caption_lengths))])[0]\n",
    "#         indices_d = list(np.random.choice(all_indices_d, size=self.batch_size))\n",
    "        \n",
    "        return list(zip(indices_t, indices_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f2de52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_tensor.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb205006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(transform,\n",
    "               mode='val',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"START\",\n",
    "               end_word=\"END\",\n",
    "               unk_word=\"UNK\",\n",
    "               pad_word=\"PAD\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               download_dir=\"../../../data/val/\",\n",
    "#                cocoapi_loc='/Users/bjartesunde/Dropbox/Udacity/Computer Vision Nanodegree/computer-vision-ND/project_2_image_captioning_project/cocoapi'\n",
    "              ):\n",
    "    \"\"\"Returns the data loader.\n",
    "    Args:\n",
    "      transform: Image transform.\n",
    "      mode: One of 'train' or 'test'.\n",
    "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: Minimum word count threshold.\n",
    "      vocab_file: File containing the vocabulary. \n",
    "      start_word: Special word denoting sentence start.\n",
    "      end_word: Special word denoting sentence end.\n",
    "      unk_word: Special word denoting unknown words.\n",
    "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
    "                       If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: Number of subprocesses to use for data loading \n",
    "      cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test', 'val'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train' or mode=='val', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'val':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json')\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"train2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") #'test2014/'\n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json') #image_info_test2014\n",
    "\n",
    "    # COCO caption dataset.\n",
    "    dataset = COCOCaptionsDataset(\n",
    "        file=annotations_file,\n",
    "        download_dir = download_dir, \n",
    "        img_transform=transform,\n",
    "        text_transform=None,\n",
    "        batch_size=batch_size,\n",
    "        mode=mode,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_token=start_word,\n",
    "        end_token=end_word,\n",
    "        unk_token=unk_word,\n",
    "        pad_token=pad_word, \n",
    "        vocab_from_file=vocab_from_file,\n",
    "        max_sequence_length=25,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=torch.utils.data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False))\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                           \n",
    "           batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "#                                       num_workers=num_workers\n",
    "                                                 )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be12263",
   "metadata": {},
   "source": [
    "trimming / padding -- check conceptually. For now, max caption length treated as target length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3cbd83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/202654] Tokenizing captions...\n",
      "[100000/202654] Tokenizing captions...\n",
      "[200000/202654] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n",
      "Total number of tokens in vocabulary: 5914\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "vocab_threshold = 6\n",
    "batch_size = 128\n",
    "transform_train = transforms.Resize(256)\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='val',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False,\n",
    "                        )\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cc7b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max caption length:  56\n",
      "Average caption length in words:  11.307761011379\n"
     ]
    }
   ],
   "source": [
    "print(\"Max caption length: \", data_loader.dataset.max_caption_length)\n",
    "print(\"Average caption length in words: \", sum(data_loader.dataset.caption_lengths)/len(data_loader.dataset.caption_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f114c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 42089),\n",
       " (11, 39220),\n",
       " (9, 34922),\n",
       " (12, 28137),\n",
       " (13, 18356),\n",
       " (14, 11081),\n",
       " (8, 10202),\n",
       " (15, 6562),\n",
       " (16, 3799),\n",
       " (17, 2410),\n",
       " (18, 1506),\n",
       " (19, 995),\n",
       " (7, 817),\n",
       " (20, 676),\n",
       " (21, 495),\n",
       " (22, 324),\n",
       " (23, 238),\n",
       " (24, 214),\n",
       " (25, 131),\n",
       " (26, 89),\n",
       " (27, 66),\n",
       " (28, 56),\n",
       " (29, 41),\n",
       " (30, 39),\n",
       " (31, 28),\n",
       " (32, 19),\n",
       " (34, 14),\n",
       " (33, 13),\n",
       " (40, 12),\n",
       " (35, 12),\n",
       " (37, 11),\n",
       " (38, 10),\n",
       " (45, 9),\n",
       " (36, 9),\n",
       " (43, 8),\n",
       " (39, 6),\n",
       " (46, 6),\n",
       " (48, 5),\n",
       " (50, 4),\n",
       " (47, 4),\n",
       " (53, 3),\n",
       " (6, 3),\n",
       " (44, 3),\n",
       " (41, 3),\n",
       " (51, 2),\n",
       " (52, 1),\n",
       " (42, 1),\n",
       " (54, 1),\n",
       " (56, 1),\n",
       " (49, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(data_loader.dataset.caption_lengths).items(), key=lambda pair: pair[1], reverse=True)\n",
    "# --> cut off at 25 tokens, pad to 25 the ones that are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8bc2e426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'START': 39235,\n",
       " 'END': 44803,\n",
       " 'UNK': 77808,\n",
       " 'PAD': 52948,\n",
       " 'a': 7,\n",
       " 'bicycle': 10888,\n",
       " 'replica': 19413,\n",
       " 'with': 17,\n",
       " 'clock': 5154,\n",
       " 'as': 18,\n",
       " 'the': 1,\n",
       " 'front': 1586,\n",
       " 'wheel': 6203,\n",
       " '.': 2,\n",
       " 'black': 882}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(data_loader.dataset.vocab.word2idx.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37755687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "# from data_loader import get_loader\n",
    "# from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 64         # batch size\n",
    "vocab_threshold = 1        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 1024           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 200          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.lstm.parameters()) + list(decoder.linear.parameters()) + list(encoder.embed.parameters()) + list(encoder.batch.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "# optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e97dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2054,  0.7625, -0.7968, -0.1389],\n",
      "        [-0.5942,  0.7384, -0.1585,  0.2094],\n",
      "        [ 0.2012,  1.1229,  1.1435, -1.9726],\n",
      "        [ 1.8615,  1.1492,  1.0917, -1.0409]])\n",
      "tensor([[-0.2054,  0.7625, -0.7968],\n",
      "        [-0.5942,  0.7384, -0.1585],\n",
      "        [ 0.2012,  1.1229,  1.1435],\n",
      "        [ 1.8615,  1.1492,  1.0917]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2054,  0.7625, -0.7968],\n",
       "        [-0.5942,  0.7384, -0.1585],\n",
       "        [ 0.2012,  1.1229,  1.1435],\n",
       "        [ 1.8615,  1.1492,  1.0917]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x)\n",
    "print(x[:, :-1])\n",
    "x[:, :x.shape[1]-1]\n",
    "# print(x.size())\n",
    "# print(x.shape)\n",
    "# y = x.view(16)\n",
    "# print(y.size())\n",
    "# print(y.shape)\n",
    "# z = x.view(-1)  # the size -1 is inferred from other dimensions\n",
    "# print(z.size())\n",
    "# print(z.shape)\n",
    "\n",
    "# a = torch.randn(1, 2, 3, 4)\n",
    "# a.size()\n",
    "# b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
    "# b.size()\n",
    "# c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
    "# c.size()\n",
    "# torch.equal(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc4e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-test.pkl' ))\n",
    "torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-test.pkl' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719a2729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "Test ds length:  40504\n",
      "Test ds length:  40504\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "transform_test = transforms.Compose([transforms.Resize((224, 224)), \\\n",
    "                                     transforms.ToTensor(), \\\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), \\\n",
    "                                                          (0.229, 0.224, 0.225))])\n",
    "data_loader_test = get_loader(transform=transform_test,    \n",
    "                         mode='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6587089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ds length:  40504\n",
      "Test ds length:  40504\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "orig_image, image = next(iter(data_loader_test))\n",
    "\n",
    "# Visualize sample image, before pre-processing.\n",
    "plt.imshow(np.squeeze(orig_image))\n",
    "plt.title('example image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9e7d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 512])   tensor([[[ 0.0275, -0.1055,  0.0283, -0.0303,  0.1047,  0.0389,  0.0570,\n",
      "           0.0558,  0.0180, -0.0252, -0.0077,  0.0940,  0.0687, -0.0621,\n",
      "          -0.0146, -0.0147,  0.0210,  0.0748,  0.0172, -0.0930, -0.0848,\n",
      "           0.0877,  0.0950, -0.0789,  0.0941, -0.0880,  0.0539,  0.0089,\n",
      "          -0.0291,  0.0903, -0.0564, -0.0306, -0.0757, -0.0999,  0.0364,\n",
      "          -0.0858,  0.0714,  0.0193, -0.0731, -0.0257,  0.0198, -0.0568,\n",
      "          -0.0782,  0.0462,  0.0975,  0.0955, -0.0401,  0.0169, -0.0339,\n",
      "          -0.0688,  0.0629,  0.0481, -0.0331,  0.0544,  0.0782, -0.0971,\n",
      "           0.0579,  0.0271,  0.0602,  0.0571,  0.0890, -0.0478, -0.0903,\n",
      "           0.0136,  0.0592,  0.1003, -0.0701, -0.0591, -0.0016,  0.0470,\n",
      "          -0.0110, -0.0484, -0.0384, -0.0359, -0.0733, -0.0858,  0.0858,\n",
      "           0.0469,  0.0783, -0.0280, -0.0642, -0.0725,  0.0269, -0.0145,\n",
      "           0.0624,  0.0879,  0.0769, -0.0301, -0.0883,  0.1018, -0.0718,\n",
      "          -0.0374,  0.0851, -0.0848,  0.0887,  0.0221,  0.0239,  0.0215,\n",
      "          -0.0767, -0.0408,  0.0392,  0.0530,  0.0597,  0.0118, -0.0979,\n",
      "          -0.0719, -0.0437,  0.0630, -0.0516,  0.1082, -0.0745,  0.0194,\n",
      "           0.0476, -0.0509,  0.0675, -0.0354,  0.0248,  0.0727, -0.0904,\n",
      "          -0.0920,  0.0132, -0.0797,  0.0768,  0.0352, -0.0427,  0.0133,\n",
      "           0.0063,  0.0237,  0.0878, -0.0337, -0.0609, -0.1057, -0.0299,\n",
      "           0.0683, -0.0967,  0.0939,  0.0285, -0.0207,  0.0327, -0.0250,\n",
      "           0.0527, -0.0257, -0.0992,  0.0330,  0.0895, -0.0368, -0.1026,\n",
      "          -0.0040,  0.0270,  0.0633, -0.0623,  0.0130,  0.0427,  0.0887,\n",
      "          -0.0363, -0.0226,  0.1047, -0.0700,  0.0344, -0.0482,  0.0686,\n",
      "          -0.0252,  0.0559, -0.0344,  0.0988, -0.0591,  0.0389, -0.0673,\n",
      "          -0.0651,  0.0854, -0.0015,  0.0757, -0.0495,  0.0898,  0.0170,\n",
      "           0.0324, -0.1059, -0.0866,  0.0870, -0.1014,  0.0328,  0.0139,\n",
      "          -0.0460, -0.0141,  0.0798,  0.0527, -0.0301, -0.0946, -0.0940,\n",
      "          -0.0058, -0.0781,  0.0651,  0.0138,  0.0109,  0.0253,  0.0931,\n",
      "           0.0443, -0.0217, -0.0089,  0.0339, -0.0482, -0.0432,  0.0493,\n",
      "           0.0902,  0.0309, -0.0167, -0.0108,  0.0776, -0.0519,  0.0932,\n",
      "           0.0964, -0.0358,  0.0135,  0.0804,  0.0153, -0.0184,  0.0853,\n",
      "          -0.0598,  0.0720, -0.0926, -0.0123, -0.0258,  0.0100, -0.0505,\n",
      "           0.0430, -0.0857, -0.0607, -0.0324,  0.0994,  0.0187, -0.0512,\n",
      "          -0.0802,  0.0980, -0.0022, -0.0884,  0.0336, -0.0098, -0.0599,\n",
      "          -0.0054,  0.0933,  0.0974,  0.0329, -0.0376, -0.0532,  0.0882,\n",
      "          -0.1033, -0.0075,  0.0658, -0.0719, -0.0439, -0.0840, -0.0306,\n",
      "          -0.0510, -0.0319,  0.0514,  0.0852, -0.0644,  0.0883,  0.0160,\n",
      "           0.0150,  0.0134, -0.0774,  0.0233,  0.0376, -0.0921, -0.0796,\n",
      "          -0.0369,  0.0507, -0.0246, -0.0658,  0.0193, -0.0569, -0.0608,\n",
      "           0.0347, -0.0157, -0.0293,  0.0248,  0.1097,  0.0214,  0.0999,\n",
      "           0.0873,  0.0706, -0.0821, -0.0965, -0.0376, -0.0362,  0.1056,\n",
      "           0.0537, -0.0297, -0.0922,  0.0540,  0.0563, -0.0598,  0.0533,\n",
      "          -0.0741, -0.0357,  0.0567,  0.0181, -0.0726,  0.0602,  0.0050,\n",
      "           0.1032, -0.0200,  0.0533, -0.0330, -0.0303,  0.0373,  0.0151,\n",
      "           0.0590,  0.0517, -0.0139, -0.0765, -0.0435,  0.0686,  0.0428,\n",
      "           0.0525,  0.0218, -0.0498, -0.0402, -0.1077, -0.0200, -0.0811,\n",
      "          -0.0562,  0.0439,  0.0007,  0.0837, -0.0634, -0.0611,  0.0655,\n",
      "          -0.0402,  0.0776, -0.0737,  0.0388,  0.0436, -0.0165, -0.0591,\n",
      "          -0.0486,  0.0974,  0.0638,  0.1072,  0.0844,  0.0410, -0.0785,\n",
      "           0.0402, -0.0962, -0.0802,  0.0304,  0.0901, -0.0197, -0.0104,\n",
      "           0.0293, -0.0686,  0.1023, -0.0234,  0.0406, -0.0835,  0.0956,\n",
      "           0.0350, -0.0719,  0.0747,  0.0676,  0.0216, -0.0461, -0.0780,\n",
      "           0.0445,  0.0083,  0.1011,  0.0985, -0.0580, -0.0508,  0.0229,\n",
      "           0.0153,  0.0496, -0.1035, -0.0239, -0.0897,  0.0618, -0.0049,\n",
      "           0.1035,  0.0068,  0.0015,  0.0504,  0.0246, -0.0712,  0.0371,\n",
      "          -0.0684,  0.0808, -0.0864,  0.0225,  0.0552,  0.0362, -0.0950,\n",
      "          -0.0461, -0.0158,  0.0910,  0.0152, -0.1006,  0.0998, -0.0367,\n",
      "          -0.0413,  0.0140, -0.0307, -0.0204,  0.0306, -0.0895, -0.0508,\n",
      "          -0.0507,  0.0204,  0.0776, -0.0125, -0.0409, -0.0120, -0.0380,\n",
      "          -0.0382, -0.0647, -0.0515,  0.0822, -0.0522,  0.0201,  0.0757,\n",
      "           0.0787, -0.0647,  0.0919, -0.0664, -0.0225,  0.0575, -0.0404,\n",
      "           0.0941, -0.0573,  0.0953, -0.0448,  0.0537,  0.0501,  0.1010,\n",
      "           0.1010, -0.0675,  0.0873, -0.0909,  0.0345, -0.0514, -0.0891,\n",
      "           0.0542, -0.0615, -0.0301, -0.0965,  0.0808, -0.1026,  0.0208,\n",
      "           0.1054,  0.0812,  0.0635,  0.0584, -0.0672,  0.0756, -0.0844,\n",
      "          -0.0584,  0.0496, -0.0241, -0.0285,  0.0492,  0.0727, -0.0449,\n",
      "          -0.0341,  0.0069,  0.1053, -0.0960,  0.0462,  0.0097,  0.1096,\n",
      "          -0.0642, -0.0301, -0.0162,  0.0141, -0.0061,  0.0524, -0.0504,\n",
      "           0.0134,  0.0670, -0.0776, -0.0698, -0.0777,  0.0508,  0.0718,\n",
      "          -0.0052, -0.0215, -0.0128,  0.0968, -0.0378,  0.0802, -0.0332,\n",
      "           0.0939,  0.0349, -0.0199,  0.0646, -0.0990, -0.0093, -0.0546,\n",
      "          -0.0418, -0.0582,  0.1024,  0.0466, -0.0654, -0.0758, -0.0707,\n",
      "           0.1065,  0.0258,  0.0405, -0.0820,  0.1001, -0.0960,  0.0728,\n",
      "           0.0651]]], grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([2])\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# def get_prediction():\n",
    "#     orig_image, image = next(iter(data_loader))\n",
    "#     plt.imshow(np.squeeze(orig_image))\n",
    "#     plt.title('Sample Image')\n",
    "#     plt.show()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "image = image.to(device)\n",
    "features = encoder(image).unsqueeze(1)\n",
    "output = decoder.sample(features)    \n",
    "sentence = clean_sentence(output)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7d6a148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (lstm): LSTM(512, 512, batch_first=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (embed): Embedding(5913, 512)\n",
       "  (linear): Linear(in_features=512, out_features=5913, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_file = 'encoder-1.pkl' \n",
    "decoder_file = 'decoder-1.pkl'\n",
    "\n",
    "# TODO #3: Select appropriate values for the Python variables below.\n",
    "embed_size = 512\n",
    "visual_embed_size = 1024\n",
    "hidden_size = 512\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder, and set each to inference mode.\n",
    "encoder = EncoderCNN(visual_embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "\n",
    "# Load the trained weights.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "\n",
    "# Move models to GPU if CUDA is available.\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "18547c8b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape:  torch.Size([1, 3, 224, 224])\n",
      "features.shape:  torch.Size([1, 1, 512])\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-daf93b7f6b1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Pass the embedded image features through the model to get a predicted caption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example output:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-032006a94b25>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lstm_out shape : (1, 1, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# outputs shape : (1, 1, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# outputs shape : (1, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/msc-thesis/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 762\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image = image.to(device)\n",
    "\n",
    "# Obtain the embedded image features.\n",
    "print(\"image.shape: \", image.shape)\n",
    "features = encoder(image).unsqueeze(1)\n",
    "print(\"features.shape: \", features.shape)\n",
    "print()\n",
    "\n",
    "# Pass the embedded image features through the model to get a predicted caption.\n",
    "output = decoder.sample(features)\n",
    "print('example output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19e11d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(output):\n",
    "    list_string = []\n",
    "    \n",
    "    for idx in output:\n",
    "        list_string.append(data_loader.dataset.vocab.idx2word[idx])\n",
    "    \n",
    "    list_string = list_string[1:-1] # Discard <start> and <end> words\n",
    "    sentence = ' '.join(list_string) # Convert list of string to full string\n",
    "    sentence = sentence.capitalize()  # Capitalize the first letter of the first word\n",
    "    return sentence\n",
    "\n",
    "# sentence = clean_sentence(output)\n",
    "# print('example sentence:\\n', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d523e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n",
      "18258\n"
     ]
    }
   ],
   "source": [
    "# training set up \n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 128         # batch size\n",
    "vocab_threshold = 1        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 1024           # dimensionality of word embeddings\n",
    "visual_embed_size = 1024   # dimensionality of the image embeding\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 200          # determines window for printing average loss\n",
    "log_file = 'training_log_train.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                         download_dir=\"../../../data/train\", \n",
    "                        )\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "995b2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(visual_embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.lstm.parameters()) + list(decoder.linear.parameters()) + list(encoder.embed.parameters()) + list(encoder.batch.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
    "# optimizer = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "783b112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d40cb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loop captions shape:  torch.Size([128, 10])\n",
      "<built-in method size of Tensor object at 0x7fc6e989ac50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6e989a590>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619373ad0>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 10, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 10, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 10, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 10, 512])\n",
      "outputs shape:  torch.Size([128, 10, 18258])\n",
      "Outputs shape:  torch.Size([128, 10, 18258])\n",
      "Outputs:  tensor([[[ 4.5054e-02, -1.4228e-01,  2.6890e-01,  ..., -2.0962e-01,\n",
      "           1.9405e-01,  1.5298e-01],\n",
      "         [-2.2231e-02, -3.2317e-02,  1.3591e-01,  ..., -8.5766e-02,\n",
      "           1.1465e-01,  9.2433e-02],\n",
      "         [-1.7327e-02, -3.0984e-02,  9.1615e-02,  ..., -3.0661e-02,\n",
      "           7.8104e-02,  4.8843e-02],\n",
      "         ...,\n",
      "         [ 1.9353e-02, -1.1712e-02, -5.8150e-03,  ..., -1.1342e-02,\n",
      "           2.5009e-02,  9.2051e-03],\n",
      "         [-1.6427e-03, -3.2632e-02,  9.0634e-03,  ..., -1.3842e-02,\n",
      "           4.0743e-02,  6.9442e-04],\n",
      "         [-3.1179e-02, -1.9479e-02,  2.3423e-03,  ..., -3.3923e-02,\n",
      "           4.4709e-03, -1.2524e-02]],\n",
      "\n",
      "        [[-1.4589e-01, -8.2958e-02, -1.3116e-01,  ...,  1.8357e-01,\n",
      "           4.7633e-01, -4.8540e-01],\n",
      "         [-8.9981e-02, -3.2890e-02, -8.5930e-02,  ...,  1.1312e-01,\n",
      "           1.8270e-01, -1.7494e-01],\n",
      "         [-6.5535e-02,  3.7076e-04, -4.8464e-02,  ...,  7.2445e-02,\n",
      "           1.0445e-01, -1.0220e-01],\n",
      "         ...,\n",
      "         [-3.4915e-02, -3.6080e-02, -2.1630e-02,  ..., -4.6986e-02,\n",
      "           1.7631e-02, -3.9883e-03],\n",
      "         [-2.5030e-02, -3.6752e-02, -3.5250e-02,  ..., -7.8643e-02,\n",
      "           3.7297e-02,  2.1796e-02],\n",
      "         [-5.6414e-03, -4.1582e-02, -1.8128e-02,  ..., -6.0456e-02,\n",
      "           2.4389e-02,  5.4304e-03]],\n",
      "\n",
      "        [[-2.4442e-02,  8.8132e-02,  2.6283e-01,  ...,  2.3902e-01,\n",
      "          -7.8132e-02,  1.3964e-01],\n",
      "         [-1.4085e-02, -2.4831e-03,  1.7864e-01,  ...,  5.3142e-02,\n",
      "          -5.3643e-03,  1.0856e-01],\n",
      "         [-2.6240e-02, -6.0786e-02,  9.4859e-02,  ..., -1.8846e-02,\n",
      "          -7.0525e-03,  3.3362e-02],\n",
      "         ...,\n",
      "         [ 9.2595e-03, -6.1434e-02,  2.0810e-02,  ..., -1.8978e-02,\n",
      "           3.8363e-02, -5.9801e-03],\n",
      "         [-6.5519e-03, -6.0818e-02,  2.3106e-02,  ..., -1.8379e-02,\n",
      "           4.8887e-02, -1.0391e-02],\n",
      "         [-2.9731e-02, -6.3828e-02,  4.0061e-02,  ..., -2.6648e-02,\n",
      "           3.2015e-02, -6.9585e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6029e-01, -1.7044e-01, -2.9503e-01,  ..., -1.2131e-01,\n",
      "          -1.9429e-01,  3.6050e-01],\n",
      "         [-1.1369e-01, -7.8107e-02, -1.1446e-01,  ..., -1.3840e-01,\n",
      "          -6.5290e-02,  1.4881e-01],\n",
      "         [-5.6421e-02, -4.3129e-02, -3.2749e-02,  ..., -8.6942e-02,\n",
      "          -1.6646e-02,  8.5691e-02],\n",
      "         ...,\n",
      "         [-3.3588e-02, -3.3735e-02,  2.5432e-04,  ..., -3.6186e-02,\n",
      "           2.1675e-02, -4.0101e-02],\n",
      "         [-2.7610e-02, -4.5227e-02,  1.0766e-02,  ..., -2.0812e-02,\n",
      "           4.0213e-02, -2.8062e-02],\n",
      "         [-1.4228e-02, -1.1076e-02, -9.5315e-03,  ..., -2.2418e-02,\n",
      "           2.3794e-02,  3.4435e-03]],\n",
      "\n",
      "        [[ 1.6104e-02,  7.1097e-02,  1.1504e-01,  ...,  2.3779e-02,\n",
      "          -2.1344e-02,  1.2262e-01],\n",
      "         [-2.7707e-03, -3.0594e-02,  1.1272e-01,  ..., -4.1659e-02,\n",
      "          -1.2990e-02,  9.2801e-02],\n",
      "         [-1.6319e-02, -6.0753e-02,  5.5674e-02,  ..., -7.6466e-02,\n",
      "          -9.4269e-03,  2.9141e-02],\n",
      "         ...,\n",
      "         [ 1.0540e-02, -5.8698e-02,  2.0617e-02,  ..., -2.1941e-02,\n",
      "           4.0197e-02, -4.0631e-03],\n",
      "         [-5.8575e-03, -5.9492e-02,  2.3378e-02,  ..., -2.0081e-02,\n",
      "           5.0209e-02, -8.9529e-03],\n",
      "         [-2.9403e-02, -6.3213e-02,  4.0519e-02,  ..., -2.7491e-02,\n",
      "           3.3045e-02, -5.9840e-03]],\n",
      "\n",
      "        [[ 3.3296e-02, -1.6348e-01,  1.8043e-01,  ...,  1.3320e-01,\n",
      "          -2.5626e-03,  2.3525e-01],\n",
      "         [ 9.8575e-02, -1.6023e-01,  1.0746e-01,  ...,  8.6317e-02,\n",
      "           6.2152e-02,  1.0528e-01],\n",
      "         [ 4.7265e-02, -1.1041e-01,  7.1576e-02,  ...,  3.2354e-02,\n",
      "           2.6984e-02,  5.4740e-02],\n",
      "         ...,\n",
      "         [-4.0159e-02, -2.1214e-02,  1.5154e-02,  ..., -4.8328e-02,\n",
      "           4.4017e-02, -3.1272e-03],\n",
      "         [-3.9602e-02, -2.8305e-02,  1.8906e-02,  ..., -3.6854e-02,\n",
      "           1.9348e-02,  1.4749e-02],\n",
      "         [-3.7268e-02,  1.0391e-02, -6.1517e-03,  ..., -3.8622e-02,\n",
      "          -6.0889e-04,  3.0347e-02]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1280])\n",
      "Outputs shape:  torch.Size([1280, 18258])\n",
      "Loss tensor(9.8182, grad_fn=<NllLossBackward0>)\n",
      "Epoch [1/1], Step [1/8], Loss: 9.8182, Perplexity: 18365.5791Training Loop captions shape:  torch.Size([128, 11])\n",
      "<built-in method size of Tensor object at 0x7fc73b642bf0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b6429b0>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b6425f0>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 11, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 11, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 11, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 11, 512])\n",
      "outputs shape:  torch.Size([128, 11, 18258])\n",
      "Outputs shape:  torch.Size([128, 11, 18258])\n",
      "Outputs:  tensor([[[ 0.1565, -0.0236,  0.4078,  ..., -0.0546, -0.2071,  0.0839],\n",
      "         [-0.0759, -0.0540,  0.1132,  ..., -0.0042, -0.0700,  0.0540],\n",
      "         [-0.0805,  0.0110,  0.0507,  ..., -0.0262, -0.0405,  0.0341],\n",
      "         ...,\n",
      "         [-0.0220,  0.0915,  0.0057,  ..., -0.0314,  0.0767,  0.0082],\n",
      "         [-0.0361,  0.0783,  0.0109,  ..., -0.0343,  0.0176,  0.0105],\n",
      "         [-0.0549,  0.0897, -0.0019,  ..., -0.0350,  0.0660, -0.0076]],\n",
      "\n",
      "        [[ 0.6508,  0.0301,  0.1819,  ...,  0.2305,  0.0981,  0.2640],\n",
      "         [ 0.1134, -0.0371,  0.1514,  ...,  0.1595,  0.0306,  0.1248],\n",
      "         [ 0.0664, -0.0443,  0.1041,  ...,  0.0679,  0.0041,  0.0851],\n",
      "         ...,\n",
      "         [-0.0275,  0.0670,  0.0155,  ..., -0.0436, -0.0081, -0.0250],\n",
      "         [-0.0029,  0.0908,  0.0067,  ..., -0.0442,  0.0108, -0.0133],\n",
      "         [-0.0339,  0.0908, -0.0028,  ..., -0.0387,  0.0652, -0.0162]],\n",
      "\n",
      "        [[-0.1195, -0.0237,  0.0285,  ...,  0.3437, -0.1075, -0.1432],\n",
      "         [-0.0560,  0.0409,  0.0573,  ...,  0.1215, -0.0723, -0.0317],\n",
      "         [-0.0215,  0.0800,  0.0623,  ...,  0.0336, -0.0248, -0.0208],\n",
      "         ...,\n",
      "         [-0.0444,  0.1309,  0.0357,  ..., -0.0589,  0.0509, -0.0066],\n",
      "         [-0.0432,  0.1358,  0.0044,  ..., -0.0590,  0.0516, -0.0091],\n",
      "         [-0.0558,  0.1306, -0.0039,  ..., -0.0493,  0.0830, -0.0144]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.9161, -0.0386, -0.3664,  ...,  0.2416, -0.2345, -0.1546],\n",
      "         [ 0.3105, -0.0337, -0.2011,  ...,  0.0987, -0.1247, -0.0814],\n",
      "         [ 0.1143,  0.0143, -0.1070,  ...,  0.0198, -0.0528, -0.0196],\n",
      "         ...,\n",
      "         [ 0.0089,  0.1021, -0.0089,  ..., -0.0366,  0.0400, -0.0167],\n",
      "         [-0.0424,  0.0815,  0.0132,  ..., -0.0385,  0.0437, -0.0215],\n",
      "         [-0.0563,  0.0925, -0.0018,  ..., -0.0343,  0.0755, -0.0215]],\n",
      "\n",
      "        [[-0.0311, -0.2256, -0.0330,  ...,  0.0699, -0.1582,  0.1883],\n",
      "         [ 0.0039, -0.1218,  0.0477,  ..., -0.0243,  0.0440,  0.0863],\n",
      "         [ 0.0014, -0.0571,  0.0258,  ..., -0.0370,  0.0945,  0.0655],\n",
      "         ...,\n",
      "         [-0.0349,  0.1019,  0.0429,  ..., -0.0394,  0.0171,  0.0239],\n",
      "         [-0.0097,  0.0997,  0.0137,  ..., -0.0614,  0.0407,  0.0182],\n",
      "         [-0.0074,  0.0487,  0.0233,  ..., -0.0385,  0.0300,  0.0159]],\n",
      "\n",
      "        [[ 0.5173, -0.1919, -0.1133,  ..., -0.2593,  0.0770, -0.0662],\n",
      "         [ 0.1714, -0.1007, -0.0653,  ..., -0.1556,  0.0799,  0.0228],\n",
      "         [ 0.0702, -0.0154, -0.0201,  ..., -0.1034,  0.0718,  0.0118],\n",
      "         ...,\n",
      "         [-0.0379,  0.0827,  0.0253,  ..., -0.0474,  0.0344, -0.0010],\n",
      "         [-0.0292,  0.0768,  0.0161,  ..., -0.0500,  0.0411, -0.0164],\n",
      "         [-0.0500,  0.0911,  0.0026,  ..., -0.0412,  0.0814, -0.0190]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1408])\n",
      "Outputs shape:  torch.Size([1408, 18258])\n",
      "Loss tensor(9.7366, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2/8], Loss: 9.7366, Perplexity: 16926.7335Training Loop captions shape:  torch.Size([128, 15])\n",
      "<built-in method size of Tensor object at 0x7fc73b5faa10>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b5fae30>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b5faef0>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 15, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 15, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 15, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 15, 512])\n",
      "outputs shape:  torch.Size([128, 15, 18258])\n",
      "Outputs shape:  torch.Size([128, 15, 18258])\n",
      "Outputs:  tensor([[[ 4.2806e-01, -3.8644e-02,  1.2530e-01,  ...,  2.8029e-01,\n",
      "          -4.8951e-02,  6.0833e-02],\n",
      "         [-4.5508e-02, -1.6101e-02,  6.6150e-02,  ...,  7.4340e-02,\n",
      "          -5.4823e-02,  2.2388e-02],\n",
      "         [-8.8780e-02,  1.2862e-01,  5.7047e-02,  ...,  9.0464e-03,\n",
      "          -3.2390e-02,  7.8481e-03],\n",
      "         ...,\n",
      "         [-3.0270e-02,  2.6768e-01,  2.5266e-02,  ..., -4.3811e-02,\n",
      "           4.5569e-02, -9.4259e-03],\n",
      "         [-2.9769e-02,  2.3806e-01,  1.2738e-02,  ..., -3.3274e-02,\n",
      "           2.4584e-02, -7.1469e-03],\n",
      "         [-6.0213e-02,  3.1189e-01, -7.0847e-03,  ..., -3.6623e-02,\n",
      "           8.0733e-02, -1.5725e-02]],\n",
      "\n",
      "        [[ 1.5223e+00, -3.4414e-01, -3.3603e-01,  ..., -1.3984e-01,\n",
      "          -4.7382e-01,  3.7594e-01],\n",
      "         [ 5.4640e-01, -2.1941e-01, -8.9088e-02,  ..., -1.0180e-01,\n",
      "          -2.9920e-01,  1.6948e-01],\n",
      "         [ 2.6101e-01, -4.6892e-02, -2.2468e-02,  ..., -9.8318e-02,\n",
      "          -1.6399e-01,  8.4847e-02],\n",
      "         ...,\n",
      "         [ 6.1753e-03,  2.2106e-01,  7.2328e-03,  ..., -5.7025e-02,\n",
      "           1.7629e-02,  1.0680e-03],\n",
      "         [ 3.7850e-03,  2.3810e-01,  7.0578e-03,  ..., -5.2621e-02,\n",
      "           4.0082e-02, -1.9948e-02],\n",
      "         [-1.3295e-03,  2.4465e-01,  2.1618e-02,  ..., -5.6249e-02,\n",
      "           5.7981e-02, -2.1230e-02]],\n",
      "\n",
      "        [[ 6.4735e-01,  5.6839e-02,  1.0818e-01,  ...,  5.6626e-02,\n",
      "           7.5192e-02, -2.6216e-01],\n",
      "         [ 6.5293e-02,  6.1763e-02,  1.0654e-01,  ...,  7.6147e-02,\n",
      "           3.9384e-02, -1.3525e-01],\n",
      "         [ 1.1831e-02,  9.4813e-02,  8.8925e-02,  ...,  6.8238e-03,\n",
      "           2.8574e-02, -8.8798e-02],\n",
      "         ...,\n",
      "         [-3.9333e-02,  1.7674e-01,  2.0084e-02,  ..., -6.2528e-02,\n",
      "          -6.6086e-03, -1.4365e-02],\n",
      "         [-4.7705e-02,  1.7514e-01,  3.8962e-02,  ..., -5.3025e-02,\n",
      "           1.3696e-02, -9.1107e-03],\n",
      "         [-7.1439e-02,  2.6540e-01,  1.3456e-02,  ..., -5.1164e-02,\n",
      "           6.9053e-02, -1.6231e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.5785e+00, -3.3771e-01, -2.0546e-01,  ...,  7.1054e-02,\n",
      "          -2.9261e-01,  3.4124e-01],\n",
      "         [ 8.5076e-01, -2.8635e-01, -4.3244e-02,  ..., -7.2239e-02,\n",
      "          -1.4976e-01,  9.9396e-02],\n",
      "         [ 4.3333e-01, -9.3997e-02, -3.9861e-03,  ..., -7.2234e-02,\n",
      "          -8.1105e-02,  2.9990e-02],\n",
      "         ...,\n",
      "         [-1.0883e-02,  2.0735e-01,  1.6958e-02,  ..., -6.7433e-02,\n",
      "           2.8920e-02,  4.4529e-03],\n",
      "         [-4.0169e-02,  1.9800e-01,  4.7460e-03,  ..., -4.8530e-02,\n",
      "           3.6126e-02, -2.2004e-02],\n",
      "         [-6.1403e-02,  2.8449e-01, -4.5987e-03,  ..., -4.6383e-02,\n",
      "           8.3542e-02, -2.1332e-02]],\n",
      "\n",
      "        [[ 8.4181e-01,  7.4872e-04,  2.5786e-01,  ..., -5.9936e-03,\n",
      "           7.4425e-02, -7.7557e-02],\n",
      "         [ 1.8130e-01,  4.7337e-02,  1.1830e-01,  ...,  7.1577e-03,\n",
      "           3.4595e-02, -3.7686e-02],\n",
      "         [ 5.7828e-02,  1.0945e-01,  7.5846e-02,  ...,  4.5553e-03,\n",
      "           3.4959e-02, -3.0924e-02],\n",
      "         ...,\n",
      "         [-4.1598e-02,  1.7405e-01,  2.1121e-02,  ..., -6.1892e-02,\n",
      "          -1.5984e-03, -1.9570e-02],\n",
      "         [-4.3429e-02,  2.3457e-01,  3.3716e-02,  ..., -6.0352e-02,\n",
      "           3.1459e-02, -1.5502e-02],\n",
      "         [-5.2625e-02,  2.5670e-01,  2.7686e-02,  ..., -6.9850e-02,\n",
      "           3.1969e-02,  8.3719e-03]],\n",
      "\n",
      "        [[ 8.3338e-01,  7.4356e-02,  2.1983e-01,  ...,  1.0866e-01,\n",
      "          -2.3579e-03, -7.2829e-02],\n",
      "         [ 1.8942e-01,  8.9866e-02,  1.2200e-01,  ...,  8.1018e-02,\n",
      "          -4.6404e-03,  7.4666e-03],\n",
      "         [ 5.7860e-02,  1.4106e-01,  8.3044e-02,  ...,  3.8726e-02,\n",
      "           1.2661e-02,  1.2118e-03],\n",
      "         ...,\n",
      "         [-4.1526e-02,  1.7466e-01,  2.0998e-02,  ..., -6.2122e-02,\n",
      "          -1.5174e-03, -1.9628e-02],\n",
      "         [-4.3358e-02,  2.3495e-01,  3.3635e-02,  ..., -6.0517e-02,\n",
      "           3.1520e-02, -1.5558e-02],\n",
      "         [-5.2560e-02,  2.5695e-01,  2.7637e-02,  ..., -6.9962e-02,\n",
      "           3.2009e-02,  8.3148e-03]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1920])\n",
      "Outputs shape:  torch.Size([1920, 18258])\n",
      "Loss tensor(9.6724, grad_fn=<NllLossBackward0>)\n",
      "Epoch [1/1], Step [3/8], Loss: 9.6724, Perplexity: 15872.6804Training Loop captions shape:  torch.Size([128, 11])\n",
      "<built-in method size of Tensor object at 0x7fc73b607d10>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b607ef0>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b6d97d0>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 11, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 11, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 11, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 11, 512])\n",
      "outputs shape:  torch.Size([128, 11, 18258])\n",
      "Outputs shape:  torch.Size([128, 11, 18258])\n",
      "Outputs:  tensor([[[ 1.0197e+00,  9.4129e-03,  1.4902e-01,  ..., -3.4777e-01,\n",
      "           1.8132e-01,  5.6671e-02],\n",
      "         [ 3.4084e-01,  1.4846e-01,  1.2146e-01,  ..., -1.9563e-01,\n",
      "           1.2324e-01,  6.7730e-02],\n",
      "         [ 1.6646e-01,  3.0425e-01,  9.2511e-02,  ..., -1.5341e-01,\n",
      "           1.2378e-01,  4.3532e-02],\n",
      "         ...,\n",
      "         [-7.4809e-02,  4.2662e-01,  4.9359e-02,  ..., -1.0370e-01,\n",
      "           6.0323e-02, -2.2725e-02],\n",
      "         [-6.2353e-02,  3.6619e-01,  5.1623e-02,  ..., -1.0704e-01,\n",
      "           4.5067e-02, -2.1101e-02],\n",
      "         [-5.8817e-02,  3.6333e-01,  6.0892e-03,  ..., -7.4561e-02,\n",
      "           3.4119e-02, -1.6021e-02]],\n",
      "\n",
      "        [[ 1.0984e+00, -1.5174e-01, -1.0047e-01,  ..., -9.0766e-02,\n",
      "           1.4458e-01,  1.5316e-01],\n",
      "         [ 2.5097e-01, -9.9856e-02,  9.4707e-02,  ..., -5.8721e-02,\n",
      "           3.6251e-03,  2.1480e-01],\n",
      "         [ 1.3076e-01,  7.2625e-03,  5.3836e-02,  ..., -7.2818e-02,\n",
      "          -3.1064e-03,  1.3839e-01],\n",
      "         ...,\n",
      "         [-5.3536e-02,  2.9124e-01, -1.8313e-02,  ..., -7.4589e-02,\n",
      "           9.7112e-03, -2.5670e-02],\n",
      "         [-4.4477e-02,  2.9572e-01, -2.2806e-02,  ..., -8.5320e-02,\n",
      "           9.9352e-03, -2.7525e-02],\n",
      "         [-2.6873e-02,  2.8478e-01, -9.0348e-03,  ..., -9.1192e-02,\n",
      "           4.4497e-02, -3.7403e-02]],\n",
      "\n",
      "        [[ 2.2342e+00,  2.1352e-01, -8.1011e-03,  ..., -5.5524e-02,\n",
      "           9.5008e-02,  1.3692e-01],\n",
      "         [ 6.0834e-01,  9.2888e-02, -5.7940e-02,  ...,  5.1954e-02,\n",
      "          -7.7771e-03,  3.4051e-03],\n",
      "         [ 3.0562e-01,  2.0258e-01, -2.2696e-02,  ...,  6.8105e-03,\n",
      "           1.4409e-02, -1.5610e-03],\n",
      "         ...,\n",
      "         [-2.0571e-02,  3.1977e-01,  4.5365e-02,  ..., -8.3931e-02,\n",
      "           2.1313e-02, -4.7488e-02],\n",
      "         [-2.2697e-02,  2.6647e-01,  3.4703e-02,  ..., -6.5842e-02,\n",
      "          -8.3146e-03, -3.4483e-02],\n",
      "         [-6.5030e-02,  4.8407e-01,  8.2595e-03,  ..., -6.6433e-02,\n",
      "           6.7331e-02, -2.8534e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1425e+00,  2.4139e-01,  2.2012e-01,  ..., -1.3571e-01,\n",
      "           1.2438e-01,  4.4972e-01],\n",
      "         [ 3.7931e-01,  2.9011e-01,  2.2107e-01,  ..., -1.3581e-01,\n",
      "           1.0476e-01,  2.3815e-01],\n",
      "         [ 2.2332e-01,  3.0368e-01,  1.1906e-01,  ..., -1.0767e-01,\n",
      "           7.5475e-02,  1.0323e-01],\n",
      "         ...,\n",
      "         [-3.9013e-02,  3.2090e-01,  2.8594e-02,  ..., -7.6222e-02,\n",
      "           3.1984e-02, -7.3399e-04],\n",
      "         [-7.2375e-02,  3.5362e-01,  3.6242e-02,  ..., -1.1216e-01,\n",
      "           4.2762e-02, -2.2020e-02],\n",
      "         [-5.1702e-02,  3.5451e-01, -1.3952e-04,  ..., -9.4029e-02,\n",
      "           4.3960e-02, -4.0378e-02]],\n",
      "\n",
      "        [[ 1.3039e+00,  1.6386e-02, -9.0473e-02,  ..., -3.2892e-02,\n",
      "           4.3983e-01, -1.1508e-01],\n",
      "         [ 2.8766e-01,  1.1596e-01,  1.3352e-01,  ..., -1.9130e-02,\n",
      "           2.0631e-01,  8.1045e-02],\n",
      "         [ 1.4358e-01,  1.7569e-01,  9.8855e-02,  ..., -3.4821e-02,\n",
      "           1.3031e-01,  6.4577e-02],\n",
      "         ...,\n",
      "         [-5.1697e-02,  4.4454e-01,  2.8044e-02,  ..., -7.7543e-02,\n",
      "           4.8400e-02, -9.8407e-03],\n",
      "         [-5.9243e-02,  3.9870e-01,  5.5397e-02,  ..., -7.3490e-02,\n",
      "           5.5172e-02, -3.6047e-03],\n",
      "         [-8.9962e-02,  5.5671e-01,  2.4049e-02,  ..., -7.1982e-02,\n",
      "           1.0076e-01, -1.3172e-02]],\n",
      "\n",
      "        [[ 1.9367e+00, -4.4696e-01,  6.6599e-02,  ..., -1.5876e-01,\n",
      "           1.5792e-01, -1.4955e-01],\n",
      "         [ 3.8742e-01, -7.0162e-02,  2.2013e-02,  ..., -3.9773e-02,\n",
      "           7.9833e-02, -6.4242e-02],\n",
      "         [ 1.4630e-01,  1.5308e-01,  2.5911e-02,  ..., -4.5233e-02,\n",
      "           3.3866e-02, -5.7006e-02],\n",
      "         ...,\n",
      "         [-1.6621e-02,  2.8118e-01,  1.1848e-02,  ..., -7.6419e-02,\n",
      "           2.4037e-02, -7.9354e-03],\n",
      "         [-5.1821e-03,  2.8857e-01,  1.4088e-02,  ..., -8.0506e-02,\n",
      "           2.9780e-02, -2.9478e-02],\n",
      "         [-6.1305e-02,  4.8706e-01, -3.1581e-03,  ..., -7.5112e-02,\n",
      "           9.0609e-02, -3.2114e-02]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1408])\n",
      "Outputs shape:  torch.Size([1408, 18258])\n",
      "Loss tensor(9.4869, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [4/8], Loss: 9.4869, Perplexity: 13186.4008Training Loop captions shape:  torch.Size([128, 11])\n",
      "<built-in method size of Tensor object at 0x7fc6d8e73770>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b632290>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b632710>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 11, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 11, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 11, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 11, 512])\n",
      "outputs shape:  torch.Size([128, 11, 18258])\n",
      "Outputs shape:  torch.Size([128, 11, 18258])\n",
      "Outputs:  tensor([[[ 2.1208e+00, -5.3831e-01, -2.0870e-01,  ...,  3.0801e-02,\n",
      "           1.7969e-01, -7.1403e-03],\n",
      "         [ 4.2694e-01, -7.7588e-02, -1.3888e-01,  ..., -3.6823e-02,\n",
      "           8.8407e-02,  1.9897e-02],\n",
      "         [ 1.2555e-01,  2.7871e-01, -6.4083e-02,  ..., -7.6452e-02,\n",
      "           4.4093e-02,  2.2052e-02],\n",
      "         ...,\n",
      "         [-6.8492e-02,  6.0996e-01,  4.4867e-02,  ..., -1.1964e-01,\n",
      "           6.8318e-02, -4.1534e-02],\n",
      "         [-6.2872e-02,  5.7235e-01,  6.2652e-02,  ..., -1.2162e-01,\n",
      "           7.0677e-02, -3.0384e-02],\n",
      "         [-1.0353e-01,  8.5904e-01,  1.9665e-02,  ..., -1.0905e-01,\n",
      "           1.1927e-01, -3.0248e-02]],\n",
      "\n",
      "        [[ 3.2268e+00, -1.0791e-01, -5.6946e-01,  ...,  1.2436e-01,\n",
      "          -1.5203e-01,  8.3476e-02],\n",
      "         [ 9.3281e-01,  4.3750e-02, -1.8215e-01,  ..., -2.2494e-02,\n",
      "          -8.7850e-02, -2.7731e-02],\n",
      "         [ 5.2318e-01,  1.7317e-01, -9.5108e-02,  ..., -7.7106e-02,\n",
      "          -4.4533e-02, -3.7363e-02],\n",
      "         ...,\n",
      "         [-1.2227e-04,  3.9868e-01,  4.2333e-02,  ..., -1.0356e-01,\n",
      "           1.0702e-02, -5.9086e-02],\n",
      "         [-2.4448e-02,  4.2176e-01,  5.2738e-02,  ..., -1.1517e-01,\n",
      "           2.2463e-02, -3.9940e-02],\n",
      "         [-8.6327e-02,  7.3689e-01,  2.0411e-02,  ..., -1.0587e-01,\n",
      "           8.3941e-02, -3.2969e-02]],\n",
      "\n",
      "        [[ 3.3490e+00, -3.5849e-02, -3.3678e-01,  ..., -7.6842e-01,\n",
      "          -3.1469e-01, -1.6772e-01],\n",
      "         [ 7.7168e-01,  2.9442e-02, -1.5523e-01,  ..., -3.2364e-01,\n",
      "           1.2105e-02, -8.3251e-02],\n",
      "         [ 3.7124e-01,  3.1658e-01, -7.7523e-02,  ..., -2.4076e-01,\n",
      "           4.4304e-02, -6.0379e-02],\n",
      "         ...,\n",
      "         [-1.4533e-02,  5.8485e-01,  2.1244e-02,  ..., -8.3438e-02,\n",
      "           4.5472e-02, -4.6411e-02],\n",
      "         [-5.5045e-02,  5.3434e-01,  1.1456e-02,  ..., -8.8435e-02,\n",
      "           4.0241e-02, -2.2291e-02],\n",
      "         [-9.5637e-02,  8.1506e-01, -2.8538e-03,  ..., -9.1977e-02,\n",
      "           1.0569e-01, -3.5541e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.7911e+00,  2.9145e-01,  4.2893e-01,  ..., -2.8653e-01,\n",
      "           3.7039e-01,  6.3850e-01],\n",
      "         [ 8.1960e-01,  3.8568e-01,  2.3890e-01,  ..., -2.0538e-01,\n",
      "           1.2195e-01,  2.0547e-01],\n",
      "         [ 4.2515e-01,  5.4728e-01,  1.7832e-01,  ..., -1.8554e-01,\n",
      "           1.0231e-01,  6.2064e-02],\n",
      "         ...,\n",
      "         [-6.0576e-02,  6.5443e-01,  3.0781e-02,  ..., -9.4597e-02,\n",
      "           4.1486e-02, -2.2079e-02],\n",
      "         [-2.9829e-02,  6.0649e-01,  2.6603e-02,  ..., -1.0678e-01,\n",
      "           4.6275e-02, -2.9005e-02],\n",
      "         [-6.1761e-03,  5.4812e-01,  1.9914e-02,  ..., -7.3137e-02,\n",
      "           2.7798e-02,  9.4841e-03]],\n",
      "\n",
      "        [[ 2.3332e+00, -1.4309e-01,  3.0215e-01,  ..., -6.0881e-02,\n",
      "          -3.2734e-01, -2.6696e-01],\n",
      "         [ 4.8230e-01,  1.5838e-02,  1.0450e-01,  ..., -7.0596e-02,\n",
      "          -7.1084e-02, -1.1194e-01],\n",
      "         [ 1.7825e-01,  1.8350e-01,  2.6596e-02,  ..., -7.0715e-02,\n",
      "          -5.0541e-02, -4.1809e-02],\n",
      "         ...,\n",
      "         [-5.0621e-02,  4.5368e-01,  2.9071e-02,  ..., -1.0816e-01,\n",
      "           6.3444e-02, -1.1215e-02],\n",
      "         [-3.4645e-02,  4.6896e-01,  5.5709e-02,  ..., -7.5851e-02,\n",
      "           3.8229e-02, -2.4659e-02],\n",
      "         [-9.0592e-02,  7.7521e-01,  2.5427e-02,  ..., -8.6517e-02,\n",
      "           1.0121e-01, -3.1063e-02]],\n",
      "\n",
      "        [[ 1.7910e+00, -8.1382e-02,  1.9929e-01,  ..., -4.7495e-02,\n",
      "          -5.2233e-02,  1.2371e-02],\n",
      "         [ 3.3107e-01,  8.1259e-02,  2.1006e-01,  ..., -9.0547e-02,\n",
      "           7.5086e-02, -3.9121e-02],\n",
      "         [ 1.4000e-01,  3.9695e-01,  1.5178e-01,  ..., -1.0188e-01,\n",
      "           1.1768e-01, -7.2010e-03],\n",
      "         ...,\n",
      "         [-3.4376e-02,  4.8431e-01,  5.8450e-02,  ..., -1.2822e-01,\n",
      "           4.5128e-03, -2.0678e-02],\n",
      "         [-7.7845e-03,  4.2057e-01,  6.0658e-02,  ..., -1.2846e-01,\n",
      "           7.0368e-03, -2.8337e-03],\n",
      "         [-7.3457e-02,  7.5181e-01,  2.6073e-02,  ..., -1.1379e-01,\n",
      "           8.7754e-02, -1.5805e-02]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1408])\n",
      "Outputs shape:  torch.Size([1408, 18258])\n",
      "Loss tensor(9.3185, grad_fn=<NllLossBackward0>)\n",
      "Epoch [1/1], Step [5/8], Loss: 9.3185, Perplexity: 11142.7505Training Loop captions shape:  torch.Size([128, 22])\n",
      "<built-in method size of Tensor object at 0x7fc73b64e3b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b64ed10>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b64e110>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 22, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 22, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 22, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 22, 512])\n",
      "outputs shape:  torch.Size([128, 22, 18258])\n",
      "Outputs shape:  torch.Size([128, 22, 18258])\n",
      "Outputs:  tensor([[[ 3.1204e+00, -2.0593e-01,  3.5611e-01,  ..., -3.2223e-01,\n",
      "          -5.1733e-03,  1.2486e-01],\n",
      "         [ 6.0124e-01,  2.4205e-01,  2.8498e-01,  ..., -1.8999e-01,\n",
      "           1.4228e-01, -9.7652e-02],\n",
      "         [ 3.0293e-01,  5.3145e-01,  2.2174e-01,  ..., -1.6889e-01,\n",
      "           1.4218e-01, -4.0967e-02],\n",
      "         ...,\n",
      "         [-7.4795e-02,  9.6673e-01,  6.2888e-02,  ..., -1.9024e-01,\n",
      "           4.9426e-02, -7.0063e-02],\n",
      "         [-8.9826e-02,  9.2275e-01,  8.5409e-02,  ..., -1.7329e-01,\n",
      "           6.2886e-02, -5.5014e-02],\n",
      "         [-1.3937e-01,  1.3073e+00,  5.1010e-02,  ..., -1.7197e-01,\n",
      "           1.2604e-01, -5.7382e-02]],\n",
      "\n",
      "        [[ 4.9362e+00, -8.9617e-02, -2.1017e-01,  ..., -4.4278e-01,\n",
      "          -6.4396e-02, -3.3204e-01],\n",
      "         [ 1.3475e+00,  1.7197e-02, -2.0515e-02,  ..., -3.9431e-01,\n",
      "          -4.5971e-02, -2.4815e-01],\n",
      "         [ 6.6687e-01,  2.1526e-01,  2.7550e-02,  ..., -3.3988e-01,\n",
      "          -3.7313e-02, -1.8132e-01],\n",
      "         ...,\n",
      "         [-7.2793e-02,  9.5212e-01,  5.6183e-02,  ..., -1.9220e-01,\n",
      "           7.3879e-02, -8.1294e-02],\n",
      "         [-8.4978e-02,  9.0467e-01,  5.7931e-02,  ..., -1.6683e-01,\n",
      "           9.4707e-02, -6.2822e-02],\n",
      "         [-1.2884e-01,  1.2962e+00,  3.0441e-02,  ..., -1.6881e-01,\n",
      "           1.3898e-01, -6.0026e-02]],\n",
      "\n",
      "        [[ 1.8811e+00, -7.3401e-01, -3.1790e-01,  ..., -3.4257e-01,\n",
      "          -2.6547e-02,  3.9931e-03],\n",
      "         [ 4.6158e-01, -2.5133e-01, -7.7910e-02,  ..., -1.9431e-01,\n",
      "           1.4629e-02, -4.0010e-03],\n",
      "         [ 1.9408e-01,  2.3365e-01, -4.0745e-04,  ..., -1.6973e-01,\n",
      "          -2.6920e-04, -5.1070e-03],\n",
      "         ...,\n",
      "         [-2.6976e-02,  8.5986e-01,  3.4911e-02,  ..., -1.5307e-01,\n",
      "           4.8872e-02, -8.2163e-02],\n",
      "         [-3.2673e-02,  8.0672e-01,  4.4556e-02,  ..., -1.8923e-01,\n",
      "           4.6414e-02, -6.2210e-02],\n",
      "         [-1.0311e-01,  1.2167e+00,  2.2879e-02,  ..., -1.7197e-01,\n",
      "           1.1143e-01, -6.4703e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.3186e+00, -5.0289e-01, -2.7582e-01,  ..., -4.9492e-01,\n",
      "          -2.6431e-02, -4.5985e-02],\n",
      "         [ 6.2042e-01, -1.6336e-01, -1.1388e-01,  ..., -2.3643e-01,\n",
      "           8.9956e-04,  4.4744e-03],\n",
      "         [ 2.7150e-01,  2.7596e-01, -1.0464e-02,  ..., -1.9695e-01,\n",
      "          -8.8674e-03, -1.8229e-02],\n",
      "         ...,\n",
      "         [-2.7004e-02,  8.6017e-01,  3.4966e-02,  ..., -1.5311e-01,\n",
      "           4.8917e-02, -8.2199e-02],\n",
      "         [-3.2694e-02,  8.0696e-01,  4.4592e-02,  ..., -1.8926e-01,\n",
      "           4.6446e-02, -6.2233e-02],\n",
      "         [-1.0313e-01,  1.2168e+00,  2.2903e-02,  ..., -1.7200e-01,\n",
      "           1.1145e-01, -6.4716e-02]],\n",
      "\n",
      "        [[ 4.0038e+00,  3.2832e-02, -2.7441e-01,  ..., -2.5451e-01,\n",
      "          -2.5309e-01, -1.8490e-01],\n",
      "         [ 1.2192e+00, -2.7614e-02, -5.9247e-02,  ..., -2.1232e-01,\n",
      "          -1.9560e-01, -1.6087e-01],\n",
      "         [ 6.1916e-01,  1.2084e-01,  1.6951e-02,  ..., -2.2174e-01,\n",
      "          -1.3545e-01, -1.2687e-01],\n",
      "         ...,\n",
      "         [-7.2573e-02,  9.4890e-01,  5.6051e-02,  ..., -1.9185e-01,\n",
      "           7.3484e-02, -8.1266e-02],\n",
      "         [-8.4805e-02,  9.0221e-01,  5.7834e-02,  ..., -1.6656e-01,\n",
      "           9.4420e-02, -6.2805e-02],\n",
      "         [-1.2872e-01,  1.2943e+00,  3.0369e-02,  ..., -1.6860e-01,\n",
      "           1.3876e-01, -6.0020e-02]],\n",
      "\n",
      "        [[ 1.4702e+00,  1.1483e-01,  2.2415e-01,  ..., -4.9944e-01,\n",
      "           1.8676e-01,  3.6378e-04],\n",
      "         [ 1.5815e-01,  2.1385e-01,  1.9686e-01,  ..., -2.0189e-01,\n",
      "           3.4060e-02,  7.7796e-02],\n",
      "         [ 5.6840e-02,  5.1948e-01,  1.7989e-01,  ..., -1.8100e-01,\n",
      "           2.7202e-02,  2.4760e-02],\n",
      "         ...,\n",
      "         [-2.6968e-02,  8.5988e-01,  3.4919e-02,  ..., -1.5307e-01,\n",
      "           4.8931e-02, -8.2145e-02],\n",
      "         [-3.2669e-02,  8.0672e-01,  4.4560e-02,  ..., -1.8922e-01,\n",
      "           4.6448e-02, -6.2195e-02],\n",
      "         [-1.0311e-01,  1.2166e+00,  2.2881e-02,  ..., -1.7197e-01,\n",
      "           1.1145e-01, -6.4692e-02]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([2816])\n",
      "Outputs shape:  torch.Size([2816, 18258])\n",
      "Loss tensor(9.4194, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [6/8], Loss: 9.4194, Perplexity: 12325.6342Training Loop captions shape:  torch.Size([128, 12])\n",
      "<built-in method size of Tensor object at 0x7fc6e989ac50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6e989add0>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d156b30>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 12, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 12, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 12, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 12, 512])\n",
      "outputs shape:  torch.Size([128, 12, 18258])\n",
      "Outputs shape:  torch.Size([128, 12, 18258])\n",
      "Outputs:  tensor([[[ 3.0342e+00, -1.7601e-01, -4.2856e-01,  ..., -7.0302e-01,\n",
      "          -3.8108e-01, -1.6455e-01],\n",
      "         [ 7.7230e-01,  1.3212e-01, -8.4695e-02,  ..., -4.5922e-01,\n",
      "          -1.9055e-01, -1.5123e-01],\n",
      "         [ 3.3131e-01,  7.1071e-01,  3.6801e-03,  ..., -4.3150e-01,\n",
      "          -1.1050e-01, -1.4587e-01],\n",
      "         ...,\n",
      "         [-1.3297e-01,  1.7670e+00,  1.2794e-01,  ..., -3.6944e-01,\n",
      "           3.3072e-02, -1.4668e-01],\n",
      "         [-1.2048e-01,  1.7343e+00,  1.0669e-01,  ..., -3.8159e-01,\n",
      "           3.4093e-02, -1.2803e-01],\n",
      "         [-1.7523e-01,  2.2895e+00,  8.5422e-02,  ..., -3.6568e-01,\n",
      "           1.2427e-01, -1.2483e-01]],\n",
      "\n",
      "        [[ 2.1619e+00,  2.9289e-01,  4.4324e-01,  ..., -1.7794e-01,\n",
      "           3.3924e-01, -1.9081e-01],\n",
      "         [ 3.7942e-01,  3.8839e-01,  3.6380e-01,  ..., -1.7239e-01,\n",
      "           1.8838e-01, -1.4430e-01],\n",
      "         [ 1.7445e-01,  5.8396e-01,  2.5710e-01,  ..., -1.7155e-01,\n",
      "           1.0954e-01, -7.9084e-02],\n",
      "         ...,\n",
      "         [-5.2246e-02,  1.4689e+00,  1.2363e-01,  ..., -2.7846e-01,\n",
      "           3.6742e-02, -1.1760e-01],\n",
      "         [-5.1668e-02,  1.5805e+00,  1.0336e-01,  ..., -2.8385e-01,\n",
      "           6.0158e-02, -1.3861e-01],\n",
      "         [-9.7614e-02,  1.5785e+00,  7.8517e-02,  ..., -3.1214e-01,\n",
      "           4.7441e-02, -1.2709e-01]],\n",
      "\n",
      "        [[ 3.7175e+00, -2.1572e-01,  7.5503e-01,  ..., -6.5284e-02,\n",
      "           4.8468e-01,  4.9357e-01],\n",
      "         [ 8.5918e-01,  2.7806e-01,  4.9835e-01,  ..., -1.1698e-01,\n",
      "           2.7135e-01,  8.0077e-02],\n",
      "         [ 4.3554e-01,  8.1955e-01,  3.6811e-01,  ..., -2.6312e-01,\n",
      "           1.7119e-01, -1.0221e-02],\n",
      "         ...,\n",
      "         [-1.0095e-01,  1.7578e+00,  1.4263e-01,  ..., -3.5875e-01,\n",
      "           6.5541e-02, -1.3279e-01],\n",
      "         [-8.3053e-02,  1.7139e+00,  1.3086e-01,  ..., -3.5432e-01,\n",
      "           9.4646e-02, -1.1743e-01],\n",
      "         [-1.5426e-01,  2.2366e+00,  9.4567e-02,  ..., -3.4734e-01,\n",
      "           1.5721e-01, -1.1575e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.8340e+00, -9.1246e-02,  4.2332e-01,  ...,  6.2809e-02,\n",
      "           4.4233e-01,  1.3404e-01],\n",
      "         [ 5.2279e-01,  3.0547e-01,  4.0345e-01,  ..., -1.2464e-02,\n",
      "           1.8153e-01, -4.5621e-03],\n",
      "         [ 2.5988e-01,  6.8512e-01,  2.9811e-01,  ..., -1.1060e-01,\n",
      "           1.3421e-01, -4.9361e-02],\n",
      "         ...,\n",
      "         [-3.3817e-02,  1.3517e+00,  8.8992e-02,  ..., -2.8128e-01,\n",
      "           4.1239e-02, -1.1599e-01],\n",
      "         [-7.3416e-02,  1.3083e+00,  7.5699e-02,  ..., -2.8381e-01,\n",
      "           1.0959e-02, -1.1471e-01],\n",
      "         [-1.4358e-01,  1.8799e+00,  5.9956e-02,  ..., -2.8687e-01,\n",
      "           1.0559e-01, -1.1272e-01]],\n",
      "\n",
      "        [[ 3.2719e+00, -2.9376e-01, -1.0802e-01,  ..., -2.9614e-01,\n",
      "          -4.6506e-01, -3.0261e-01],\n",
      "         [ 8.4924e-01, -2.3433e-02,  1.2448e-01,  ..., -1.5071e-01,\n",
      "          -1.5458e-01, -1.7816e-01],\n",
      "         [ 3.9664e-01,  5.3796e-01,  1.3944e-01,  ..., -1.9895e-01,\n",
      "          -6.5743e-02, -1.5492e-01],\n",
      "         ...,\n",
      "         [-7.9945e-02,  1.4277e+00,  9.9059e-02,  ..., -2.8965e-01,\n",
      "           4.7849e-02, -1.1359e-01],\n",
      "         [-7.9844e-02,  1.4681e+00,  9.8913e-02,  ..., -3.1021e-01,\n",
      "           6.0435e-02, -1.1107e-01],\n",
      "         [-8.8019e-02,  1.5116e+00,  7.1484e-02,  ..., -2.9852e-01,\n",
      "           7.1031e-02, -1.3676e-01]],\n",
      "\n",
      "        [[ 4.6043e+00, -3.3959e-02,  5.5293e-01,  ..., -2.5722e-03,\n",
      "           4.6273e-01,  5.3422e-01],\n",
      "         [ 1.1502e+00,  4.4618e-01,  4.5609e-01,  ..., -1.7731e-01,\n",
      "           2.4153e-01,  1.4282e-01],\n",
      "         [ 5.7518e-01,  9.6427e-01,  3.5667e-01,  ..., -2.8638e-01,\n",
      "           1.8466e-01, -1.6427e-02],\n",
      "         ...,\n",
      "         [-1.1083e-01,  1.5540e+00,  1.2157e-01,  ..., -2.8326e-01,\n",
      "           6.1217e-02, -9.3501e-02],\n",
      "         [-1.0542e-01,  1.5225e+00,  1.0531e-01,  ..., -2.9188e-01,\n",
      "           1.1271e-02, -8.2505e-02],\n",
      "         [-1.6348e-01,  2.0581e+00,  7.4032e-02,  ..., -3.0156e-01,\n",
      "           1.0913e-01, -9.8630e-02]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1536])\n",
      "Outputs shape:  torch.Size([1536, 18258])\n",
      "Loss tensor(8.8507, grad_fn=<NllLossBackward0>)\n",
      "Epoch [1/1], Step [7/8], Loss: 8.8507, Perplexity: 6979.4019Training Loop captions shape:  torch.Size([128, 14])\n",
      "<built-in method size of Tensor object at 0x7fc619373bf0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73fc291d0>\n",
      "torch.Size([128, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73fc29fb0>\n",
      "torch.Size([128, 1024])\n",
      "LM embeddings shape original:  torch.Size([128, 14, 1024])\n",
      "unsqueezed features:  torch.Size([128, 1, 1024])\n",
      "embeddings shape after cat:  torch.Size([128, 14, 1024])\n",
      "Hidden LSTM shape:  torch.Size([128, 14, 512])\n",
      "Hidden LSTM unsqueezed shape:  torch.Size([128, 1, 14, 512])\n",
      "outputs shape:  torch.Size([128, 14, 18258])\n",
      "Outputs shape:  torch.Size([128, 14, 18258])\n",
      "Outputs:  tensor([[[ 5.4171e+00,  9.7225e-03, -6.5925e-01,  ..., -2.8819e-01,\n",
      "          -4.6337e-01, -2.1705e-01],\n",
      "         [ 1.5263e+00,  3.3139e-01, -1.1632e-01,  ..., -4.2321e-01,\n",
      "          -1.4390e-01, -3.0544e-01],\n",
      "         [ 7.6019e-01,  1.2315e+00,  4.4201e-02,  ..., -5.4630e-01,\n",
      "          -1.0252e-01, -3.2578e-01],\n",
      "         ...,\n",
      "         [-1.9346e-01,  4.3676e+00,  3.0937e-01,  ..., -8.8405e-01,\n",
      "           6.2931e-02, -3.9303e-01],\n",
      "         [-2.1058e-01,  4.4798e+00,  3.2716e-01,  ..., -9.1196e-01,\n",
      "           7.5250e-02, -4.0991e-01],\n",
      "         [-2.6180e-01,  5.2280e+00,  3.1798e-01,  ..., -9.5615e-01,\n",
      "           1.3744e-01, -4.1773e-01]],\n",
      "\n",
      "        [[ 6.2557e+00, -3.3353e-01, -6.2536e-01,  ..., -5.5034e-01,\n",
      "          -4.4800e-01, -4.1629e-01],\n",
      "         [ 1.7719e+00, -4.1947e-02, -9.9589e-02,  ..., -4.8024e-01,\n",
      "          -2.7476e-01, -3.3231e-01],\n",
      "         [ 9.2600e-01,  7.1011e-01,  4.2080e-02,  ..., -5.2421e-01,\n",
      "          -2.1941e-01, -3.3271e-01],\n",
      "         ...,\n",
      "         [-1.8322e-01,  3.7348e+00,  2.8294e-01,  ..., -7.9453e-01,\n",
      "           5.8483e-02, -3.6060e-01],\n",
      "         [-1.7175e-01,  3.9090e+00,  3.1176e-01,  ..., -8.0600e-01,\n",
      "           6.5210e-02, -3.2056e-01],\n",
      "         [-2.4390e-01,  4.6959e+00,  2.9692e-01,  ..., -8.5493e-01,\n",
      "           1.2641e-01, -3.4641e-01]],\n",
      "\n",
      "        [[ 4.7718e+00, -2.2128e-01,  1.0494e+00,  ...,  2.1690e-02,\n",
      "           4.4960e-01,  1.2117e-01],\n",
      "         [ 1.2182e+00,  4.1986e-01,  5.6228e-01,  ..., -1.4833e-01,\n",
      "           1.5694e-01, -1.6498e-01],\n",
      "         [ 7.3312e-01,  9.2505e-01,  3.9401e-01,  ..., -2.7875e-01,\n",
      "           9.1274e-02, -1.8245e-01],\n",
      "         ...,\n",
      "         [-1.4077e-01,  3.2589e+00,  2.2308e-01,  ..., -6.7108e-01,\n",
      "           6.3068e-02, -3.0345e-01],\n",
      "         [-1.3155e-01,  3.4305e+00,  2.1207e-01,  ..., -6.9045e-01,\n",
      "           6.2792e-02, -3.0572e-01],\n",
      "         [-2.1106e-01,  4.2320e+00,  2.2505e-01,  ..., -7.3961e-01,\n",
      "           1.2992e-01, -3.1500e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.9821e+00, -5.3519e-01, -5.4236e-01,  ..., -4.6425e-01,\n",
      "           3.7701e-03, -6.2008e-01],\n",
      "         [ 1.3941e+00, -1.2547e-01, -1.3229e-01,  ..., -3.9622e-01,\n",
      "          -9.5018e-02, -3.4923e-01],\n",
      "         [ 6.9176e-01,  7.0946e-01,  2.6519e-02,  ..., -4.7772e-01,\n",
      "          -1.1265e-01, -3.0574e-01],\n",
      "         ...,\n",
      "         [-1.8566e-01,  4.1949e+00,  3.0374e-01,  ..., -8.8780e-01,\n",
      "           4.5927e-02, -3.8097e-01],\n",
      "         [-1.8564e-01,  4.3688e+00,  3.1303e-01,  ..., -8.9587e-01,\n",
      "           2.7077e-02, -3.8379e-01],\n",
      "         [-2.5828e-01,  5.1250e+00,  3.0534e-01,  ..., -9.4187e-01,\n",
      "           1.1404e-01, -4.0299e-01]],\n",
      "\n",
      "        [[ 6.7913e+00, -7.9089e-01, -4.0236e-01,  ..., -2.8409e-01,\n",
      "          -4.1741e-01, -5.6131e-01],\n",
      "         [ 1.8699e+00, -2.1237e-01, -3.8196e-02,  ..., -2.7520e-01,\n",
      "          -1.6200e-01, -4.5409e-01],\n",
      "         [ 9.7384e-01,  6.8160e-01,  8.1677e-02,  ..., -4.5174e-01,\n",
      "          -1.4127e-01, -4.0820e-01],\n",
      "         ...,\n",
      "         [-1.9844e-01,  3.9582e+00,  3.0403e-01,  ..., -8.3560e-01,\n",
      "           2.4464e-02, -3.8517e-01],\n",
      "         [-1.7900e-01,  4.1311e+00,  2.9819e-01,  ..., -8.6342e-01,\n",
      "           6.2885e-02, -3.7317e-01],\n",
      "         [-2.4910e-01,  4.9428e+00,  2.9660e-01,  ..., -9.0651e-01,\n",
      "           1.2223e-01, -3.8891e-01]],\n",
      "\n",
      "        [[ 8.4250e+00, -6.3939e-01, -3.9673e-01,  ..., -8.8422e-01,\n",
      "          -7.9148e-01, -3.1221e-01],\n",
      "         [ 2.3139e+00, -1.2952e-01, -3.1098e-03,  ..., -6.5125e-01,\n",
      "          -3.6656e-01, -3.7551e-01],\n",
      "         [ 1.2370e+00,  8.7418e-01,  9.5593e-02,  ..., -6.8194e-01,\n",
      "          -2.6353e-01, -3.9232e-01],\n",
      "         ...,\n",
      "         [-1.7780e-01,  4.2611e+00,  3.0679e-01,  ..., -8.9799e-01,\n",
      "           4.9391e-02, -4.0680e-01],\n",
      "         [-1.7679e-01,  4.4413e+00,  3.3714e-01,  ..., -9.3700e-01,\n",
      "           7.9536e-02, -4.1665e-01],\n",
      "         [-1.5662e-01,  4.6385e+00,  3.3987e-01,  ..., -9.3951e-01,\n",
      "           7.4006e-02, -4.0872e-01]]], grad_fn=<AddBackward0>)\n",
      "target shape original:  torch.Size([1792])\n",
      "Outputs shape:  torch.Size([1792, 18258])\n",
      "Loss tensor(8.2890, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch [1/1], Step [8/8], Loss: 8.2890, Perplexity: 3979.9758"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "                \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        print(\"Training Loop captions shape: \", captions.shape)\n",
    "        \n",
    "        # Zero the gradients (reset).\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        print(\"Outputs shape: \", outputs.shape)\n",
    "        print(\"Outputs: \", outputs)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "#         print(\"target shape before view: \", captions.shape)\n",
    "        print(\"target shape original: \", captions.view(-1).shape)\n",
    "#         print(\"target shape transformed: \", captions[:, 1:].reshape(-1).shape)\n",
    "        print(\"Outputs shape: \", outputs.contiguous().view(-1, vocab_size).shape)\n",
    "        \n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.reshape(-1))\n",
    "        print(\"Loss\", loss)\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'func-decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'func-encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eae7c1",
   "metadata": {},
   "source": [
    "## Reference game setting\n",
    "\n",
    "Here, the speaker is initialized with the pretrained model. Then, it is: \n",
    "* put in \"inference mode\", \n",
    "* i.e., we sample a caption until the END token was sampled\n",
    "* this is transformed to the NL message (or can we skip this step? the tok2idx transformation is normally happening in the get_item call, but we probably don't need getting the sample here)\n",
    "* this is input into an encoder lstm (i.e., (1, len(sent))\n",
    "* the image embedding is created with the same visual encoder (here, it also has a hidden size of 512 - to match the lstm hidden size)\n",
    "* output lstm vector before linear layer, reshaped to the same shape as the visual embedding, to allow computing similarity\n",
    "\n",
    "Then, the training loop is the most complicated thing. We need to combine the computation of the cross entropy loss with reinforce here. Furthermore, I need to check how to backpropagate to the speaker model, when it is set in \"inference\" mode. Maybe I can use @grad, or similar (analogous to .no_grad())?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32cdbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListenerEncoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the langauge module consisting of a one-layer LSTM, a dropout layer and \n",
    "        trainable embeddings. The image embedding is used as additional context at every step of the training \n",
    "        (prepended at the embedding beginning). \n",
    "        \"\"\"\n",
    "        super(ListenerEncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size= embed_size\n",
    "        self.vocabulary_size = vocab_size\n",
    "        # not sure if this will be pretrained\n",
    "        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size) # .from_pretrained(\n",
    "#                             torch.FloatTensor(self.fasttext.vectors)\n",
    "#                         )\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size , self.num_layers, batch_first=True)\n",
    "        # reshape into a vector of (1, 512), such that similarity with the image can be computed\n",
    "        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    # how is the lstm initialized?\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" At the start of training, we need to initialize a hidden state;\n",
    "        there will be none because the hidden state is formed based on previously seen data.\n",
    "        So, this function defines a hidden state with all zeroes\n",
    "        The axes semantics are (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n",
    "                torch.zeros((1, batch_size, self.hidden_size), device=device))\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        # initialize hidden layer\n",
    "        # check if this isn't reinitializing the hidden state in the middle of the sequence\n",
    "#         self.hidden = self.init_hidden(self.hidden_size)\n",
    "        print(\"Check batch size: \", captions.shape)\n",
    "        embeddings = self.embed(captions)\n",
    "        print(\"LSTM encoder embeddings shape: \", embeddings.shape)\n",
    "        # where is the previous hidden state coming from?\n",
    "        hiddens, self.hidden = self.lstm(embeddings) # , self.hidden\n",
    "        outputs = self.linear(hiddens)\n",
    "        print(\"LSTM encoder outputs shape: \", outputs.shape)\n",
    "        print(outputs)\n",
    "        return hiddens, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fda189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListenerEncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        Initialize pretrained Resnet 50.\n",
    "        \"\"\"\n",
    "        super(ListenerEncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # remove the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.batch= nn.BatchNorm1d(embed_size,momentum = 0.01)\n",
    "        self.embed.weight.data.normal_(0., 0.02)\n",
    "        self.embed.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images1, images2, caption):\n",
    "        print(\"Encoder forward images shape: \", images1.shape)\n",
    "        print(images2.shape)\n",
    "        print(\"Encoder forward cpation before transform: \", caption.shape)\n",
    "        features1 = self.resnet(images1) #[0]\n",
    "        features2 = self.resnet(images2) # [1]\n",
    "        # reshape features to shape (300, -1) - adapt to first dim\n",
    "        features1 = features1.view(features1.size(0), -1)\n",
    "        features1 = self.batch(self.embed(features1))\n",
    "        features2 = features2.view(features2.size(0), -1)\n",
    "        features2 = self.batch(self.embed(features2))\n",
    "        print(\"Encoder out shape: \", features1.shape)\n",
    "        # compute dot product between images and caption\n",
    "        caption = caption.mean(1)\n",
    "        dot_products_1 = torch.bmm(features1.view(images1.size()[0], 1, features1.size()[1]),\n",
    "                                   caption.view(images1.size()[0], features1.size()[1], 1))\n",
    "        dot_products_2 = torch.bmm(features2.view(images2.size()[0], 1, features2.size()[1]),\n",
    "                                   caption.view(images2.size()[0], features2.size()[1], 1))\n",
    "        print(\"Dots shape: \", dot_products_1.shape)\n",
    "        # compose targets and distractors dot products\n",
    "        pairs = torch.stack((dot_products_1, dot_products_2), dim=1) # stack into pairs, assuming dim=0 is the batch dimension\n",
    "        print(\"Pairs ashape: \", pairs.shape)\n",
    "        pairs_flat = pairs.squeeze(-1).squeeze(-1)\n",
    "        print(\"Pairs flat shape: \", pairs_flat.shape)\n",
    "        print(pairs_flat)\n",
    "        # get out a tensor with indices of the larger dot product, tensor of shape (batch,)\n",
    "#         out = torch.argmax(pairs_flat, dim=1)\n",
    "#         print(\"Out: \", out.shape)\n",
    "        return pairs_flat # out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d66933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOCaptionsDataset_functional(COCOCaptionsDataset):\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return an image-caption tuple. The caption is sampled from the list of five captions/image at random.\n",
    "\n",
    "        Arguments:\n",
    "        -------\n",
    "        idx: (int, int)\n",
    "            Index of the target and distractor items to be returned.\n",
    "        \"\"\"\n",
    "        # TODO watch out that the same image-caption pair isn't used too often\n",
    "\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode != 'test':\n",
    "            # sample target index\n",
    "            target = np.random.choice([0,1])\n",
    "            target_idx = idx[target]\n",
    "            distractor_idx = idx[(1-target)]\n",
    "#             print(\"Target index: \", target_idx)\n",
    "#             print(\"Distarctor index: \", distractor_idx)\n",
    "            \n",
    "            ann_id = self.ids[target_idx]\n",
    "            target_caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            target_path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # get distarctor\n",
    "            dist_id = self.ids[distractor_idx]\n",
    "            dist_img_id = self.coco.anns[dist_id]['image_id']\n",
    "            distractor_path = self.coco.loadImgs(dist_img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            target_image = Image.open(os.path.join(self.image_dir, target_path)).convert('RGB')\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "            distractor_image = Image.open(os.path.join(self.image_dir, distractor_path)).convert('RGB')\n",
    "            distractor_image = self.transform(distractor_image)\n",
    "\n",
    "            # TODO check if any other preprocessing of the caption needs to be performed\n",
    "\n",
    "            tokenizer = get_tokenizer(\"basic_english\")\n",
    "            # TODO possibly shorten too long captions\n",
    "            tokens = tokenizer(str(target_caption).lower())\n",
    "            # Convert caption to tensor of word ids, append start and end tokens.\n",
    "            target_caption = []\n",
    "            target_caption.append(self.vocab(self.vocab.start_word))\n",
    "\n",
    "            # check if the sequence needs to be padded or truncated\n",
    "#             if self.max_sequence_length == 0:\n",
    "#                 while len(tokens) < self.max_caption_length:\n",
    "#                     tokens.append(self.pad_token)\n",
    "#             else:\n",
    "#                 while len(tokens) < self.max_sequence_length:\n",
    "#                     tokens.append(self.pad_token)\n",
    "#                 tokens = tokens[:self.max_sequence_length]\n",
    "\n",
    "            target_caption.extend([self.vocab(token) for token in tokens])\n",
    "            target_caption.append(self.vocab(self.vocab.end_word))\n",
    "#             print(\"Indexed caption: \", caption)\n",
    "            target_caption = torch.Tensor(target_caption).long()\n",
    "#             print(\"DTYPES: \", type(target_image), \" \", distractor_image, \" \", target_caption, \" \", target)\n",
    "            # return pre-processed image and caption tensors\n",
    "            return target_image, distractor_image, target_caption, target\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[idx]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.image_dir, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9427936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader_functional(transform,\n",
    "               mode='val',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"START\",\n",
    "               end_word=\"END\",\n",
    "               unk_word=\"UNK\",\n",
    "               pad_word=\"PAD\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               download_dir=\"../../../data/val/\",\n",
    "#                cocoapi_loc='/Users/bjartesunde/Dropbox/Udacity/Computer Vision Nanodegree/computer-vision-ND/project_2_image_captioning_project/cocoapi'\n",
    "              ):\n",
    "    \"\"\"Returns the data loader.\n",
    "    Args:\n",
    "      transform: Image transform.\n",
    "      mode: One of 'train' or 'test'.\n",
    "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: Minimum word count threshold.\n",
    "      vocab_file: File containing the vocabulary. \n",
    "      start_word: Special word denoting sentence start.\n",
    "      end_word: Special word denoting sentence end.\n",
    "      unk_word: Special word denoting unknown words.\n",
    "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
    "                       If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: Number of subprocesses to use for data loading \n",
    "      cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test', 'val'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train' or mode=='val', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'val':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json')\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder = os.path.join(download_dir, \"train2014/\") \n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = os.path.join(download_dir, \"val2014/\") #'test2014/'\n",
    "        annotations_file = os.path.join(download_dir, 'annotations/captions_val2014.json') #image_info_test2014\n",
    "\n",
    "    # COCO caption dataset.\n",
    "    dataset = COCOCaptionsDataset_functional(\n",
    "        file=annotations_file,\n",
    "        download_dir = download_dir, \n",
    "        img_transform=transform,\n",
    "        text_transform=None,\n",
    "        batch_size=batch_size,\n",
    "        mode=mode,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_token=start_word,\n",
    "        end_token=end_word,\n",
    "        unk_token=unk_word,\n",
    "        pad_token=pad_word, \n",
    "        vocab_from_file=vocab_from_file,\n",
    "        max_sequence_length=25,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=torch.utils.data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False),\n",
    "#                                                  collate_fn=custom_collate_fn\n",
    "                                                 )\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                           \n",
    "           batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "#                                       num_workers=num_workers\n",
    "                                                 )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2b1129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# define a data loader returning two images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader_pairs = get_loader_functional(transform=transform_train,\n",
    "                         mode='val',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                         download_dir=\"../../../data/val\", \n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aae23584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584\n"
     ]
    }
   ],
   "source": [
    "# reference game training loop\n",
    "import math\n",
    "# instantiate litener models\n",
    "\n",
    "listener_embed_size = 512\n",
    "speaker_embed_size = 1024\n",
    "vocab_threshold = 1        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 1          # determines window for printing average loss\n",
    "log_file = 'listener_log_train.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# vocab size should be the same as for speaker / pretrained model\n",
    "\n",
    "# NOTE: embed_size for speaker is 1024, different from listener\n",
    "\n",
    "speaker_encoder = EncoderCNN(speaker_embed_size)\n",
    "speaker_decoder = DecoderRNN(speaker_embed_size, hidden_size, vocab_size)\n",
    "speaker_encoder.load_state_dict(torch.load(os.path.join('./models', 'func-encoder-1.pkl')))\n",
    "speaker_decoder.load_state_dict(torch.load(os.path.join('./models', 'func-decoder-1.pkl')))\n",
    "\n",
    "\n",
    "listener_encoder = ListenerEncoderCNN(listener_embed_size)\n",
    "listener_rnn = ListenerEncoderRNN(listener_embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "speaker_encoder.to(device)\n",
    "speaker_encoder.eval()\n",
    "speaker_decoder.to(device)\n",
    "listener_encoder.to(device)\n",
    "listener_rnn.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "# probably different sets of params will have to be defined, depending on applicable loss\n",
    "params = list(listener_rnn.lstm.parameters()) + list(listener_encoder.embed.parameters()) + list(listener_encoder.batch.parameters()) # + list(listener_rnn.linear.parameters())\n",
    "\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "total_step = math.ceil(len(data_loader_pairs.dataset.caption_lengths) / data_loader_pairs.batch_sampler.batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609d48b",
   "metadata": {},
   "source": [
    "**RN the captions of target and distractor have to be same length as the sampling which one is the target happens after batching - if this is moved to an earlier point, this could be avoided**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0caf6",
   "metadata": {},
   "source": [
    "**WArning** for now the mean over the embeddings is computed to handle the shape for computing the dot product - not sure how this is actually intended. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7e3a6",
   "metadata": {},
   "source": [
    "maybe i can override the getitem fct to take tuples of indices and return two images from the dataset. this would allow for the dataloader to take a list on tuple indices, and not instantiate two data loaders on separate index lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff88f07",
   "metadata": {},
   "source": [
    "Feed the message embedding right into the Encoder CNN, compute embeddings on pairs, compute dot product similarities for the pairs and output a tensor with shape (batch, 2), the second being a list with scores over the target, i.e. [1,0] if first image is the target, [0,1] is vice versa. The data loader just needs to sample something like `(img1, img2), [1,0], caption for target img`. \n",
    "\n",
    "Then cross entropy can be applied straight on the output of the encoder net. Only question - does the target need to be the list [1,0] or just the index (0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0947bc",
   "metadata": {},
   "source": [
    "I don't think i need to call .eval() on the speaker model - i just need to call decoder.sample(features) instead of decoder(features, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6cc6c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18258"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader_pairs.dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b66ae00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTHS:  128   128\n",
      "Batch size before:  128\n",
      "Shape of caption from data loader:  torch.Size([128, 16])\n",
      "<class 'torch.Tensor'>\n",
      "Target images shape:  128\n",
      "Target images shape:  torch.Size([128, 3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed492a70>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492e90>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492cb0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed492ad0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c470>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492a70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492e90>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed492b30>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492ad0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492a70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492ad0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492ad0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619233230>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492b30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619264290>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6dd57ab90>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6192644d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619233230>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4829b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed482e30>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492b90>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed482e30>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b883bf0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed492b90>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b84c470>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed492b90>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b883a70>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed482e30>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619233230>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4829b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed492b90>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619233230>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6192644d0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b8901d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482e30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b883890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619233230>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482e30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b883a70>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482e30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b795e30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619233230>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b883890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b883a70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b883890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b883a70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b883890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619233230>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b883a70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b795d10>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b795d10>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240d70>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b899950>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed482a10>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6dd57a8f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b795fb0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240d70>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240d70>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b887cb0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b881f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896ef0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b899950>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b8870b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b899530>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b881f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b887cb0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b881f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b887590>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b887cb0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4929b0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b887cb0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a9e30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b887cb0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b77ba10>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b887cb0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899530>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7a9770>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a9770>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7b6c50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73d051650>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7ae110>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b899950>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a9770>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a9410>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7ae110>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a9770>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc619240f50>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b79c830>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b896890>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b79c830>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b899950>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a6a70>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b896890>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a6ef0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73d051650>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b7ae290>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc619240f50>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b79c830>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a6ef0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b890350>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b84c0b0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b899950>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b77bb30>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b79c830>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "<built-in method size of Tensor object at 0x7fc73b7ae110>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc73b890350>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b79c590>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7fc73b87e1d0>\n",
      "features size after view:  <built-in method size of Tensor object at 0x7fc6ed4928f0>\n",
      "torch.Size([1, 2048])\n",
      "features after applying batch norm:  <built-in method size of Tensor object at 0x7fc73b7a91d0>\n",
      "torch.Size([1, 1024])\n",
      "predicting next word\n",
      "Sampled max index:  tensor([0])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0999, -0.0760, -0.0576,  ...,  0.0113,  0.0204,  0.0388]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([4])\n",
      "Inputs at end of sampling step:  torch.Size([1, 1, 1024])   tensor([[[-0.0784,  0.0505,  0.0338,  ...,  0.0150,  0.0739,  0.0175]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "predicting next word\n",
      "Sampled max index:  tensor([1])\n",
      "Preds_out;  [[tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])], [tensor([0]), tensor([4]), tensor([4]), tensor([4]), tensor([1])]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [5, 1] at entry 0 and [4, 1] at entry 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-b33f239f7a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preds_out; \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mpreds_out_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mpreds_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_out_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape preds out: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mhiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistener_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [5, 1] at entry 0 and [4, 1] at entry 18"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        # TODO: also retriveing the distractor captions and reinstantiating the dataloader seems \n",
    "        # rather inefficient w r t memory / runtime \n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices_pairs = data_loader_pairs.dataset.get_func_train_indices()\n",
    "        \n",
    "        # Create and assign a batch sampler to retrieve a target batch with the sampled indices.\n",
    "        new_sampler_pairs = torch.utils.data.sampler.SubsetRandomSampler(indices=indices_pairs)\n",
    "        \n",
    "        #####\n",
    "        # just turn the indices passed in here into tuples for target + dist (zipped or something)\n",
    "        # and modify __getitem__ to accept tuples\n",
    "        # sample at random in the __getitem__ method whether the first or second image is target,\n",
    "        # return respective caption and index\n",
    "#         new_sampler_distractor = torch.utils.data.sampler.SubsetRandomSampler(indices=indices_distrctors)\n",
    "        data_loader_pairs.batch_sampler.sampler = new_sampler_pairs\n",
    "        ######\n",
    "        print(\"Batch size before: \", batch_size)\n",
    "        # Obtain the target batch.\n",
    "        images1, images2, captions, targets = next(iter(data_loader_pairs))\n",
    "        print(\"Shape of caption from data loader: \", captions.shape)\n",
    "        print(type(images1))\n",
    "#         (target_image, distractor_image), target_caption, target \n",
    "\n",
    "\n",
    "        ## batch will the look like image_tuples, caption, target_ind = next(iter(data_loader))\n",
    "        ## modify listener CNN encoder to accept tuple of images, message embedding\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images1 = images1.to(device)\n",
    "        images2 = images2.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = targets.to(device)\n",
    "#         print(\"Training Loop captions shape: \", captions.shape)\n",
    "        \n",
    "        # Zero the gradients (reset).\n",
    "        speaker_encoder.zero_grad()\n",
    "        speaker_decoder.zero_grad()\n",
    "        listener_encoder.zero_grad()\n",
    "        listener_rnn.zero_grad()\n",
    "        \n",
    "        # Pass the targets through the CNN-RNN model.\n",
    "        \n",
    "        # sample caption from speaker \n",
    "        # zip images and target indices such that we can input correct image into speaker\n",
    "        train_pairs = list(zip(zip(images1, images2), targets))\n",
    "        target_images = [im[j] for im, j in train_pairs]\n",
    "        print(\"Target images shape: \", len(target_images))\n",
    "        print(\"Target images shape: \", torch.stack(target_images).shape)\n",
    "        preds_out = []\n",
    "        for im in target_images: \n",
    "            print(im.shape)\n",
    "            print(im.unsqueeze(0).shape)\n",
    "            speaker_features = speaker_encoder(im.unsqueeze(0))\n",
    "            captions_pred = speaker_decoder.sample(speaker_features.unsqueeze(1))\n",
    "            preds_out.append(captions_pred)\n",
    "        print(\"Preds_out; \", preds_out)    \n",
    "        preds_out_tensor = [torch.stack(x) for x in preds_out]\n",
    "        preds_out = torch.stack(preds_out_tensor)    \n",
    "        print(\"Shape preds out: \", preds_out.shape)\n",
    "        hiddens, outputs = listener_rnn(preds_out)\n",
    "        \n",
    "        # get the predictions\n",
    "        # make this like in the sample method: retrieve indices of most likely words, retrieve their embeddings for inputting into the visual encoder\n",
    "#         outputs_pred, max_indices = torch.max(outputs, dim = -1)\n",
    "#         outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n",
    "#             _, max_indice = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)\n",
    "#             print(\"Sampled max index: \", max_indice)\n",
    "#             output.append(max_indice.cpu().numpy()[0].item()) # storing the word predicted\n",
    "            \n",
    "            \n",
    "            \n",
    "#             ## Prepare to embed the last predicted word to be the new input of the lstm\n",
    "#             inputs = self.embed(max_indice) # inputs shape : (1, embed_size)\n",
    "            ##########\n",
    "        print(\"Shape of double output predicted caption: \", hiddens.shape, outputs.shape)\n",
    "        \n",
    "        \n",
    "        predictions = listener_encoder(images1, images2, hiddens) # TODO - make this images, outputs\n",
    "        \n",
    "        print(\"PREDS: \", predictions)\n",
    "        print(type(predictions))\n",
    "        print(predictions.shape)\n",
    "        print(\"TARGETS: \", targets)\n",
    "        print(type(targets))\n",
    "        print(targets.to(torch.float32).shape)\n",
    "        print(targets.shape)\n",
    "        print(\"OUTPUTS: \", outputs.shape)\n",
    "        #######\n",
    "        # Pass the distractors through the CNN\n",
    "        # TODO: check if really needs re-initialization / how to design the data_loader better\n",
    "        \n",
    "#         data_loader_distractor.batch_sampler.sampler = new_sampler_distractor\n",
    "#         images_distractor, _ = next(iter(data_loader_distractor))\n",
    "#         images_distractor = images_distractor.to(device)\n",
    "#         features_distractor = listener_encoder(images_distractor)\n",
    "        ######\n",
    "        print(\"GOT THROUGH STEP: \", i_step)\n",
    "        # compute the dot product similarity between each image and caption encoding\n",
    "        \n",
    "        \n",
    "        # TODO instatiate the notion of target smh\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        \n",
    "        # REINFORCE for functional part, applied to speaker LSTM weights (maybe also Linear ones)\n",
    "        # cross entropy for Listener LSTM and Linear\n",
    "        # and also cross entropy for Speaker params, optimizing against target caption of the target image\n",
    "        # (last implemented just like for pretraining)\n",
    "        print(outputs.contiguous().view(-1, vocab_size).shape)\n",
    "        print(captions.reshape(-1).shape)\n",
    "        \n",
    "        # combine listener loss and structural loss\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.reshape(-1)) + criterion(predictions, targets)\n",
    "        print(\"LOSSES\")\n",
    "        print(loss)\n",
    "        # REWARD\n",
    "        # if target index and output index match, 1, else -1\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "#         # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "#         f.write(stats + '\\n')\n",
    "#         f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "#     if epoch % save_every == 0:\n",
    "#         torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "#         torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6ec1a",
   "metadata": {},
   "source": [
    "--> Doesn't seem to work in batches, since predicted captions may be of different length and therefore cannot be packed in one batch, except if padded somehow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2a909",
   "metadata": {},
   "source": [
    "shape is not quite right and the forward step takes quite some time.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f5222912",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= torch.empty((128, 15, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8715a817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 512])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.mean(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d0a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "msc-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
